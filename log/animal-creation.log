6.1s 1 0.00s - Debugger warning: It seems that frozen modules are being used, which may
6.1s 2 0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off
6.1s 3 0.00s - to python to disable frozen modules.
6.1s 4 0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
6.8s 5 0.00s - Debugger warning: It seems that frozen modules are being used, which may
6.8s 6 0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off
6.8s 7 0.00s - to python to disable frozen modules.
6.8s 8 0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
11.3s 9 Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)
11.3s 10 Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)
11.3s 11 Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)
11.3s 12 Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)
11.3s 13 Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)
11.3s 14 Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)
11.3s 15 Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.52.4)
11.3s 16 Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)
11.3s 17 Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.8.1)
11.3s 18 Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)
11.3s 19 Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.33.1)
11.4s 20 Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)
11.4s 21 Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.5.1)
11.4s 22 Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)
11.4s 23 Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)
11.4s 24 Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)
11.4s 25 Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)
11.4s 26 Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)
11.4s 27 Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)
11.4s 28 Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.2.0)
11.4s 29 Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.2.0)
11.4s 30 Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)
11.4s 31 Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)
11.4s 32 Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)
11.5s 33 Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)
11.5s 34 Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
11.6s 35 Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)
11.6s 36 Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
11.7s 37 Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)
11.7s 38 Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
11.8s 39 Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)
11.8s 40 Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
11.8s 41 Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)
11.8s 42 Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
11.9s 43 Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)
11.9s 44 Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
12.0s 45 Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)
12.0s 46 Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
12.0s 47 Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)
12.0s 48 Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
12.1s 49 Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)
12.1s 50 Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
12.1s 51 Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)
12.1s 52 Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)
12.1s 53 Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)
12.2s 54 Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)
12.2s 55 Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
12.2s 56 Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)
12.2s 57 Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)
12.2s 58 Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)
12.3s 59 Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)
12.3s 60 Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.2)
12.3s 61 Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)
12.3s 62 Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)
12.3s 63 Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.2.0)
12.3s 64 Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.4.0)
12.3s 65 Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)
12.3s 66 Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)
12.3s 67 Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)
12.3s 68 Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)
12.3s 69 Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.6.15)
12.3s 70 Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)
12.4s 71 Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)
16.9s 72 [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/363.4 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.3/363.4 MB[0m [31m8.3 MB/s[0m eta [36m0:00:44[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.7/363.4 MB[0m [31m25.2 MB/s[0m eta [36m0:00:15[0m[2K   [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m7.1/363.4 MB[0m [31m69.0 MB/s[0m eta [36m0:00:06[0m[2K   [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m11.9/363.4 MB[0m [31m144.6 MB/s[0m eta [36m0:00:03[0m[2K   [91m━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m17.7/363.4 MB[0m [31m153.6 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m24.5/363.4 MB[0m [31m177.5 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m29.4/363.4 MB[0m [31m187.3 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m34.8/363.4 MB[0m [31m149.7 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m37.7/363.4 MB[0m [31m123.9 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m40.9/363.4 MB[0m [31m106.4 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m45.4/363.4 MB[0m [31m105.3 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m47.8/363.4 MB[0m [31m93.4 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m53.1/363.4 MB[0m [31m121.1 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m57.3/363.4 MB[0m [31m136.8 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m61.5/363.4 MB[0m [31m122.8 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m68.6/363.4 MB[0m [31m176.6 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m74.6/363.4 MB[0m [31m190.8 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m79.6/363.4 MB[0m [31m157.1 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m83.5/363.4 MB[0m [31m123.1 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m88.2/363.4 MB[0m [31m120.6 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m94.2/363.4 MB[0m [31m154.3 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m99.3/363.4 MB[0m [31m158.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m105.6/363.4 MB[0m [31m162.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m110.1/363.4 MB[0m [31m157.3 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m113.6/363.4 MB[0m [31m123.3 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m118.0/363.4 MB[0m [31m116.8 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m122.9/363.4 MB[0m [31m134.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m128.2/363.4 MB[0m [31m144.4 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m133.0/363.4 MB[0m [31m143.4 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m139.3/363.4 MB[0m [31m168.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m145.8/363.4 MB[0m [31m191.6 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m152.3/363.4 MB[0m [31m190.3 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m158.9/363.4 MB[0m [31m190.1 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m164.6/363.4 MB[0m [31m197.8 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m167.2/363.4 MB[0m [31m127.5 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m171.4/363.4 MB[0m [31m112.7 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m176.7/363.4 MB[0m [31m121.7 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m182.0/363.4 MB[0m [31m154.8 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m187.3/363.4 MB[0m [31m153.5 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m193.2/363.4 MB[0m [31m166.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━[0m [32m198.7/363.4 MB[0m [31m164.3 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m205.4/363.4 MB[0m [31m190.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━[0m [32m211.8/363.4 MB[0m [31m188.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m218.1/363.4 MB[0m [31m183.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━[0m [32m222.3/363.4 MB[0m [31m193.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━[0m [32m228.2/363.4 MB[0m [31m145.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━[0m [32m233.2/363.4 MB[0m [31m159.0 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m237.8/363.4 MB[0m [31m135.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━[0m [32m242.6/363.4 MB[0m [31m129.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━[0m [32m249.0/363.4 MB[0m [31m161.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━[0m [32m251.7/363.4 MB[0m [31m150.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━[0m [32m256.1/363.4 MB[0m [31m117.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━[0m [32m262.4/363.4 MB[0m [31m159.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━[0m [32m269.1/363.4 MB[0m [31m189.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━[0m [32m274.7/363.4 MB[0m [31m178.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━[0m [32m281.3/363.4 MB[0m [31m177.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━[0m [32m285.8/363.4 MB[0m [31m157.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━[0m [32m290.5/363.4 MB[0m [31m136.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━[0m [32m295.7/363.4 MB[0m [31m143.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━[0m [32m302.2/363.4 MB[0m [31m188.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━[0m [32m308.5/363.4 MB[0m [31m186.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━[0m [32m315.2/363.4 MB[0m [31m191.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━[0m [32m321.5/363.4 MB[0m [31m187.0 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━[0m [32m328.1/363.4 MB[0m [31m190.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━[0m [32m334.7/363.4 MB[0m [31m189.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m341.6/363.4 MB[0m [31m199.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━[0m [32m347.1/363.4 MB[0m [31m179.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m353.6/363.4 MB[0m [31m189.0 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m359.3/363.4 MB[0m [31m176.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m363.4/363.4 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m363.4/363.4 MB[0m [31m4.7 MB/s[0m eta [36m0:00:00[0m
16.9s 73 [?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)
17.1s 74 [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/13.8 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91m━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.6/13.8 MB[0m [31m137.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m8.0/13.8 MB[0m [31m117.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m13.0/13.8 MB[0m [31m130.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m13.8/13.8 MB[0m [31m128.8 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m13.8/13.8 MB[0m [31m78.8 MB/s[0m eta [36m0:00:00[0m
17.1s 75 [?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)
17.3s 76 [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/24.6 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91m━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m5.4/24.6 MB[0m [31m162.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m11.7/24.6 MB[0m [31m165.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━[0m [32m16.8/24.6 MB[0m [31m191.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━[0m [32m19.6/24.6 MB[0m [31m127.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m24.6/24.6 MB[0m [31m114.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m24.6/24.6 MB[0m [31m112.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m24.6/24.6 MB[0m [31m112.2 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m24.6/24.6 MB[0m [31m57.5 MB/s[0m eta [36m0:00:00[0m
17.4s 77 [?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)
17.4s 78 [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/883.7 kB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m883.7/883.7 kB[0m [31m43.8 MB/s[0m eta [36m0:00:00[0m
17.4s 79 [?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)
25.3s 80 [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/664.8 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m7.6/664.8 MB[0m [31m228.5 MB/s[0m eta [36m0:00:03[0m[2K   [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m15.3/664.8 MB[0m [31m224.9 MB/s[0m eta [36m0:00:03[0m[2K   [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m22.6/664.8 MB[0m [31m212.5 MB/s[0m eta [36m0:00:04[0m[2K   [91m━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m28.1/664.8 MB[0m [31m199.3 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m33.6/664.8 MB[0m [31m158.1 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m39.3/664.8 MB[0m [31m172.5 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m43.3/664.8 MB[0m [31m137.5 MB/s[0m eta [36m0:00:05[0m[2K   [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m49.6/664.8 MB[0m [31m149.7 MB/s[0m eta [36m0:00:05[0m[2K   [91m━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m55.2/664.8 MB[0m [31m168.7 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m60.2/664.8 MB[0m [31m152.9 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m67.8/664.8 MB[0m [31m188.1 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m74.5/664.8 MB[0m [31m204.3 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m82.0/664.8 MB[0m [31m199.0 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m87.9/664.8 MB[0m [31m196.6 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m91.6/664.8 MB[0m [31m146.2 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m95.8/664.8 MB[0m [31m120.6 MB/s[0m eta [36m0:00:05[0m[2K   [91m━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m103.3/664.8 MB[0m [31m177.1 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m108.6/664.8 MB[0m [31m179.5 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m114.5/664.8 MB[0m [31m162.8 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m119.6/664.8 MB[0m [31m163.4 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m125.5/664.8 MB[0m [31m167.4 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m129.5/664.8 MB[0m [31m142.7 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m136.6/664.8 MB[0m [31m163.7 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m142.1/664.8 MB[0m [31m187.6 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m146.4/664.8 MB[0m [31m141.9 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m152.8/664.8 MB[0m [31m153.8 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m158.3/664.8 MB[0m [31m176.5 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m165.7/664.8 MB[0m [31m193.7 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m170.6/664.8 MB[0m [31m174.9 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m177.4/664.8 MB[0m [31m193.2 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m182.2/664.8 MB[0m [31m169.6 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m188.7/664.8 MB[0m [31m171.2 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m194.1/664.8 MB[0m [31m166.7 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m199.4/664.8 MB[0m [31m155.3 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m204.3/664.8 MB[0m [31m145.8 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m208.9/664.8 MB[0m [31m135.7 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m213.9/664.8 MB[0m [31m143.0 MB/s[0m eta [36m0:00:04[0m[2K   [91m━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m220.2/664.8 MB[0m [31m161.3 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m225.8/664.8 MB[0m [31m171.9 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m230.9/664.8 MB[0m [31m156.2 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m237.6/664.8 MB[0m [31m169.3 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m245.4/664.8 MB[0m [31m210.6 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m253.1/664.8 MB[0m [31m223.4 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m259.0/664.8 MB[0m [31m198.1 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m266.8/664.8 MB[0m [31m190.0 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m274.5/664.8 MB[0m [31m223.3 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m282.3/664.8 MB[0m [31m224.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m290.2/664.8 MB[0m [31m228.1 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m295.5/664.8 MB[0m [31m184.6 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m301.6/664.8 MB[0m [31m161.0 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m308.0/664.8 MB[0m [31m190.4 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m314.8/664.8 MB[0m [31m186.5 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m319.4/664.8 MB[0m [31m163.0 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m323.3/664.8 MB[0m [31m127.3 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m326.4/664.8 MB[0m [31m117.6 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m332.2/664.8 MB[0m [31m113.9 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m339.1/664.8 MB[0m [31m193.9 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m343.3/664.8 MB[0m [31m162.3 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m347.3/664.8 MB[0m [31m130.2 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m352.1/664.8 MB[0m [31m118.2 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━[0m [32m358.7/664.8 MB[0m [31m161.6 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━[0m [32m362.1/664.8 MB[0m [31m140.9 MB/s[0m eta [36m0:00:03[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━[0m [32m369.1/664.8 MB[0m [31m151.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m375.4/664.8 MB[0m [31m187.4 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m383.0/664.8 MB[0m [31m208.4 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━[0m [32m389.4/664.8 MB[0m [31m197.1 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m394.6/664.8 MB[0m [31m163.3 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m399.2/664.8 MB[0m [31m140.1 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━[0m [32m405.9/664.8 MB[0m [31m161.3 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━[0m [32m411.0/664.8 MB[0m [31m176.3 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━[0m [32m417.3/664.8 MB[0m [31m165.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━[0m [32m424.0/664.8 MB[0m [31m192.3 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━[0m [32m430.8/664.8 MB[0m [31m196.0 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m435.2/664.8 MB[0m [31m163.5 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m438.6/664.8 MB[0m [31m125.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━[0m [32m443.3/664.8 MB[0m [31m114.5 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━[0m [32m449.0/664.8 MB[0m [31m151.5 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━[0m [32m454.9/664.8 MB[0m [31m162.6 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━[0m [32m462.1/664.8 MB[0m [31m198.5 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━[0m [32m468.0/664.8 MB[0m [31m189.9 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━[0m [32m473.1/664.8 MB[0m [31m156.9 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━[0m [32m477.1/664.8 MB[0m [31m130.5 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━[0m [32m480.3/664.8 MB[0m [31m110.9 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━[0m [32m487.4/664.8 MB[0m [31m156.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━[0m [32m491.7/664.8 MB[0m [31m162.8 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━[0m [32m499.0/664.8 MB[0m [31m193.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━[0m [32m503.1/664.8 MB[0m [31m161.4 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━[0m [32m510.4/664.8 MB[0m [31m172.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━[0m [32m518.0/664.8 MB[0m [31m216.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━[0m [32m523.1/664.8 MB[0m [31m178.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━[0m [32m528.1/664.8 MB[0m [31m143.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━[0m [32m533.1/664.8 MB[0m [31m139.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━[0m [32m540.1/664.8 MB[0m [31m186.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━[0m [32m546.2/664.8 MB[0m [31m180.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━[0m [32m551.1/664.8 MB[0m [31m153.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━[0m [32m555.7/664.8 MB[0m [31m134.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━[0m [32m559.9/664.8 MB[0m [31m124.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━[0m [32m566.4/664.8 MB[0m [31m154.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━[0m [32m572.6/664.8 MB[0m [31m186.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━[0m [32m577.8/664.8 MB[0m [31m164.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━[0m [32m584.3/664.8 MB[0m [31m177.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━[0m [32m590.3/664.8 MB[0m [31m179.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━[0m [32m596.2/664.8 MB[0m [31m170.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━[0m [32m602.3/664.8 MB[0m [31m175.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━[0m [32m607.0/664.8 MB[0m [31m157.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━[0m [32m614.7/664.8 MB[0m [31m192.0 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m622.7/664.8 MB[0m [31m225.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m627.7/664.8 MB[0m [31m179.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━[0m [32m632.9/664.8 MB[0m [31m145.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━[0m [32m638.8/664.8 MB[0m [31m158.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m645.1/664.8 MB[0m [31m190.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m [32m649.9/664.8 MB[0m [31m160.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m [32m653.3/664.8 MB[0m [31m125.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m657.6/664.8 MB[0m [31m115.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m663.4/664.8 MB[0m [31m143.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m664.8/664.8 MB[0m [31m146.3 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m664.8/664.8 MB[0m [31m2.6 MB/s[0m eta [36m0:00:00[0m
25.3s 81 [?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)
28.4s 82 [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/211.5 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.2/211.5 MB[0m [31m96.5 MB/s[0m eta [36m0:00:03[0m[2K   [91m━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m9.8/211.5 MB[0m [31m143.6 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m14.9/211.5 MB[0m [31m163.9 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19.0/211.5 MB[0m [31m136.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m23.2/211.5 MB[0m [31m119.7 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m27.7/211.5 MB[0m [31m117.4 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m32.5/211.5 MB[0m [31m137.6 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m36.6/211.5 MB[0m [31m118.6 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m41.5/211.5 MB[0m [31m125.4 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m45.9/211.5 MB[0m [31m136.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m50.9/211.5 MB[0m [31m124.2 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m56.2/211.5 MB[0m [31m151.6 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m62.6/211.5 MB[0m [31m184.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m67.5/211.5 MB[0m [31m163.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m73.9/211.5 MB[0m [31m177.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m79.5/211.5 MB[0m [31m172.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m85.9/211.5 MB[0m [31m179.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m90.5/211.5 MB[0m [31m160.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m95.0/211.5 MB[0m [31m135.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m101.2/211.5 MB[0m [31m154.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m107.7/211.5 MB[0m [31m186.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━[0m [32m114.6/211.5 MB[0m [31m193.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━[0m [32m118.4/211.5 MB[0m [31m157.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━[0m [32m123.3/211.5 MB[0m [31m133.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━[0m [32m127.6/211.5 MB[0m [31m116.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━[0m [32m131.5/211.5 MB[0m [31m127.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━[0m [32m134.3/211.5 MB[0m [31m103.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━[0m [32m137.9/211.5 MB[0m [31m99.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m140.8/211.5 MB[0m [31m91.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m142.5/211.5 MB[0m [31m78.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━[0m [32m144.7/211.5 MB[0m [31m75.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━[0m [32m148.9/211.5 MB[0m [31m80.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━[0m [32m151.6/211.5 MB[0m [31m81.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━[0m [32m156.5/211.5 MB[0m [31m107.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━[0m [32m159.4/211.5 MB[0m [31m102.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━[0m [32m162.5/211.5 MB[0m [31m110.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━[0m [32m165.3/211.5 MB[0m [31m89.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━[0m [32m168.6/211.5 MB[0m [31m81.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━[0m [32m172.2/211.5 MB[0m [31m95.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━[0m [32m176.5/211.5 MB[0m [31m112.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━[0m [32m182.6/211.5 MB[0m [31m152.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━[0m [32m187.3/211.5 MB[0m [31m155.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━[0m [32m190.6/211.5 MB[0m [31m121.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━[0m [32m195.0/211.5 MB[0m [31m112.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m199.2/211.5 MB[0m [31m112.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m205.5/211.5 MB[0m [31m155.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m211.5/211.5 MB[0m [31m205.5 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m211.5/211.5 MB[0m [31m6.5 MB/s[0m eta [36m0:00:00[0m
28.5s 83 [?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)
29.1s 84 [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/56.3 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91m━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m7.9/56.3 MB[0m [31m238.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m12.6/56.3 MB[0m [31m202.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m15.7/56.3 MB[0m [31m147.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m22.0/56.3 MB[0m [31m127.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m28.0/56.3 MB[0m [31m179.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m34.2/56.3 MB[0m [31m182.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━[0m [32m40.9/56.3 MB[0m [31m196.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━[0m [32m47.3/56.3 MB[0m [31m189.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━[0m [32m54.2/56.3 MB[0m [31m199.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m56.3/56.3 MB[0m [31m200.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m56.3/56.3 MB[0m [31m200.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m56.3/56.3 MB[0m [31m200.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m56.3/56.3 MB[0m [31m200.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m56.3/56.3 MB[0m [31m200.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m56.3/56.3 MB[0m [31m200.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m56.3/56.3 MB[0m [31m200.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m56.3/56.3 MB[0m [31m200.5 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m56.3/56.3 MB[0m [31m31.2 MB/s[0m eta [36m0:00:00[0m
29.1s 85 [?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)
30.6s 86 [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/127.9 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.2/127.9 MB[0m [31m125.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m6.9/127.9 MB[0m [31m100.9 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m9.4/127.9 MB[0m [31m93.8 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m14.9/127.9 MB[0m [31m103.0 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19.4/127.9 MB[0m [31m139.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m24.2/127.9 MB[0m [31m129.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m29.1/127.9 MB[0m [31m138.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m35.6/127.9 MB[0m [31m172.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m42.3/127.9 MB[0m [31m189.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m48.0/127.9 MB[0m [31m180.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m54.4/127.9 MB[0m [31m172.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m59.5/127.9 MB[0m [31m166.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m65.8/127.9 MB[0m [31m168.0 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━[0m [32m72.1/127.9 MB[0m [31m185.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m77.0/127.9 MB[0m [31m160.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━[0m [32m81.5/127.9 MB[0m [31m137.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m84.9/127.9 MB[0m [31m128.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━[0m [32m88.2/127.9 MB[0m [31m104.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━[0m [32m95.7/127.9 MB[0m [31m186.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━[0m [32m103.2/127.9 MB[0m [31m217.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━[0m [32m109.0/127.9 MB[0m [31m185.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━[0m [32m114.3/127.9 MB[0m [31m170.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m120.4/127.9 MB[0m [31m162.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m215.0 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m127.9/127.9 MB[0m [31m208.3 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m127.9/127.9 MB[0m [31m13.5 MB/s[0m eta [36m0:00:00[0m
30.6s 87 [?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)
38.0s 88 [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/207.5 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.5/207.5 MB[0m [31m137.8 MB/s[0m eta [36m0:00:02[0m[2K   [91m━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m9.7/207.5 MB[0m [31m141.8 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m13.2/207.5 MB[0m [31m117.9 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m16.9/207.5 MB[0m [31m111.1 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m21.9/207.5 MB[0m [31m115.4 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m28.6/207.5 MB[0m [31m168.7 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m34.7/207.5 MB[0m [31m185.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m39.9/207.5 MB[0m [31m160.5 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m45.6/207.5 MB[0m [31m159.0 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m51.0/207.5 MB[0m [31m154.4 MB/s[0m eta [36m0:00:02[0m[2K   [91m━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m56.5/207.5 MB[0m [31m154.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m62.5/207.5 MB[0m [31m164.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m68.1/207.5 MB[0m [31m163.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m73.5/207.5 MB[0m [31m159.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m79.0/207.5 MB[0m [31m155.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m85.3/207.5 MB[0m [31m170.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m90.5/207.5 MB[0m [31m168.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m95.9/207.5 MB[0m [31m153.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m102.0/207.5 MB[0m [31m171.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m108.0/207.5 MB[0m [31m173.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━[0m [32m113.0/207.5 MB[0m [31m158.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m118.5/207.5 MB[0m [31m153.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m122.6/207.5 MB[0m [31m133.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━[0m [32m127.5/207.5 MB[0m [31m129.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━[0m [32m132.9/207.5 MB[0m [31m146.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━[0m [32m137.8/207.5 MB[0m [31m147.8 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━[0m [32m141.5/207.5 MB[0m [31m122.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━[0m [32m145.4/207.5 MB[0m [31m114.7 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━[0m [32m149.2/207.5 MB[0m [31m114.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━[0m [32m155.9/207.5 MB[0m [31m157.4 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━[0m [32m162.0/207.5 MB[0m [31m180.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━[0m [32m166.2/207.5 MB[0m [31m150.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━[0m [32m169.7/207.5 MB[0m [31m113.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━[0m [32m175.6/207.5 MB[0m [31m126.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━[0m [32m181.4/207.5 MB[0m [31m174.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━[0m [32m183.7/207.5 MB[0m [31m124.0 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━[0m [32m188.9/207.5 MB[0m [31m117.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━[0m [32m191.8/207.5 MB[0m [31m108.9 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━[0m [32m197.4/207.5 MB[0m [31m127.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m200.2/207.5 MB[0m [31m104.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m201.9/207.5 MB[0m [31m95.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m205.4/207.5 MB[0m [31m85.3 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m207.5/207.5 MB[0m [31m76.1 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m207.5/207.5 MB[0m [31m1.7 MB/s[0m eta [36m0:00:00[0m
38.0s 89 [?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)
38.2s 90 [?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/21.1 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91m━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m6.5/21.1 MB[0m [31m194.0 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m11.3/21.1 MB[0m [31m170.2 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━[0m [32m16.2/21.1 MB[0m [31m143.6 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m21.1/21.1 MB[0m [31m157.1 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m21.1/21.1 MB[0m [31m157.1 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m21.1/21.1 MB[0m [31m75.3 MB/s[0m eta [36m0:00:00[0m
44.7s 91 [?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12
44.7s 92 Attempting uninstall: nvidia-nvjitlink-cu12
44.7s 93 Found existing installation: nvidia-nvjitlink-cu12 12.5.82
44.7s 94 Uninstalling nvidia-nvjitlink-cu12-12.5.82:
45.1s 95 Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82
45.4s 96 Attempting uninstall: nvidia-curand-cu12
45.5s 97 Found existing installation: nvidia-curand-cu12 10.3.6.82
45.5s 98 Uninstalling nvidia-curand-cu12-10.3.6.82:
45.9s 99 Successfully uninstalled nvidia-curand-cu12-10.3.6.82
46.4s 100 Attempting uninstall: nvidia-cufft-cu12
46.4s 101 Found existing installation: nvidia-cufft-cu12 11.2.3.61
46.4s 102 Uninstalling nvidia-cufft-cu12-11.2.3.61:
47.6s 103 Successfully uninstalled nvidia-cufft-cu12-11.2.3.61
49.7s 104 Attempting uninstall: nvidia-cuda-runtime-cu12
49.8s 105 Found existing installation: nvidia-cuda-runtime-cu12 12.5.82
49.8s 106 Uninstalling nvidia-cuda-runtime-cu12-12.5.82:
49.8s 107 Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82
49.9s 108 Attempting uninstall: nvidia-cuda-nvrtc-cu12
49.9s 109 Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82
49.9s 110 Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:
50.2s 111 Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82
50.6s 112 Attempting uninstall: nvidia-cuda-cupti-cu12
50.7s 113 Found existing installation: nvidia-cuda-cupti-cu12 12.5.82
50.7s 114 Uninstalling nvidia-cuda-cupti-cu12-12.5.82:
50.9s 115 Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82
51.2s 116 Attempting uninstall: nvidia-cublas-cu12
51.2s 117 Found existing installation: nvidia-cublas-cu12 12.5.3.2
51.2s 118 Uninstalling nvidia-cublas-cu12-12.5.3.2:
53.4s 119 Successfully uninstalled nvidia-cublas-cu12-12.5.3.2
57.7s 120 Attempting uninstall: nvidia-cusparse-cu12
57.7s 121 Found existing installation: nvidia-cusparse-cu12 12.5.1.3
57.7s 122 Uninstalling nvidia-cusparse-cu12-12.5.1.3:
59.1s 123 Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3
65.4s 124 Attempting uninstall: nvidia-cudnn-cu12
65.4s 125 Found existing installation: nvidia-cudnn-cu12 9.3.0.75
65.4s 126 Uninstalling nvidia-cudnn-cu12-9.3.0.75:
69.0s 127 Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75
78.7s 128 Attempting uninstall: nvidia-cusolver-cu12
78.7s 129 Found existing installation: nvidia-cusolver-cu12 11.6.3.83
78.7s 130 Uninstalling nvidia-cusolver-cu12-11.6.3.83:
79.6s 131 Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83
82.0s 132 Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127
101.1s 133 2025-08-31 02:49:43.306539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
101.1s 134 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
101.1s 135 E0000 00:00:1756608583.501745      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
101.1s 136 E0000 00:00:1756608583.564275      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
101.3s 137 2025-08-31 02:49:43.306539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
101.3s 138 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
101.3s 139 E0000 00:00:1756608583.501745      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
101.3s 140 E0000 00:00:1756608583.564275      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
114.3s 141 Starting LoRA training...
141.2s 142 trainable params: 398,592 || all params: 859,919,556 || trainable%: 0.0464
149.9s 143 Epoch 0, Step 0, Loss: 0.0044, LR: 0.000000, Grad Norm: 0.000000
150.2s 144 Epoch 0, Step 0, Loss: 0.0077, LR: 0.000000, Grad Norm: 0.000000
150.5s 145 Epoch 0, Step 0, Loss: 0.0216, LR: 0.000000, Grad Norm: 0.000000
209.0s 146 Epoch 0, Step 50, Loss: 0.2194, LR: 0.000003, Grad Norm: 0.227759
209.2s 147 Epoch 0, Step 50, Loss: 0.2198, LR: 0.000003, Grad Norm: 0.227759
209.5s 148 Epoch 0, Step 50, Loss: 0.2231, LR: 0.000003, Grad Norm: 0.227759
209.8s 149 Epoch 0, Step 50, Loss: 0.2228, LR: 0.000003, Grad Norm: 0.227759
267.0s 150 Epoch 0, Step 100, Loss: 0.2407, LR: 0.000005, Grad Norm: 0.207880
267.3s 151 Epoch 0, Step 100, Loss: 0.2403, LR: 0.000005, Grad Norm: 0.207880
267.9s 152 Epoch 0, Step 100, Loss: 0.2397, LR: 0.000005, Grad Norm: 0.207880
268.2s 153 Epoch 0, Step 100, Loss: 0.2395, LR: 0.000005, Grad Norm: 0.207880
325.3s 154 Epoch 0, Step 150, Loss: 0.2398, LR: 0.000008, Grad Norm: 0.217230
325.6s 155 Epoch 0, Step 150, Loss: 0.2400, LR: 0.000008, Grad Norm: 0.217230
325.9s 156 Epoch 0, Step 150, Loss: 0.2402, LR: 0.000008, Grad Norm: 0.217230
326.1s 157 Epoch 0, Step 150, Loss: 0.2409, LR: 0.000008, Grad Norm: 0.217230
383.7s 158 Epoch 0, Step 200, Loss: 0.2334, LR: 0.000010, Grad Norm: 0.174078
383.9s 159 Epoch 0, Step 200, Loss: 0.2338, LR: 0.000010, Grad Norm: 0.174078
384.2s 160 Epoch 0, Step 200, Loss: 0.2338, LR: 0.000010, Grad Norm: 0.174078
384.5s 161 Epoch 0, Step 200, Loss: 0.2335, LR: 0.000010, Grad Norm: 0.174078
441.7s 162 Epoch 0, Step 250, Loss: 0.2283, LR: 0.000010, Grad Norm: 0.142723
441.9s 163 Epoch 0, Step 250, Loss: 0.2283, LR: 0.000010, Grad Norm: 0.142723
442.2s 164 Epoch 0, Step 250, Loss: 0.2283, LR: 0.000010, Grad Norm: 0.142723
442.5s 165 Epoch 0, Step 250, Loss: 0.2280, LR: 0.000010, Grad Norm: 0.142723
500.0s 166 Epoch 0, Step 300, Loss: 0.2263, LR: 0.000010, Grad Norm: 0.133600
500.2s 167 Epoch 0, Step 300, Loss: 0.2264, LR: 0.000010, Grad Norm: 0.133600
500.5s 168 Epoch 0, Step 300, Loss: 0.2263, LR: 0.000010, Grad Norm: 0.133600
500.8s 169 Epoch 0, Step 300, Loss: 0.2262, LR: 0.000010, Grad Norm: 0.133600
557.9s 170 Epoch 0, Step 350, Loss: 0.2180, LR: 0.000010, Grad Norm: 0.098085
558.1s 171 Epoch 0, Step 350, Loss: 0.2183, LR: 0.000010, Grad Norm: 0.098085
558.4s 172 Epoch 0, Step 350, Loss: 0.2184, LR: 0.000010, Grad Norm: 0.098085
558.7s 173 Epoch 0, Step 350, Loss: 0.2183, LR: 0.000010, Grad Norm: 0.098085
616.0s 174 Epoch 0, Step 400, Loss: 0.2163, LR: 0.000010, Grad Norm: 0.073202
616.2s 175 Epoch 0, Step 400, Loss: 0.2166, LR: 0.000010, Grad Norm: 0.073202
616.5s 176 Epoch 0, Step 400, Loss: 0.2164, LR: 0.000010, Grad Norm: 0.073202
616.8s 177 Epoch 0, Step 400, Loss: 0.2164, LR: 0.000010, Grad Norm: 0.073202
673.7s 178 Epoch 0, Step 450, Loss: 0.2143, LR: 0.000010, Grad Norm: 0.119813
674.0s 179 Epoch 0, Step 450, Loss: 0.2142, LR: 0.000010, Grad Norm: 0.119813
674.2s 180 Epoch 0, Step 450, Loss: 0.2146, LR: 0.000010, Grad Norm: 0.119813
674.5s 181 Epoch 0, Step 450, Loss: 0.2146, LR: 0.000010, Grad Norm: 0.119813
731.7s 182 Epoch 0, Step 500, Loss: 0.2127, LR: 0.000010, Grad Norm: 0.082218
732.0s 183 Epoch 0, Step 500, Loss: 0.2126, LR: 0.000010, Grad Norm: 0.082218
732.3s 184 Epoch 0, Step 500, Loss: 0.2126, LR: 0.000010, Grad Norm: 0.082218
732.5s 185 Epoch 0, Step 500, Loss: 0.2125, LR: 0.000010, Grad Norm: 0.082218
789.7s 186 Epoch 0, Step 550, Loss: 0.2114, LR: 0.000010, Grad Norm: 0.060000
790.1s 187 Epoch 0, Step 550, Loss: 0.2115, LR: 0.000010, Grad Norm: 0.060000
790.4s 188 Epoch 0, Step 550, Loss: 0.2116, LR: 0.000010, Grad Norm: 0.060000
790.7s 189 Epoch 0, Step 550, Loss: 0.2115, LR: 0.000010, Grad Norm: 0.060000
847.7s 190 Epoch 0, Step 600, Loss: 0.2114, LR: 0.000010, Grad Norm: 0.150365
848.0s 191 Epoch 0, Step 600, Loss: 0.2114, LR: 0.000010, Grad Norm: 0.150365
848.2s 192 Epoch 0, Step 600, Loss: 0.2114, LR: 0.000010, Grad Norm: 0.150365
848.5s 193 Epoch 0, Step 600, Loss: 0.2113, LR: 0.000010, Grad Norm: 0.150365
905.9s 194 Epoch 0, Step 650, Loss: 0.2099, LR: 0.000010, Grad Norm: 0.094256
906.2s 195 Epoch 0, Step 650, Loss: 0.2098, LR: 0.000010, Grad Norm: 0.094256
906.4s 196 Epoch 0, Step 650, Loss: 0.2098, LR: 0.000010, Grad Norm: 0.094256
906.7s 197 Epoch 0, Step 650, Loss: 0.2097, LR: 0.000010, Grad Norm: 0.094256
963.7s 198 Epoch 0, Step 700, Loss: 0.2082, LR: 0.000010, Grad Norm: 0.081648
964.0s 199 Epoch 0, Step 700, Loss: 0.2083, LR: 0.000010, Grad Norm: 0.081648
964.3s 200 Epoch 0, Step 700, Loss: 0.2084, LR: 0.000010, Grad Norm: 0.081648
964.5s 201 Epoch 0, Step 700, Loss: 0.2083, LR: 0.000010, Grad Norm: 0.081648
1021.9s 202 Epoch 0, Step 750, Loss: 0.2075, LR: 0.000010, Grad Norm: 0.105688
1022.1s 203 Epoch 0, Step 750, Loss: 0.2077, LR: 0.000010, Grad Norm: 0.105688
1022.4s 204 Epoch 0, Step 750, Loss: 0.2078, LR: 0.000010, Grad Norm: 0.105688
1022.7s 205 Epoch 0, Step 750, Loss: 0.2078, LR: 0.000010, Grad Norm: 0.105688
1079.7s 206 Epoch 0, Step 800, Loss: 0.2063, LR: 0.000010, Grad Norm: 0.096009
1080.0s 207 Epoch 0, Step 800, Loss: 0.2063, LR: 0.000010, Grad Norm: 0.096009
1080.3s 208 Epoch 0, Step 800, Loss: 0.2065, LR: 0.000010, Grad Norm: 0.096009
1080.5s 209 Epoch 0, Step 800, Loss: 0.2065, LR: 0.000010, Grad Norm: 0.096009
1138.0s 210 Epoch 0, Step 850, Loss: 0.2049, LR: 0.000010, Grad Norm: 0.094091
1138.2s 211 Epoch 0, Step 850, Loss: 0.2048, LR: 0.000010, Grad Norm: 0.094091
1138.5s 212 Epoch 0, Step 850, Loss: 0.2048, LR: 0.000010, Grad Norm: 0.094091
1138.8s 213 Epoch 0, Step 850, Loss: 0.2048, LR: 0.000010, Grad Norm: 0.094091
1195.9s 214 Epoch 0, Step 900, Loss: 0.2049, LR: 0.000010, Grad Norm: 0.209486
1196.1s 215 Epoch 0, Step 900, Loss: 0.2048, LR: 0.000010, Grad Norm: 0.209486
1196.4s 216 Epoch 0, Step 900, Loss: 0.2048, LR: 0.000010, Grad Norm: 0.209486
1196.7s 217 Epoch 0, Step 900, Loss: 0.2047, LR: 0.000010, Grad Norm: 0.209486
1254.0s 218 Epoch 0, Step 950, Loss: 0.2046, LR: 0.000010, Grad Norm: 0.075040
1254.3s 219 Epoch 0, Step 950, Loss: 0.2046, LR: 0.000010, Grad Norm: 0.075040
1254.6s 220 Epoch 0, Step 950, Loss: 0.2046, LR: 0.000010, Grad Norm: 0.075040
1254.8s 221 Epoch 0, Step 950, Loss: 0.2046, LR: 0.000010, Grad Norm: 0.075040
1312.2s 222 Epoch 0, Step 1000, Loss: 0.2032, LR: 0.000010, Grad Norm: 0.122866
1312.4s 223 Epoch 0, Step 1000, Loss: 0.2032, LR: 0.000010, Grad Norm: 0.122866
1312.7s 224 Epoch 0, Step 1000, Loss: 0.2032, LR: 0.000010, Grad Norm: 0.122866
1313.0s 225 Epoch 0, Step 1000, Loss: 0.2031, LR: 0.000010, Grad Norm: 0.122866
1370.1s 226 Epoch 0, Step 1050, Loss: 0.2023, LR: 0.000010, Grad Norm: 0.083555
1370.3s 227 Epoch 0, Step 1050, Loss: 0.2023, LR: 0.000010, Grad Norm: 0.083555
1370.6s 228 Epoch 0, Step 1050, Loss: 0.2022, LR: 0.000010, Grad Norm: 0.083555
1370.9s 229 Epoch 0, Step 1050, Loss: 0.2023, LR: 0.000010, Grad Norm: 0.083555
1428.4s 230 Epoch 0, Step 1100, Loss: 0.2010, LR: 0.000010, Grad Norm: 0.066839
1428.6s 231 Epoch 0, Step 1100, Loss: 0.2010, LR: 0.000010, Grad Norm: 0.066839
1428.9s 232 Epoch 0, Step 1100, Loss: 0.2010, LR: 0.000010, Grad Norm: 0.066839
1429.2s 233 Epoch 0, Step 1100, Loss: 0.2010, LR: 0.000010, Grad Norm: 0.066839
1486.1s 234 Epoch 0, Step 1150, Loss: 0.2002, LR: 0.000010, Grad Norm: 0.058125
1486.4s 235 Epoch 0, Step 1150, Loss: 0.2002, LR: 0.000010, Grad Norm: 0.058125
1486.6s 236 Epoch 0, Step 1150, Loss: 0.2001, LR: 0.000010, Grad Norm: 0.058125
1486.9s 237 Epoch 0, Step 1150, Loss: 0.2002, LR: 0.000010, Grad Norm: 0.058125
1544.7s 238 Epoch 0, Step 1200, Loss: 0.1997, LR: 0.000010, Grad Norm: 0.055141
1544.9s 239 Epoch 0, Step 1200, Loss: 0.1996, LR: 0.000010, Grad Norm: 0.055141
1545.2s 240 Epoch 0, Step 1200, Loss: 0.1996, LR: 0.000010, Grad Norm: 0.055141
1545.5s 241 Epoch 0, Step 1200, Loss: 0.1996, LR: 0.000010, Grad Norm: 0.055141
1602.7s 242 Epoch 0, Step 1250, Loss: 0.1996, LR: 0.000010, Grad Norm: 0.109492
1602.9s 243 Epoch 0, Step 1250, Loss: 0.1996, LR: 0.000010, Grad Norm: 0.109492
1603.2s 244 Epoch 0, Step 1250, Loss: 0.1996, LR: 0.000010, Grad Norm: 0.109492
1603.5s 245 Epoch 0, Step 1250, Loss: 0.1996, LR: 0.000010, Grad Norm: 0.109492
1660.8s 246 Epoch 0, Step 1300, Loss: 0.1990, LR: 0.000010, Grad Norm: 0.116471
1661.1s 247 Epoch 0, Step 1300, Loss: 0.1989, LR: 0.000010, Grad Norm: 0.116471
1661.4s 248 Epoch 0, Step 1300, Loss: 0.1989, LR: 0.000010, Grad Norm: 0.116471
1661.7s 249 Epoch 0, Step 1300, Loss: 0.1989, LR: 0.000010, Grad Norm: 0.116471
1718.5s 250 Epoch 0, Step 1350, Loss: 0.1987, LR: 0.000010, Grad Norm: 0.067237
1718.8s 251 Epoch 0, Step 1350, Loss: 0.1987, LR: 0.000010, Grad Norm: 0.067237
1719.1s 252 Epoch 0, Step 1350, Loss: 0.1987, LR: 0.000010, Grad Norm: 0.067237
1719.6s 253 Epoch 0, Step 1350, Loss: 0.1987, LR: 0.000010, Grad Norm: 0.067237
1776.5s 254 Epoch 0, Step 1400, Loss: 0.1987, LR: 0.000010, Grad Norm: 0.108723
1776.8s 255 Epoch 0, Step 1400, Loss: 0.1987, LR: 0.000010, Grad Norm: 0.108723
1777.1s 256 Epoch 0, Step 1400, Loss: 0.1987, LR: 0.000010, Grad Norm: 0.108723
1777.3s 257 Epoch 0, Step 1400, Loss: 0.1987, LR: 0.000010, Grad Norm: 0.108723
1834.5s 258 Epoch 0, Step 1450, Loss: 0.1979, LR: 0.000010, Grad Norm: 0.121702
1834.7s 259 Epoch 0, Step 1450, Loss: 0.1979, LR: 0.000010, Grad Norm: 0.121702
1835.0s 260 Epoch 0, Step 1450, Loss: 0.1979, LR: 0.000010, Grad Norm: 0.121702
1835.3s 261 Epoch 0, Step 1450, Loss: 0.1978, LR: 0.000010, Grad Norm: 0.121702
1892.0s 262 Epoch 0, Step 1500, Loss: 0.1981, LR: 0.000010, Grad Norm: 0.115090
1892.3s 263 Epoch 0, Step 1500, Loss: 0.1981, LR: 0.000010, Grad Norm: 0.115090
1892.5s 264 Epoch 0, Step 1500, Loss: 0.1981, LR: 0.000010, Grad Norm: 0.115090
1892.8s 265 Epoch 0, Step 1500, Loss: 0.1981, LR: 0.000010, Grad Norm: 0.115090
1949.9s 266 Epoch 0, Step 1550, Loss: 0.1977, LR: 0.000010, Grad Norm: 0.079674
1950.2s 267 Epoch 0, Step 1550, Loss: 0.1976, LR: 0.000010, Grad Norm: 0.079674
1950.5s 268 Epoch 0, Step 1550, Loss: 0.1976, LR: 0.000010, Grad Norm: 0.079674
1950.7s 269 Epoch 0, Step 1550, Loss: 0.1976, LR: 0.000010, Grad Norm: 0.079674
2007.5s 270 Epoch 0, Step 1600, Loss: 0.1968, LR: 0.000010, Grad Norm: 0.046715
2007.8s 271 Epoch 0, Step 1600, Loss: 0.1968, LR: 0.000010, Grad Norm: 0.046715
2008.1s 272 Epoch 0, Step 1600, Loss: 0.1968, LR: 0.000010, Grad Norm: 0.046715
2008.3s 273 Epoch 0, Step 1600, Loss: 0.1968, LR: 0.000010, Grad Norm: 0.046715
2065.6s 274 Epoch 0, Step 1650, Loss: 0.1963, LR: 0.000010, Grad Norm: 0.080005
2065.8s 275 Epoch 0, Step 1650, Loss: 0.1963, LR: 0.000010, Grad Norm: 0.080005
2066.1s 276 Epoch 0, Step 1650, Loss: 0.1963, LR: 0.000010, Grad Norm: 0.080005
2066.4s 277 Epoch 0, Step 1650, Loss: 0.1963, LR: 0.000010, Grad Norm: 0.080005
2123.2s 278 Epoch 0, Step 1700, Loss: 0.1966, LR: 0.000010, Grad Norm: 0.118482
2123.5s 279 Epoch 0, Step 1700, Loss: 0.1966, LR: 0.000010, Grad Norm: 0.118482
2123.7s 280 Epoch 0, Step 1700, Loss: 0.1966, LR: 0.000010, Grad Norm: 0.118482
2124.0s 281 Epoch 0, Step 1700, Loss: 0.1965, LR: 0.000010, Grad Norm: 0.118482
2181.2s 282 Epoch 0, Step 1750, Loss: 0.1967, LR: 0.000010, Grad Norm: 0.075908
2181.5s 283 Epoch 0, Step 1750, Loss: 0.1967, LR: 0.000010, Grad Norm: 0.075908
2181.8s 284 Epoch 0, Step 1750, Loss: 0.1967, LR: 0.000010, Grad Norm: 0.075908
2182.1s 285 Epoch 0, Step 1750, Loss: 0.1967, LR: 0.000010, Grad Norm: 0.075908
2239.2s 286 Epoch 0, Step 1800, Loss: 0.1965, LR: 0.000010, Grad Norm: 0.119201
2239.6s 287 Epoch 0, Step 1800, Loss: 0.1965, LR: 0.000010, Grad Norm: 0.119201
2240.1s 288 Epoch 0, Step 1800, Loss: 0.1965, LR: 0.000010, Grad Norm: 0.119201
2240.3s 289 Epoch 0, Step 1800, Loss: 0.1965, LR: 0.000010, Grad Norm: 0.119201
2297.3s 290 Epoch 0, Step 1850, Loss: 0.1966, LR: 0.000010, Grad Norm: 0.139866
2297.5s 291 Epoch 0, Step 1850, Loss: 0.1965, LR: 0.000010, Grad Norm: 0.139866
2297.8s 292 Epoch 0, Step 1850, Loss: 0.1965, LR: 0.000010, Grad Norm: 0.139866
2298.1s 293 Epoch 0, Step 1850, Loss: 0.1965, LR: 0.000010, Grad Norm: 0.139866
2355.9s 294 Epoch 0, Step 1900, Loss: 0.1957, LR: 0.000010, Grad Norm: 0.096550
2356.2s 295 Epoch 0, Step 1900, Loss: 0.1957, LR: 0.000010, Grad Norm: 0.096550
2356.5s 296 Epoch 0, Step 1900, Loss: 0.1957, LR: 0.000010, Grad Norm: 0.096550
2356.8s 297 Epoch 0, Step 1900, Loss: 0.1957, LR: 0.000010, Grad Norm: 0.096550
2413.9s 298 Epoch 0, Step 1950, Loss: 0.1951, LR: 0.000010, Grad Norm: 0.041392
2414.1s 299 Epoch 0, Step 1950, Loss: 0.1951, LR: 0.000010, Grad Norm: 0.041392
2414.4s 300 Epoch 0, Step 1950, Loss: 0.1951, LR: 0.000010, Grad Norm: 0.041392
2414.7s 301 Epoch 0, Step 1950, Loss: 0.1951, LR: 0.000010, Grad Norm: 0.041392
2472.5s 302 Epoch 0, Step 2000, Loss: 0.1948, LR: 0.000010, Grad Norm: 0.074229
2472.7s 303 Epoch 0, Step 2000, Loss: 0.1947, LR: 0.000010, Grad Norm: 0.074229
2473.0s 304 Epoch 0, Step 2000, Loss: 0.1948, LR: 0.000010, Grad Norm: 0.074229
2473.3s 305 Epoch 0, Step 2000, Loss: 0.1948, LR: 0.000010, Grad Norm: 0.074229
2530.3s 306 Epoch 0, Step 2050, Loss: 0.1949, LR: 0.000010, Grad Norm: 0.114576
2530.5s 307 Epoch 0, Step 2050, Loss: 0.1949, LR: 0.000010, Grad Norm: 0.114576
2530.8s 308 Epoch 0, Step 2050, Loss: 0.1949, LR: 0.000010, Grad Norm: 0.114576
2531.1s 309 Epoch 0, Step 2050, Loss: 0.1948, LR: 0.000010, Grad Norm: 0.114576
2588.9s 310 Epoch 0, Step 2100, Loss: 0.1948, LR: 0.000010, Grad Norm: 0.106395
2589.1s 311 Epoch 0, Step 2100, Loss: 0.1948, LR: 0.000010, Grad Norm: 0.106395
2589.4s 312 Epoch 0, Step 2100, Loss: 0.1948, LR: 0.000010, Grad Norm: 0.106395
2589.7s 313 Epoch 0, Step 2100, Loss: 0.1948, LR: 0.000010, Grad Norm: 0.106395
2646.7s 314 Epoch 0, Step 2150, Loss: 0.1943, LR: 0.000010, Grad Norm: 0.119987
2647.0s 315 Epoch 0, Step 2150, Loss: 0.1943, LR: 0.000010, Grad Norm: 0.119987
2647.3s 316 Epoch 0, Step 2150, Loss: 0.1943, LR: 0.000010, Grad Norm: 0.119987
2647.5s 317 Epoch 0, Step 2150, Loss: 0.1943, LR: 0.000010, Grad Norm: 0.119987
2705.0s 318 Epoch 0, Step 2200, Loss: 0.1942, LR: 0.000010, Grad Norm: 0.093217
2705.2s 319 Epoch 0, Step 2200, Loss: 0.1942, LR: 0.000010, Grad Norm: 0.093217
2705.5s 320 Epoch 0, Step 2200, Loss: 0.1941, LR: 0.000010, Grad Norm: 0.093217
2705.8s 321 Epoch 0, Step 2200, Loss: 0.1942, LR: 0.000010, Grad Norm: 0.093217
2762.6s 322 Epoch 0, Step 2250, Loss: 0.1937, LR: 0.000010, Grad Norm: 0.048456
2868.4s 323 Epoch 0 completed. Train Loss: 0.1937, Validation Loss: 0.1842
2868.4s 324 Selected animals for validation CLIP score: ['dolphin', 'zebra', 'sandpiper', 'swan', 'pig']
2868.8s 325 Expected types for unet: (<class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionModel'>,), got <class 'peft.peft_model.PeftModel'>.
2868.8s 326 You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
2873.3s 327 Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
2877.8s 328 Animal: dolphin, CLIP Score: 29.2128
2880.5s 329 Animal: zebra, CLIP Score: 32.8276
2883.1s 330 Animal: sandpiper, CLIP Score: 30.2431
2885.6s 331 Animal: swan, CLIP Score: 28.6533
2888.2s 332 Animal: pig, CLIP Score: 29.8067
2888.2s 333 Average Validation CLIP Score: 30.1487
2888.2s 334 Saved best model with CLIP score: 30.1487
2888.3s 335 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2888.3s 336 To disable this warning, you can either:
2888.3s 337 - Avoid using `tokenizers` before the fork if possible
2888.3s 338 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2888.3s 339 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2888.3s 340 To disable this warning, you can either:
2888.3s 341 - Avoid using `tokenizers` before the fork if possible
2888.3s 342 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2888.3s 343 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2888.3s 344 To disable this warning, you can either:
2888.3s 345 - Avoid using `tokenizers` before the fork if possible
2888.3s 346 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2888.5s 347 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
2888.5s 348 To disable this warning, you can either:
2888.5s 349 - Avoid using `tokenizers` before the fork if possible
2888.5s 350 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2888.9s 351 Epoch 1, Step 2250, Loss: 0.5069, LR: 0.000010, Grad Norm: 0.000000
2889.2s 352 Epoch 1, Step 2250, Loss: 0.2574, LR: 0.000010, Grad Norm: 0.000000
2889.5s 353 Epoch 1, Step 2250, Loss: 0.1936, LR: 0.000010, Grad Norm: 0.000000
2947.1s 354 Epoch 1, Step 2300, Loss: 0.1453, LR: 0.000010, Grad Norm: 0.056933
2947.3s 355 Epoch 1, Step 2300, Loss: 0.1458, LR: 0.000010, Grad Norm: 0.056933
2947.6s 356 Epoch 1, Step 2300, Loss: 0.1462, LR: 0.000010, Grad Norm: 0.056933
2947.9s 357 Epoch 1, Step 2300, Loss: 0.1456, LR: 0.000010, Grad Norm: 0.056933
3006.2s 358 Epoch 1, Step 2350, Loss: 0.1734, LR: 0.000010, Grad Norm: 0.067604
3006.5s 359 Epoch 1, Step 2350, Loss: 0.1731, LR: 0.000010, Grad Norm: 0.067604
3006.8s 360 Epoch 1, Step 2350, Loss: 0.1734, LR: 0.000010, Grad Norm: 0.067604
3007.1s 361 Epoch 1, Step 2350, Loss: 0.1737, LR: 0.000010, Grad Norm: 0.067604
3065.0s 362 Epoch 1, Step 2400, Loss: 0.1725, LR: 0.000010, Grad Norm: 0.109154
3065.2s 363 Epoch 1, Step 2400, Loss: 0.1722, LR: 0.000010, Grad Norm: 0.109154
3065.5s 364 Epoch 1, Step 2400, Loss: 0.1726, LR: 0.000010, Grad Norm: 0.109154
3065.8s 365 Epoch 1, Step 2400, Loss: 0.1725, LR: 0.000010, Grad Norm: 0.109154
3124.0s 366 Epoch 1, Step 2450, Loss: 0.1687, LR: 0.000010, Grad Norm: 0.087786
3124.3s 367 Epoch 1, Step 2450, Loss: 0.1686, LR: 0.000010, Grad Norm: 0.087786
3124.6s 368 Epoch 1, Step 2450, Loss: 0.1685, LR: 0.000010, Grad Norm: 0.087786
3124.9s 369 Epoch 1, Step 2450, Loss: 0.1688, LR: 0.000010, Grad Norm: 0.087786
3182.9s 370 Epoch 1, Step 2500, Loss: 0.1720, LR: 0.000010, Grad Norm: 0.058082
3183.2s 371 Epoch 1, Step 2500, Loss: 0.1724, LR: 0.000010, Grad Norm: 0.058082
3183.5s 372 Epoch 1, Step 2500, Loss: 0.1722, LR: 0.000010, Grad Norm: 0.058082
3183.8s 373 Epoch 1, Step 2500, Loss: 0.1721, LR: 0.000010, Grad Norm: 0.058082
3242.3s 374 Epoch 1, Step 2550, Loss: 0.1747, LR: 0.000010, Grad Norm: 0.084682
3242.5s 375 Epoch 1, Step 2550, Loss: 0.1746, LR: 0.000010, Grad Norm: 0.084682
3242.8s 376 Epoch 1, Step 2550, Loss: 0.1744, LR: 0.000010, Grad Norm: 0.084682
3243.1s 377 Epoch 1, Step 2550, Loss: 0.1743, LR: 0.000010, Grad Norm: 0.084682
3301.1s 378 Epoch 1, Step 2600, Loss: 0.1760, LR: 0.000010, Grad Norm: 0.102654
3301.4s 379 Epoch 1, Step 2600, Loss: 0.1759, LR: 0.000010, Grad Norm: 0.102654
3301.7s 380 Epoch 1, Step 2600, Loss: 0.1758, LR: 0.000010, Grad Norm: 0.102654
3302.3s 381 Epoch 1, Step 2600, Loss: 0.1758, LR: 0.000010, Grad Norm: 0.102654
3360.3s 382 Epoch 1, Step 2650, Loss: 0.1790, LR: 0.000010, Grad Norm: 0.077117
3360.6s 383 Epoch 1, Step 2650, Loss: 0.1791, LR: 0.000010, Grad Norm: 0.077117
3360.9s 384 Epoch 1, Step 2650, Loss: 0.1791, LR: 0.000010, Grad Norm: 0.077117
3361.2s 385 Epoch 1, Step 2650, Loss: 0.1791, LR: 0.000010, Grad Norm: 0.077117
3419.9s 386 Epoch 1, Step 2700, Loss: 0.1791, LR: 0.000010, Grad Norm: 0.121449
3420.2s 387 Epoch 1, Step 2700, Loss: 0.1790, LR: 0.000010, Grad Norm: 0.121449
3420.5s 388 Epoch 1, Step 2700, Loss: 0.1789, LR: 0.000010, Grad Norm: 0.121449
3420.7s 389 Epoch 1, Step 2700, Loss: 0.1789, LR: 0.000010, Grad Norm: 0.121449
3478.8s 390 Epoch 1, Step 2750, Loss: 0.1811, LR: 0.000010, Grad Norm: 0.128610
3479.0s 391 Epoch 1, Step 2750, Loss: 0.1811, LR: 0.000010, Grad Norm: 0.128610
3479.3s 392 Epoch 1, Step 2750, Loss: 0.1810, LR: 0.000010, Grad Norm: 0.128610
3479.6s 393 Epoch 1, Step 2750, Loss: 0.1809, LR: 0.000010, Grad Norm: 0.128610
3538.0s 394 Epoch 1, Step 2800, Loss: 0.1822, LR: 0.000010, Grad Norm: 0.068548
3538.3s 395 Epoch 1, Step 2800, Loss: 0.1822, LR: 0.000010, Grad Norm: 0.068548
3538.6s 396 Epoch 1, Step 2800, Loss: 0.1822, LR: 0.000010, Grad Norm: 0.068548
3538.9s 397 Epoch 1, Step 2800, Loss: 0.1821, LR: 0.000010, Grad Norm: 0.068548
3596.7s 398 Epoch 1, Step 2850, Loss: 0.1830, LR: 0.000010, Grad Norm: 0.081058
3597.0s 399 Epoch 1, Step 2850, Loss: 0.1830, LR: 0.000010, Grad Norm: 0.081058
3597.2s 400 Epoch 1, Step 2850, Loss: 0.1831, LR: 0.000010, Grad Norm: 0.081058
3597.5s 401 Epoch 1, Step 2850, Loss: 0.1832, LR: 0.000010, Grad Norm: 0.081058
3655.7s 402 Epoch 1, Step 2900, Loss: 0.1832, LR: 0.000010, Grad Norm: 0.124446
3656.0s 403 Epoch 1, Step 2900, Loss: 0.1832, LR: 0.000010, Grad Norm: 0.124446
3656.3s 404 Epoch 1, Step 2900, Loss: 0.1833, LR: 0.000010, Grad Norm: 0.124446
3656.6s 405 Epoch 1, Step 2900, Loss: 0.1833, LR: 0.000010, Grad Norm: 0.124446
3714.4s 406 Epoch 1, Step 2950, Loss: 0.1853, LR: 0.000010, Grad Norm: 0.012948
3714.7s 407 Epoch 1, Step 2950, Loss: 0.1853, LR: 0.000010, Grad Norm: 0.012948
3715.0s 408 Epoch 1, Step 2950, Loss: 0.1853, LR: 0.000010, Grad Norm: 0.012948
3715.3s 409 Epoch 1, Step 2950, Loss: 0.1852, LR: 0.000010, Grad Norm: 0.012948
3773.6s 410 Epoch 1, Step 3000, Loss: 0.1864, LR: 0.000010, Grad Norm: 0.065271
3773.8s 411 Epoch 1, Step 3000, Loss: 0.1864, LR: 0.000010, Grad Norm: 0.065271
3774.1s 412 Epoch 1, Step 3000, Loss: 0.1864, LR: 0.000010, Grad Norm: 0.065271
3774.4s 413 Epoch 1, Step 3000, Loss: 0.1865, LR: 0.000010, Grad Norm: 0.065271
3832.6s 414 Epoch 1, Step 3050, Loss: 0.1871, LR: 0.000010, Grad Norm: 0.080130
3833.0s 415 Epoch 1, Step 3050, Loss: 0.1871, LR: 0.000010, Grad Norm: 0.080130
3833.5s 416 Epoch 1, Step 3050, Loss: 0.1871, LR: 0.000010, Grad Norm: 0.080130
3833.7s 417 Epoch 1, Step 3050, Loss: 0.1871, LR: 0.000010, Grad Norm: 0.080130
3892.1s 418 Epoch 1, Step 3100, Loss: 0.1862, LR: 0.000010, Grad Norm: 0.060407
3892.4s 419 Epoch 1, Step 3100, Loss: 0.1862, LR: 0.000010, Grad Norm: 0.060407
3892.7s 420 Epoch 1, Step 3100, Loss: 0.1861, LR: 0.000010, Grad Norm: 0.060407
3892.9s 421 Epoch 1, Step 3100, Loss: 0.1863, LR: 0.000010, Grad Norm: 0.060407
3951.5s 422 Epoch 1, Step 3150, Loss: 0.1870, LR: 0.000010, Grad Norm: 0.113678
3951.7s 423 Epoch 1, Step 3150, Loss: 0.1870, LR: 0.000010, Grad Norm: 0.113678
3952.0s 424 Epoch 1, Step 3150, Loss: 0.1869, LR: 0.000010, Grad Norm: 0.113678
3952.3s 425 Epoch 1, Step 3150, Loss: 0.1869, LR: 0.000010, Grad Norm: 0.113678
4010.9s 426 Epoch 1, Step 3200, Loss: 0.1860, LR: 0.000010, Grad Norm: 0.083956
4011.2s 427 Epoch 1, Step 3200, Loss: 0.1860, LR: 0.000010, Grad Norm: 0.083956
4011.5s 428 Epoch 1, Step 3200, Loss: 0.1859, LR: 0.000010, Grad Norm: 0.083956
4011.8s 429 Epoch 1, Step 3200, Loss: 0.1859, LR: 0.000010, Grad Norm: 0.083956
4070.6s 430 Epoch 1, Step 3250, Loss: 0.1866, LR: 0.000010, Grad Norm: 0.093275
4070.8s 431 Epoch 1, Step 3250, Loss: 0.1866, LR: 0.000010, Grad Norm: 0.093275
4071.2s 432 Epoch 1, Step 3250, Loss: 0.1866, LR: 0.000010, Grad Norm: 0.093275
4071.4s 433 Epoch 1, Step 3250, Loss: 0.1867, LR: 0.000010, Grad Norm: 0.093275
4130.0s 434 Epoch 1, Step 3300, Loss: 0.1868, LR: 0.000010, Grad Norm: 0.097569
4130.2s 435 Epoch 1, Step 3300, Loss: 0.1868, LR: 0.000010, Grad Norm: 0.097569
4130.5s 436 Epoch 1, Step 3300, Loss: 0.1868, LR: 0.000010, Grad Norm: 0.097569
4130.8s 437 Epoch 1, Step 3300, Loss: 0.1868, LR: 0.000010, Grad Norm: 0.097569
4189.7s 438 Epoch 1, Step 3350, Loss: 0.1875, LR: 0.000010, Grad Norm: 0.088030
4190.0s 439 Epoch 1, Step 3350, Loss: 0.1875, LR: 0.000010, Grad Norm: 0.088030
4190.3s 440 Epoch 1, Step 3350, Loss: 0.1874, LR: 0.000010, Grad Norm: 0.088030
4190.6s 441 Epoch 1, Step 3350, Loss: 0.1875, LR: 0.000010, Grad Norm: 0.088030
4249.2s 442 Epoch 1, Step 3400, Loss: 0.1878, LR: 0.000010, Grad Norm: 0.097142
4249.5s 443 Epoch 1, Step 3400, Loss: 0.1878, LR: 0.000010, Grad Norm: 0.097142
4249.8s 444 Epoch 1, Step 3400, Loss: 0.1878, LR: 0.000010, Grad Norm: 0.097142
4250.1s 445 Epoch 1, Step 3400, Loss: 0.1878, LR: 0.000010, Grad Norm: 0.097142
4309.1s 446 Epoch 1, Step 3450, Loss: 0.1873, LR: 0.000009, Grad Norm: 0.108496
4309.3s 447 Epoch 1, Step 3450, Loss: 0.1873, LR: 0.000009, Grad Norm: 0.108496
4309.6s 448 Epoch 1, Step 3450, Loss: 0.1873, LR: 0.000009, Grad Norm: 0.108496
4309.9s 449 Epoch 1, Step 3450, Loss: 0.1873, LR: 0.000009, Grad Norm: 0.108496
4368.9s 450 Epoch 1, Step 3500, Loss: 0.1873, LR: 0.000009, Grad Norm: 0.058821
4369.2s 451 Epoch 1, Step 3500, Loss: 0.1872, LR: 0.000009, Grad Norm: 0.058821
4369.5s 452 Epoch 1, Step 3500, Loss: 0.1872, LR: 0.000009, Grad Norm: 0.058821
4369.8s 453 Epoch 1, Step 3500, Loss: 0.1872, LR: 0.000009, Grad Norm: 0.058821
4428.3s 454 Epoch 1, Step 3550, Loss: 0.1877, LR: 0.000009, Grad Norm: 0.073811
4428.6s 455 Epoch 1, Step 3550, Loss: 0.1878, LR: 0.000009, Grad Norm: 0.073811
4428.8s 456 Epoch 1, Step 3550, Loss: 0.1878, LR: 0.000009, Grad Norm: 0.073811
4429.1s 457 Epoch 1, Step 3550, Loss: 0.1878, LR: 0.000009, Grad Norm: 0.073811
4488.2s 458 Epoch 1, Step 3600, Loss: 0.1870, LR: 0.000009, Grad Norm: 0.093318
4488.4s 459 Epoch 1, Step 3600, Loss: 0.1870, LR: 0.000009, Grad Norm: 0.093318
4488.7s 460 Epoch 1, Step 3600, Loss: 0.1870, LR: 0.000009, Grad Norm: 0.093318
4489.0s 461 Epoch 1, Step 3600, Loss: 0.1871, LR: 0.000009, Grad Norm: 0.093318
4547.5s 462 Epoch 1, Step 3650, Loss: 0.1868, LR: 0.000009, Grad Norm: 0.042420
4547.7s 463 Epoch 1, Step 3650, Loss: 0.1868, LR: 0.000009, Grad Norm: 0.042420
4548.0s 464 Epoch 1, Step 3650, Loss: 0.1869, LR: 0.000009, Grad Norm: 0.042420
4548.3s 465 Epoch 1, Step 3650, Loss: 0.1869, LR: 0.000009, Grad Norm: 0.042420
4607.2s 466 Epoch 1, Step 3700, Loss: 0.1863, LR: 0.000009, Grad Norm: 0.098613
4607.5s 467 Epoch 1, Step 3700, Loss: 0.1862, LR: 0.000009, Grad Norm: 0.098613
4607.8s 468 Epoch 1, Step 3700, Loss: 0.1863, LR: 0.000009, Grad Norm: 0.098613
4608.0s 469 Epoch 1, Step 3700, Loss: 0.1863, LR: 0.000009, Grad Norm: 0.098613
4666.5s 470 Epoch 1, Step 3750, Loss: 0.1867, LR: 0.000009, Grad Norm: 0.117033
4666.8s 471 Epoch 1, Step 3750, Loss: 0.1866, LR: 0.000009, Grad Norm: 0.117033
4667.1s 472 Epoch 1, Step 3750, Loss: 0.1866, LR: 0.000009, Grad Norm: 0.117033
4667.3s 473 Epoch 1, Step 3750, Loss: 0.1866, LR: 0.000009, Grad Norm: 0.117033
4726.4s 474 Epoch 1, Step 3800, Loss: 0.1859, LR: 0.000009, Grad Norm: 0.057055
4726.7s 475 Epoch 1, Step 3800, Loss: 0.1859, LR: 0.000009, Grad Norm: 0.057055
4727.0s 476 Epoch 1, Step 3800, Loss: 0.1859, LR: 0.000009, Grad Norm: 0.057055
4727.3s 477 Epoch 1, Step 3800, Loss: 0.1859, LR: 0.000009, Grad Norm: 0.057055
4785.4s 478 Epoch 1, Step 3850, Loss: 0.1861, LR: 0.000009, Grad Norm: 0.075365
4785.7s 479 Epoch 1, Step 3850, Loss: 0.1861, LR: 0.000009, Grad Norm: 0.075365
4786.0s 480 Epoch 1, Step 3850, Loss: 0.1862, LR: 0.000009, Grad Norm: 0.075365
4786.2s 481 Epoch 1, Step 3850, Loss: 0.1861, LR: 0.000009, Grad Norm: 0.075365
4845.1s 482 Epoch 1, Step 3900, Loss: 0.1857, LR: 0.000009, Grad Norm: 0.018659
4845.3s 483 Epoch 1, Step 3900, Loss: 0.1858, LR: 0.000009, Grad Norm: 0.018659
4845.6s 484 Epoch 1, Step 3900, Loss: 0.1858, LR: 0.000009, Grad Norm: 0.018659
4845.9s 485 Epoch 1, Step 3900, Loss: 0.1858, LR: 0.000009, Grad Norm: 0.018659
4904.2s 486 Epoch 1, Step 3950, Loss: 0.1859, LR: 0.000009, Grad Norm: 0.109480
4904.7s 487 Epoch 1, Step 3950, Loss: 0.1858, LR: 0.000009, Grad Norm: 0.109480
4905.1s 488 Epoch 1, Step 3950, Loss: 0.1858, LR: 0.000009, Grad Norm: 0.109480
4905.4s 489 Epoch 1, Step 3950, Loss: 0.1858, LR: 0.000009, Grad Norm: 0.109480
4963.8s 490 Epoch 1, Step 4000, Loss: 0.1856, LR: 0.000009, Grad Norm: 0.083411
4964.0s 491 Epoch 1, Step 4000, Loss: 0.1856, LR: 0.000009, Grad Norm: 0.083411
4964.3s 492 Epoch 1, Step 4000, Loss: 0.1856, LR: 0.000009, Grad Norm: 0.083411
4964.6s 493 Epoch 1, Step 4000, Loss: 0.1855, LR: 0.000009, Grad Norm: 0.083411
5023.3s 494 Epoch 1, Step 4050, Loss: 0.1854, LR: 0.000009, Grad Norm: 0.094185
5023.6s 495 Epoch 1, Step 4050, Loss: 0.1854, LR: 0.000009, Grad Norm: 0.094185
5023.9s 496 Epoch 1, Step 4050, Loss: 0.1854, LR: 0.000009, Grad Norm: 0.094185
5024.2s 497 Epoch 1, Step 4050, Loss: 0.1854, LR: 0.000009, Grad Norm: 0.094185
5082.4s 498 Epoch 1, Step 4100, Loss: 0.1857, LR: 0.000009, Grad Norm: 0.103969
5082.6s 499 Epoch 1, Step 4100, Loss: 0.1857, LR: 0.000009, Grad Norm: 0.103969
5082.9s 500 Epoch 1, Step 4100, Loss: 0.1857, LR: 0.000009, Grad Norm: 0.103969
5083.2s 501 Epoch 1, Step 4100, Loss: 0.1858, LR: 0.000009, Grad Norm: 0.103969
5141.9s 502 Epoch 1, Step 4150, Loss: 0.1853, LR: 0.000009, Grad Norm: 0.088062
5142.1s 503 Epoch 1, Step 4150, Loss: 0.1853, LR: 0.000009, Grad Norm: 0.088062
5142.4s 504 Epoch 1, Step 4150, Loss: 0.1854, LR: 0.000009, Grad Norm: 0.088062
5142.7s 505 Epoch 1, Step 4150, Loss: 0.1853, LR: 0.000009, Grad Norm: 0.088062
5200.8s 506 Epoch 1, Step 4200, Loss: 0.1858, LR: 0.000009, Grad Norm: 0.120091
5201.0s 507 Epoch 1, Step 4200, Loss: 0.1858, LR: 0.000009, Grad Norm: 0.120091
5201.3s 508 Epoch 1, Step 4200, Loss: 0.1858, LR: 0.000009, Grad Norm: 0.120091
5201.6s 509 Epoch 1, Step 4200, Loss: 0.1858, LR: 0.000009, Grad Norm: 0.120091
5260.1s 510 Epoch 1, Step 4250, Loss: 0.1867, LR: 0.000009, Grad Norm: 0.130397
5260.3s 511 Epoch 1, Step 4250, Loss: 0.1867, LR: 0.000009, Grad Norm: 0.130397
5260.6s 512 Epoch 1, Step 4250, Loss: 0.1867, LR: 0.000009, Grad Norm: 0.130397
5260.9s 513 Epoch 1, Step 4250, Loss: 0.1867, LR: 0.000009, Grad Norm: 0.130397
5319.1s 514 Epoch 1, Step 4300, Loss: 0.1866, LR: 0.000009, Grad Norm: 0.123066
5319.4s 515 Epoch 1, Step 4300, Loss: 0.1866, LR: 0.000009, Grad Norm: 0.123066
5319.7s 516 Epoch 1, Step 4300, Loss: 0.1866, LR: 0.000009, Grad Norm: 0.123066
5320.0s 517 Epoch 1, Step 4300, Loss: 0.1866, LR: 0.000009, Grad Norm: 0.123066
5378.5s 518 Epoch 1, Step 4350, Loss: 0.1867, LR: 0.000009, Grad Norm: 0.111719
5378.7s 519 Epoch 1, Step 4350, Loss: 0.1867, LR: 0.000009, Grad Norm: 0.111719
5379.0s 520 Epoch 1, Step 4350, Loss: 0.1867, LR: 0.000009, Grad Norm: 0.111719
5379.3s 521 Epoch 1, Step 4350, Loss: 0.1867, LR: 0.000009, Grad Norm: 0.111719
5437.9s 522 Epoch 1, Step 4400, Loss: 0.1866, LR: 0.000009, Grad Norm: 0.097600
5438.1s 523 Epoch 1, Step 4400, Loss: 0.1866, LR: 0.000009, Grad Norm: 0.097600
5438.4s 524 Epoch 1, Step 4400, Loss: 0.1866, LR: 0.000009, Grad Norm: 0.097600
5438.7s 525 Epoch 1, Step 4400, Loss: 0.1866, LR: 0.000009, Grad Norm: 0.097600
5497.1s 526 Epoch 1, Step 4450, Loss: 0.1864, LR: 0.000009, Grad Norm: 0.099942
5497.4s 527 Epoch 1, Step 4450, Loss: 0.1864, LR: 0.000009, Grad Norm: 0.099942
5497.7s 528 Epoch 1, Step 4450, Loss: 0.1865, LR: 0.000009, Grad Norm: 0.099942
5498.0s 529 Epoch 1, Step 4450, Loss: 0.1865, LR: 0.000009, Grad Norm: 0.099942
5556.5s 530 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
5556.5s 531 To disable this warning, you can either:
5556.5s 532 - Avoid using `tokenizers` before the fork if possible
5556.5s 533 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
5556.5s 534 Epoch 1, Step 4500, Loss: 0.1869, LR: 0.000009, Grad Norm: 0.100506
5556.6s 535 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
5556.6s 536 To disable this warning, you can either:
5556.6s 537 - Avoid using `tokenizers` before the fork if possible
5556.6s 538 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
5556.6s 539 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
5556.6s 540 To disable this warning, you can either:
5556.6s 541 - Avoid using `tokenizers` before the fork if possible
5556.6s 542 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
5556.8s 543 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
5556.8s 544 To disable this warning, you can either:
5556.8s 545 - Avoid using `tokenizers` before the fork if possible
5556.8s 546 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
5663.1s 547 Epoch 1 completed. Train Loss: 0.1869, Validation Loss: 0.1927
5663.1s 548 Selected animals for validation CLIP score: ['antelope', 'tiger', 'grasshopper', 'mosquito', 'sparrow']
5663.1s 549 Expected types for unet: (<class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionModel'>,), got <class 'peft.peft_model.PeftModel'>.
5663.1s 550 You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
5667.4s 551 Animal: antelope, CLIP Score: 31.7694
5670.0s 552 Animal: tiger, CLIP Score: 31.6541
5672.6s 553 Animal: grasshopper, CLIP Score: 31.8977
5675.2s 554 Animal: mosquito, CLIP Score: 29.9118
5677.8s 555 Animal: sparrow, CLIP Score: 31.8546
5677.8s 556 Average Validation CLIP Score: 31.4175
5677.8s 557 Saved best model with CLIP score: 31.4175
5677.8s 558 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
5677.8s 559 To disable this warning, you can either:
5677.8s 560 - Avoid using `tokenizers` before the fork if possible
5677.8s 561 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
5677.9s 562 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
5677.9s 563 To disable this warning, you can either:
5677.9s 564 - Avoid using `tokenizers` before the fork if possible
5677.9s 565 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
5677.9s 566 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
5677.9s 567 To disable this warning, you can either:
5677.9s 568 - Avoid using `tokenizers` before the fork if possible
5677.9s 569 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
5678.0s 570 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
5678.0s 571 To disable this warning, you can either:
5678.0s 572 - Avoid using `tokenizers` before the fork if possible
5678.0s 573 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
5678.5s 574 Epoch 2, Step 4500, Loss: 0.1893, LR: 0.000009, Grad Norm: 0.000000
5678.7s 575 Epoch 2, Step 4500, Loss: 0.3273, LR: 0.000009, Grad Norm: 0.000000
5679.0s 576 Epoch 2, Step 4500, Loss: 0.2316, LR: 0.000009, Grad Norm: 0.000000
5737.8s 577 Epoch 2, Step 4550, Loss: 0.1857, LR: 0.000009, Grad Norm: 0.046415
5738.0s 578 Epoch 2, Step 4550, Loss: 0.1863, LR: 0.000009, Grad Norm: 0.046415
5738.3s 579 Epoch 2, Step 4550, Loss: 0.1868, LR: 0.000009, Grad Norm: 0.046415
5738.6s 580 Epoch 2, Step 4550, Loss: 0.1859, LR: 0.000009, Grad Norm: 0.046415
5797.3s 581 Epoch 2, Step 4600, Loss: 0.1725, LR: 0.000009, Grad Norm: 0.092604
5797.6s 582 Epoch 2, Step 4600, Loss: 0.1724, LR: 0.000009, Grad Norm: 0.092604
5797.9s 583 Epoch 2, Step 4600, Loss: 0.1721, LR: 0.000009, Grad Norm: 0.092604
5798.1s 584 Epoch 2, Step 4600, Loss: 0.1725, LR: 0.000009, Grad Norm: 0.092604
5856.5s 585 Epoch 2, Step 4650, Loss: 0.1687, LR: 0.000009, Grad Norm: 0.061543
5856.7s 586 Epoch 2, Step 4650, Loss: 0.1684, LR: 0.000009, Grad Norm: 0.061543
5857.0s 587 Epoch 2, Step 4650, Loss: 0.1683, LR: 0.000009, Grad Norm: 0.061543
5857.3s 588 Epoch 2, Step 4650, Loss: 0.1680, LR: 0.000009, Grad Norm: 0.061543
5916.1s 589 Epoch 2, Step 4700, Loss: 0.1695, LR: 0.000009, Grad Norm: 0.126087
5916.3s 590 Epoch 2, Step 4700, Loss: 0.1705, LR: 0.000009, Grad Norm: 0.126087
5916.6s 591 Epoch 2, Step 4700, Loss: 0.1703, LR: 0.000009, Grad Norm: 0.126087
5916.9s 592 Epoch 2, Step 4700, Loss: 0.1705, LR: 0.000009, Grad Norm: 0.126087
5975.4s 593 Epoch 2, Step 4750, Loss: 0.1788, LR: 0.000009, Grad Norm: 0.107087
5975.7s 594 Epoch 2, Step 4750, Loss: 0.1789, LR: 0.000009, Grad Norm: 0.107087
5976.0s 595 Epoch 2, Step 4750, Loss: 0.1788, LR: 0.000009, Grad Norm: 0.107087
5976.2s 596 Epoch 2, Step 4750, Loss: 0.1788, LR: 0.000009, Grad Norm: 0.107087
6035.1s 597 Epoch 2, Step 4800, Loss: 0.1825, LR: 0.000009, Grad Norm: 0.097567
6035.3s 598 Epoch 2, Step 4800, Loss: 0.1825, LR: 0.000009, Grad Norm: 0.097567
6035.6s 599 Epoch 2, Step 4800, Loss: 0.1823, LR: 0.000009, Grad Norm: 0.097567
6035.9s 600 Epoch 2, Step 4800, Loss: 0.1824, LR: 0.000009, Grad Norm: 0.097567
6094.5s 601 Epoch 2, Step 4850, Loss: 0.1804, LR: 0.000009, Grad Norm: 0.050027
6094.7s 602 Epoch 2, Step 4850, Loss: 0.1803, LR: 0.000009, Grad Norm: 0.050027
6095.3s 603 Epoch 2, Step 4850, Loss: 0.1802, LR: 0.000009, Grad Norm: 0.050027
6095.7s 604 Epoch 2, Step 4850, Loss: 0.1801, LR: 0.000009, Grad Norm: 0.050027
6154.1s 605 Epoch 2, Step 4900, Loss: 0.1788, LR: 0.000009, Grad Norm: 0.087796
6154.4s 606 Epoch 2, Step 4900, Loss: 0.1792, LR: 0.000009, Grad Norm: 0.087796
6154.7s 607 Epoch 2, Step 4900, Loss: 0.1791, LR: 0.000009, Grad Norm: 0.087796
6154.9s 608 Epoch 2, Step 4900, Loss: 0.1790, LR: 0.000009, Grad Norm: 0.087796
6213.4s 609 Epoch 2, Step 4950, Loss: 0.1816, LR: 0.000009, Grad Norm: 0.060036
6213.8s 610 Epoch 2, Step 4950, Loss: 0.1816, LR: 0.000009, Grad Norm: 0.060036
6214.3s 611 Epoch 2, Step 4950, Loss: 0.1818, LR: 0.000009, Grad Norm: 0.060036
6214.6s 612 Epoch 2, Step 4950, Loss: 0.1817, LR: 0.000009, Grad Norm: 0.060036
6273.0s 613 Epoch 2, Step 5000, Loss: 0.1819, LR: 0.000009, Grad Norm: 0.078493
6273.2s 614 Epoch 2, Step 5000, Loss: 0.1819, LR: 0.000009, Grad Norm: 0.078493
6273.5s 615 Epoch 2, Step 5000, Loss: 0.1819, LR: 0.000009, Grad Norm: 0.078493
6273.8s 616 Epoch 2, Step 5000, Loss: 0.1822, LR: 0.000009, Grad Norm: 0.078493
6333.7s 617 Epoch 2, Step 5050, Loss: 0.1822, LR: 0.000009, Grad Norm: 0.179751
6334.0s 618 Epoch 2, Step 5050, Loss: 0.1823, LR: 0.000009, Grad Norm: 0.179751
6334.2s 619 Epoch 2, Step 5050, Loss: 0.1822, LR: 0.000009, Grad Norm: 0.179751
6334.5s 620 Epoch 2, Step 5050, Loss: 0.1822, LR: 0.000009, Grad Norm: 0.179751
6393.7s 621 Epoch 2, Step 5100, Loss: 0.1802, LR: 0.000009, Grad Norm: 0.053791
6393.9s 622 Epoch 2, Step 5100, Loss: 0.1803, LR: 0.000009, Grad Norm: 0.053791
6394.2s 623 Epoch 2, Step 5100, Loss: 0.1802, LR: 0.000009, Grad Norm: 0.053791
6394.5s 624 Epoch 2, Step 5100, Loss: 0.1802, LR: 0.000009, Grad Norm: 0.053791
6453.9s 625 Epoch 2, Step 5150, Loss: 0.1809, LR: 0.000009, Grad Norm: 0.029046
6454.2s 626 Epoch 2, Step 5150, Loss: 0.1809, LR: 0.000009, Grad Norm: 0.029046
6454.4s 627 Epoch 2, Step 5150, Loss: 0.1809, LR: 0.000009, Grad Norm: 0.029046
6454.7s 628 Epoch 2, Step 5150, Loss: 0.1809, LR: 0.000009, Grad Norm: 0.029046
6513.4s 629 Epoch 2, Step 5200, Loss: 0.1812, LR: 0.000009, Grad Norm: 0.088725
6513.7s 630 Epoch 2, Step 5200, Loss: 0.1811, LR: 0.000009, Grad Norm: 0.088725
6514.0s 631 Epoch 2, Step 5200, Loss: 0.1811, LR: 0.000009, Grad Norm: 0.088725
6514.2s 632 Epoch 2, Step 5200, Loss: 0.1810, LR: 0.000009, Grad Norm: 0.088725
6573.2s 633 Epoch 2, Step 5250, Loss: 0.1831, LR: 0.000009, Grad Norm: 0.135197
6573.4s 634 Epoch 2, Step 5250, Loss: 0.1832, LR: 0.000009, Grad Norm: 0.135197
6573.7s 635 Epoch 2, Step 5250, Loss: 0.1832, LR: 0.000009, Grad Norm: 0.135197
6574.0s 636 Epoch 2, Step 5250, Loss: 0.1833, LR: 0.000009, Grad Norm: 0.135197
6632.7s 637 Epoch 2, Step 5300, Loss: 0.1842, LR: 0.000009, Grad Norm: 0.127700
6632.9s 638 Epoch 2, Step 5300, Loss: 0.1841, LR: 0.000009, Grad Norm: 0.127700
6633.2s 639 Epoch 2, Step 5300, Loss: 0.1843, LR: 0.000009, Grad Norm: 0.127700
6633.5s 640 Epoch 2, Step 5300, Loss: 0.1842, LR: 0.000009, Grad Norm: 0.127700
6692.6s 641 Epoch 2, Step 5350, Loss: 0.1829, LR: 0.000009, Grad Norm: 0.070654
6692.8s 642 Epoch 2, Step 5350, Loss: 0.1828, LR: 0.000009, Grad Norm: 0.070654
6693.1s 643 Epoch 2, Step 5350, Loss: 0.1829, LR: 0.000009, Grad Norm: 0.070654
6693.4s 644 Epoch 2, Step 5350, Loss: 0.1829, LR: 0.000009, Grad Norm: 0.070654
6751.9s 645 Epoch 2, Step 5400, Loss: 0.1832, LR: 0.000009, Grad Norm: 0.071386
6752.2s 646 Epoch 2, Step 5400, Loss: 0.1832, LR: 0.000009, Grad Norm: 0.071386
6752.5s 647 Epoch 2, Step 5400, Loss: 0.1834, LR: 0.000009, Grad Norm: 0.071386
6752.7s 648 Epoch 2, Step 5400, Loss: 0.1834, LR: 0.000009, Grad Norm: 0.071386
6811.7s 649 Epoch 2, Step 5450, Loss: 0.1820, LR: 0.000009, Grad Norm: 0.068416
6811.9s 650 Epoch 2, Step 5450, Loss: 0.1821, LR: 0.000009, Grad Norm: 0.068416
6812.2s 651 Epoch 2, Step 5450, Loss: 0.1823, LR: 0.000009, Grad Norm: 0.068416
6812.5s 652 Epoch 2, Step 5450, Loss: 0.1822, LR: 0.000009, Grad Norm: 0.068416
6871.0s 653 Epoch 2, Step 5500, Loss: 0.1812, LR: 0.000009, Grad Norm: 0.068675
6871.3s 654 Epoch 2, Step 5500, Loss: 0.1812, LR: 0.000009, Grad Norm: 0.068675
6871.9s 655 Epoch 2, Step 5500, Loss: 0.1811, LR: 0.000009, Grad Norm: 0.068675
6872.2s 656 Epoch 2, Step 5500, Loss: 0.1811, LR: 0.000009, Grad Norm: 0.068675
6930.7s 657 Epoch 2, Step 5550, Loss: 0.1812, LR: 0.000009, Grad Norm: 0.087629
6930.9s 658 Epoch 2, Step 5550, Loss: 0.1811, LR: 0.000009, Grad Norm: 0.087629
6931.2s 659 Epoch 2, Step 5550, Loss: 0.1811, LR: 0.000009, Grad Norm: 0.087629
6931.5s 660 Epoch 2, Step 5550, Loss: 0.1811, LR: 0.000009, Grad Norm: 0.087629
6990.4s 661 Epoch 2, Step 5600, Loss: 0.1813, LR: 0.000009, Grad Norm: 0.098502
6990.7s 662 Epoch 2, Step 5600, Loss: 0.1813, LR: 0.000009, Grad Norm: 0.098502
6991.0s 663 Epoch 2, Step 5600, Loss: 0.1813, LR: 0.000009, Grad Norm: 0.098502
6991.3s 664 Epoch 2, Step 5600, Loss: 0.1814, LR: 0.000009, Grad Norm: 0.098502
7049.7s 665 Epoch 2, Step 5650, Loss: 0.1812, LR: 0.000009, Grad Norm: 0.097519
7049.9s 666 Epoch 2, Step 5650, Loss: 0.1812, LR: 0.000009, Grad Norm: 0.097519
7050.2s 667 Epoch 2, Step 5650, Loss: 0.1813, LR: 0.000009, Grad Norm: 0.097519
7050.5s 668 Epoch 2, Step 5650, Loss: 0.1813, LR: 0.000009, Grad Norm: 0.097519
7109.5s 669 Epoch 2, Step 5700, Loss: 0.1806, LR: 0.000009, Grad Norm: 0.082712
7109.7s 670 Epoch 2, Step 5700, Loss: 0.1807, LR: 0.000009, Grad Norm: 0.082712
7110.0s 671 Epoch 2, Step 5700, Loss: 0.1807, LR: 0.000009, Grad Norm: 0.082712
7110.3s 672 Epoch 2, Step 5700, Loss: 0.1806, LR: 0.000009, Grad Norm: 0.082712
7168.6s 673 Epoch 2, Step 5750, Loss: 0.1804, LR: 0.000009, Grad Norm: 0.089117
7168.8s 674 Epoch 2, Step 5750, Loss: 0.1803, LR: 0.000009, Grad Norm: 0.089117
7169.1s 675 Epoch 2, Step 5750, Loss: 0.1803, LR: 0.000009, Grad Norm: 0.089117
7169.4s 676 Epoch 2, Step 5750, Loss: 0.1804, LR: 0.000009, Grad Norm: 0.089117
7228.2s 677 Epoch 2, Step 5800, Loss: 0.1809, LR: 0.000009, Grad Norm: 0.054826
7228.5s 678 Epoch 2, Step 5800, Loss: 0.1809, LR: 0.000009, Grad Norm: 0.054826
7228.8s 679 Epoch 2, Step 5800, Loss: 0.1809, LR: 0.000009, Grad Norm: 0.054826
7229.0s 680 Epoch 2, Step 5800, Loss: 0.1809, LR: 0.000009, Grad Norm: 0.054826
7287.4s 681 Epoch 2, Step 5850, Loss: 0.1810, LR: 0.000008, Grad Norm: 0.068072
7287.6s 682 Epoch 2, Step 5850, Loss: 0.1810, LR: 0.000008, Grad Norm: 0.068072
7287.9s 683 Epoch 2, Step 5850, Loss: 0.1810, LR: 0.000008, Grad Norm: 0.068072
7288.2s 684 Epoch 2, Step 5850, Loss: 0.1810, LR: 0.000008, Grad Norm: 0.068072
7347.0s 685 Epoch 2, Step 5900, Loss: 0.1800, LR: 0.000008, Grad Norm: 0.102781
7347.2s 686 Epoch 2, Step 5900, Loss: 0.1800, LR: 0.000008, Grad Norm: 0.102781
7347.5s 687 Epoch 2, Step 5900, Loss: 0.1800, LR: 0.000008, Grad Norm: 0.102781
7347.8s 688 Epoch 2, Step 5900, Loss: 0.1800, LR: 0.000008, Grad Norm: 0.102781
7406.0s 689 Epoch 2, Step 5950, Loss: 0.1801, LR: 0.000008, Grad Norm: 0.108870
7406.3s 690 Epoch 2, Step 5950, Loss: 0.1801, LR: 0.000008, Grad Norm: 0.108870
7406.5s 691 Epoch 2, Step 5950, Loss: 0.1800, LR: 0.000008, Grad Norm: 0.108870
7406.8s 692 Epoch 2, Step 5950, Loss: 0.1801, LR: 0.000008, Grad Norm: 0.108870
7465.6s 693 Epoch 2, Step 6000, Loss: 0.1797, LR: 0.000008, Grad Norm: 0.091099
7465.8s 694 Epoch 2, Step 6000, Loss: 0.1797, LR: 0.000008, Grad Norm: 0.091099
7466.1s 695 Epoch 2, Step 6000, Loss: 0.1797, LR: 0.000008, Grad Norm: 0.091099
7466.4s 696 Epoch 2, Step 6000, Loss: 0.1797, LR: 0.000008, Grad Norm: 0.091099
7524.9s 697 Epoch 2, Step 6050, Loss: 0.1801, LR: 0.000008, Grad Norm: 0.062041
7525.1s 698 Epoch 2, Step 6050, Loss: 0.1801, LR: 0.000008, Grad Norm: 0.062041
7525.4s 699 Epoch 2, Step 6050, Loss: 0.1801, LR: 0.000008, Grad Norm: 0.062041
7525.7s 700 Epoch 2, Step 6050, Loss: 0.1801, LR: 0.000008, Grad Norm: 0.062041
7584.5s 701 Epoch 2, Step 6100, Loss: 0.1807, LR: 0.000008, Grad Norm: 0.083708
7584.8s 702 Epoch 2, Step 6100, Loss: 0.1807, LR: 0.000008, Grad Norm: 0.083708
7585.0s 703 Epoch 2, Step 6100, Loss: 0.1807, LR: 0.000008, Grad Norm: 0.083708
7585.3s 704 Epoch 2, Step 6100, Loss: 0.1806, LR: 0.000008, Grad Norm: 0.083708
7643.8s 705 Epoch 2, Step 6150, Loss: 0.1806, LR: 0.000008, Grad Norm: 0.099310
7644.0s 706 Epoch 2, Step 6150, Loss: 0.1807, LR: 0.000008, Grad Norm: 0.099310
7644.6s 707 Epoch 2, Step 6150, Loss: 0.1806, LR: 0.000008, Grad Norm: 0.099310
7645.0s 708 Epoch 2, Step 6150, Loss: 0.1806, LR: 0.000008, Grad Norm: 0.099310
7703.5s 709 Epoch 2, Step 6200, Loss: 0.1804, LR: 0.000008, Grad Norm: 0.051962
7703.7s 710 Epoch 2, Step 6200, Loss: 0.1804, LR: 0.000008, Grad Norm: 0.051962
7704.0s 711 Epoch 2, Step 6200, Loss: 0.1804, LR: 0.000008, Grad Norm: 0.051962
7704.3s 712 Epoch 2, Step 6200, Loss: 0.1804, LR: 0.000008, Grad Norm: 0.051962
7763.1s 713 Epoch 2, Step 6250, Loss: 0.1809, LR: 0.000008, Grad Norm: 0.060441
7763.3s 714 Epoch 2, Step 6250, Loss: 0.1809, LR: 0.000008, Grad Norm: 0.060441
7763.6s 715 Epoch 2, Step 6250, Loss: 0.1809, LR: 0.000008, Grad Norm: 0.060441
7763.9s 716 Epoch 2, Step 6250, Loss: 0.1809, LR: 0.000008, Grad Norm: 0.060441
7822.2s 717 Epoch 2, Step 6300, Loss: 0.1813, LR: 0.000008, Grad Norm: 0.067464
7822.5s 718 Epoch 2, Step 6300, Loss: 0.1813, LR: 0.000008, Grad Norm: 0.067464
7822.7s 719 Epoch 2, Step 6300, Loss: 0.1814, LR: 0.000008, Grad Norm: 0.067464
7823.0s 720 Epoch 2, Step 6300, Loss: 0.1813, LR: 0.000008, Grad Norm: 0.067464
7881.6s 721 Epoch 2, Step 6350, Loss: 0.1820, LR: 0.000008, Grad Norm: 0.136806
7881.8s 722 Epoch 2, Step 6350, Loss: 0.1820, LR: 0.000008, Grad Norm: 0.136806
7882.1s 723 Epoch 2, Step 6350, Loss: 0.1820, LR: 0.000008, Grad Norm: 0.136806
7882.4s 724 Epoch 2, Step 6350, Loss: 0.1820, LR: 0.000008, Grad Norm: 0.136806
7940.6s 725 Epoch 2, Step 6400, Loss: 0.1814, LR: 0.000008, Grad Norm: 0.053239
7940.9s 726 Epoch 2, Step 6400, Loss: 0.1814, LR: 0.000008, Grad Norm: 0.053239
7941.2s 727 Epoch 2, Step 6400, Loss: 0.1814, LR: 0.000008, Grad Norm: 0.053239
7941.5s 728 Epoch 2, Step 6400, Loss: 0.1814, LR: 0.000008, Grad Norm: 0.053239
8000.3s 729 Epoch 2, Step 6450, Loss: 0.1817, LR: 0.000008, Grad Norm: 0.137750
8000.5s 730 Epoch 2, Step 6450, Loss: 0.1817, LR: 0.000008, Grad Norm: 0.137750
8000.8s 731 Epoch 2, Step 6450, Loss: 0.1818, LR: 0.000008, Grad Norm: 0.137750
8001.1s 732 Epoch 2, Step 6450, Loss: 0.1818, LR: 0.000008, Grad Norm: 0.137750
8059.7s 733 Epoch 2, Step 6500, Loss: 0.1830, LR: 0.000008, Grad Norm: 0.206811
8059.9s 734 Epoch 2, Step 6500, Loss: 0.1830, LR: 0.000008, Grad Norm: 0.206811
8060.2s 735 Epoch 2, Step 6500, Loss: 0.1830, LR: 0.000008, Grad Norm: 0.206811
8060.5s 736 Epoch 2, Step 6500, Loss: 0.1830, LR: 0.000008, Grad Norm: 0.206811
8119.6s 737 Epoch 2, Step 6550, Loss: 0.1835, LR: 0.000008, Grad Norm: 0.082209
8119.8s 738 Epoch 2, Step 6550, Loss: 0.1835, LR: 0.000008, Grad Norm: 0.082209
8120.1s 739 Epoch 2, Step 6550, Loss: 0.1835, LR: 0.000008, Grad Norm: 0.082209
8120.4s 740 Epoch 2, Step 6550, Loss: 0.1834, LR: 0.000008, Grad Norm: 0.082209
8178.2s 741 Epoch 2, Step 6600, Loss: 0.1832, LR: 0.000008, Grad Norm: 0.063000
8178.5s 742 Epoch 2, Step 6600, Loss: 0.1832, LR: 0.000008, Grad Norm: 0.063000
8178.7s 743 Epoch 2, Step 6600, Loss: 0.1832, LR: 0.000008, Grad Norm: 0.063000
8179.0s 744 Epoch 2, Step 6600, Loss: 0.1832, LR: 0.000008, Grad Norm: 0.063000
8236.9s 745 Epoch 2, Step 6650, Loss: 0.1833, LR: 0.000008, Grad Norm: 0.063141
8237.1s 746 Epoch 2, Step 6650, Loss: 0.1833, LR: 0.000008, Grad Norm: 0.063141
8237.4s 747 Epoch 2, Step 6650, Loss: 0.1834, LR: 0.000008, Grad Norm: 0.063141
8237.7s 748 Epoch 2, Step 6650, Loss: 0.1834, LR: 0.000008, Grad Norm: 0.063141
8295.5s 749 Epoch 2, Step 6700, Loss: 0.1834, LR: 0.000008, Grad Norm: 0.129228
8295.7s 750 Epoch 2, Step 6700, Loss: 0.1835, LR: 0.000008, Grad Norm: 0.129228
8296.0s 751 Epoch 2, Step 6700, Loss: 0.1835, LR: 0.000008, Grad Norm: 0.129228
8296.3s 752 Epoch 2, Step 6700, Loss: 0.1834, LR: 0.000008, Grad Norm: 0.129228
8354.2s 753 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
8354.2s 754 To disable this warning, you can either:
8354.2s 755 - Avoid using `tokenizers` before the fork if possible
8354.2s 756 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
8354.2s 757 Epoch 2, Step 6750, Loss: 0.1835, LR: 0.000008, Grad Norm: 0.070096
8354.2s 758 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
8354.2s 759 To disable this warning, you can either:
8354.2s 760 - Avoid using `tokenizers` before the fork if possible
8354.2s 761 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
8354.3s 762 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
8354.3s 763 To disable this warning, you can either:
8354.3s 764 - Avoid using `tokenizers` before the fork if possible
8354.3s 765 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
8354.4s 766 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
8354.4s 767 To disable this warning, you can either:
8354.4s 768 - Avoid using `tokenizers` before the fork if possible
8354.4s 769 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
8460.1s 770 Epoch 2 completed. Train Loss: 0.1835, Validation Loss: 0.1905
8460.1s 771 Selected animals for validation CLIP score: ['frog', 'horse', 'dog', 'hummingbird', 'penguin']
8460.2s 772 Expected types for unet: (<class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionModel'>,), got <class 'peft.peft_model.PeftModel'>.
8460.2s 773 You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
8464.3s 774 Animal: frog, CLIP Score: 30.4219
8466.8s 775 Animal: horse, CLIP Score: 30.8651
8469.4s 776 Animal: dog, CLIP Score: 26.3635
8472.0s 777 Animal: hummingbird, CLIP Score: 32.7665
8474.5s 778 Animal: penguin, CLIP Score: 28.0898
8474.5s 779 Average Validation CLIP Score: 29.7014
8474.5s 780 EarlyStopping counter: 1 out of 5
8474.6s 781 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
8474.6s 782 To disable this warning, you can either:
8474.6s 783 - Avoid using `tokenizers` before the fork if possible
8474.6s 784 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
8474.6s 785 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
8474.6s 786 To disable this warning, you can either:
8474.6s 787 - Avoid using `tokenizers` before the fork if possible
8474.6s 788 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
8474.6s 789 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
8474.6s 790 To disable this warning, you can either:
8474.6s 791 - Avoid using `tokenizers` before the fork if possible
8474.6s 792 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
8474.8s 793 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
8474.8s 794 To disable this warning, you can either:
8474.8s 795 - Avoid using `tokenizers` before the fork if possible
8474.8s 796 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
8475.2s 797 Epoch 3, Step 6750, Loss: 0.1213, LR: 0.000008, Grad Norm: 0.000000
8475.5s 798 Epoch 3, Step 6750, Loss: 0.0636, LR: 0.000008, Grad Norm: 0.000000
8475.8s 799 Epoch 3, Step 6750, Loss: 0.0616, LR: 0.000008, Grad Norm: 0.000000
8533.6s 800 Epoch 3, Step 6800, Loss: 0.1818, LR: 0.000008, Grad Norm: 0.079869
8534.0s 801 Epoch 3, Step 6800, Loss: 0.1809, LR: 0.000008, Grad Norm: 0.079869
8534.3s 802 Epoch 3, Step 6800, Loss: 0.1805, LR: 0.000008, Grad Norm: 0.079869
8534.6s 803 Epoch 3, Step 6800, Loss: 0.1801, LR: 0.000008, Grad Norm: 0.079869
8592.2s 804 Epoch 3, Step 6850, Loss: 0.1786, LR: 0.000008, Grad Norm: 0.109924
8592.4s 805 Epoch 3, Step 6850, Loss: 0.1789, LR: 0.000008, Grad Norm: 0.109924
8592.7s 806 Epoch 3, Step 6850, Loss: 0.1785, LR: 0.000008, Grad Norm: 0.109924
8592.9s 807 Epoch 3, Step 6850, Loss: 0.1792, LR: 0.000008, Grad Norm: 0.109924
8650.5s 808 Epoch 3, Step 6900, Loss: 0.1803, LR: 0.000008, Grad Norm: 0.076645
8650.8s 809 Epoch 3, Step 6900, Loss: 0.1805, LR: 0.000008, Grad Norm: 0.076645
8651.0s 810 Epoch 3, Step 6900, Loss: 0.1808, LR: 0.000008, Grad Norm: 0.076645
8651.3s 811 Epoch 3, Step 6900, Loss: 0.1816, LR: 0.000008, Grad Norm: 0.076645
8708.8s 812 Epoch 3, Step 6950, Loss: 0.1802, LR: 0.000008, Grad Norm: 0.061197
8709.0s 813 Epoch 3, Step 6950, Loss: 0.1807, LR: 0.000008, Grad Norm: 0.061197
8709.3s 814 Epoch 3, Step 6950, Loss: 0.1808, LR: 0.000008, Grad Norm: 0.061197
8709.6s 815 Epoch 3, Step 6950, Loss: 0.1805, LR: 0.000008, Grad Norm: 0.061197
8767.5s 816 Epoch 3, Step 7000, Loss: 0.1841, LR: 0.000008, Grad Norm: 0.135941
8767.8s 817 Epoch 3, Step 7000, Loss: 0.1839, LR: 0.000008, Grad Norm: 0.135941
8768.1s 818 Epoch 3, Step 7000, Loss: 0.1841, LR: 0.000008, Grad Norm: 0.135941
8768.3s 819 Epoch 3, Step 7000, Loss: 0.1842, LR: 0.000008, Grad Norm: 0.135941
8825.6s 820 Epoch 3, Step 7050, Loss: 0.1826, LR: 0.000008, Grad Norm: 0.053540
8825.8s 821 Epoch 3, Step 7050, Loss: 0.1825, LR: 0.000008, Grad Norm: 0.053540
8826.1s 822 Epoch 3, Step 7050, Loss: 0.1826, LR: 0.000008, Grad Norm: 0.053540
8826.4s 823 Epoch 3, Step 7050, Loss: 0.1827, LR: 0.000008, Grad Norm: 0.053540
8883.8s 824 Epoch 3, Step 7100, Loss: 0.1839, LR: 0.000008, Grad Norm: 0.115610
8884.0s 825 Epoch 3, Step 7100, Loss: 0.1838, LR: 0.000008, Grad Norm: 0.115610
8884.3s 826 Epoch 3, Step 7100, Loss: 0.1837, LR: 0.000008, Grad Norm: 0.115610
8884.6s 827 Epoch 3, Step 7100, Loss: 0.1836, LR: 0.000008, Grad Norm: 0.115610
8941.8s 828 Epoch 3, Step 7150, Loss: 0.1876, LR: 0.000008, Grad Norm: 0.067389
8942.0s 829 Epoch 3, Step 7150, Loss: 0.1878, LR: 0.000008, Grad Norm: 0.067389
8942.3s 830 Epoch 3, Step 7150, Loss: 0.1879, LR: 0.000008, Grad Norm: 0.067389
8942.6s 831 Epoch 3, Step 7150, Loss: 0.1878, LR: 0.000008, Grad Norm: 0.067389
9000.9s 832 Epoch 3, Step 7200, Loss: 0.1838, LR: 0.000008, Grad Norm: 0.049766
9001.0s 833 Epoch 3, Step 7200, Loss: 0.1837, LR: 0.000008, Grad Norm: 0.049766
9001.3s 834 Epoch 3, Step 7200, Loss: 0.1837, LR: 0.000008, Grad Norm: 0.049766
9001.6s 835 Epoch 3, Step 7200, Loss: 0.1836, LR: 0.000008, Grad Norm: 0.049766
9059.3s 836 Epoch 3, Step 7250, Loss: 0.1863, LR: 0.000008, Grad Norm: 0.108263
9059.6s 837 Epoch 3, Step 7250, Loss: 0.1862, LR: 0.000008, Grad Norm: 0.108263
9059.8s 838 Epoch 3, Step 7250, Loss: 0.1863, LR: 0.000008, Grad Norm: 0.108263
9060.1s 839 Epoch 3, Step 7250, Loss: 0.1862, LR: 0.000008, Grad Norm: 0.108263
9117.9s 840 Epoch 3, Step 7300, Loss: 0.1856, LR: 0.000008, Grad Norm: 0.125614
9118.1s 841 Epoch 3, Step 7300, Loss: 0.1856, LR: 0.000008, Grad Norm: 0.125614
9118.4s 842 Epoch 3, Step 7300, Loss: 0.1855, LR: 0.000008, Grad Norm: 0.125614
9118.6s 843 Epoch 3, Step 7300, Loss: 0.1855, LR: 0.000008, Grad Norm: 0.125614
9175.9s 844 Epoch 3, Step 7350, Loss: 0.1859, LR: 0.000008, Grad Norm: 0.062530
9176.1s 845 Epoch 3, Step 7350, Loss: 0.1859, LR: 0.000008, Grad Norm: 0.062530
9176.4s 846 Epoch 3, Step 7350, Loss: 0.1859, LR: 0.000008, Grad Norm: 0.062530
9176.7s 847 Epoch 3, Step 7350, Loss: 0.1859, LR: 0.000008, Grad Norm: 0.062530
9234.5s 848 Epoch 3, Step 7400, Loss: 0.1861, LR: 0.000008, Grad Norm: 0.105420
9234.7s 849 Epoch 3, Step 7400, Loss: 0.1861, LR: 0.000008, Grad Norm: 0.105420
9235.0s 850 Epoch 3, Step 7400, Loss: 0.1862, LR: 0.000008, Grad Norm: 0.105420
9235.3s 851 Epoch 3, Step 7400, Loss: 0.1864, LR: 0.000008, Grad Norm: 0.105420
9292.6s 852 Epoch 3, Step 7450, Loss: 0.1851, LR: 0.000008, Grad Norm: 0.087256
9292.8s 853 Epoch 3, Step 7450, Loss: 0.1850, LR: 0.000008, Grad Norm: 0.087256
9293.1s 854 Epoch 3, Step 7450, Loss: 0.1850, LR: 0.000008, Grad Norm: 0.087256
9293.4s 855 Epoch 3, Step 7450, Loss: 0.1849, LR: 0.000008, Grad Norm: 0.087256
9351.3s 856 Epoch 3, Step 7500, Loss: 0.1868, LR: 0.000008, Grad Norm: 0.065747
9351.5s 857 Epoch 3, Step 7500, Loss: 0.1867, LR: 0.000008, Grad Norm: 0.065747
9351.8s 858 Epoch 3, Step 7500, Loss: 0.1867, LR: 0.000008, Grad Norm: 0.065747
9352.0s 859 Epoch 3, Step 7500, Loss: 0.1866, LR: 0.000008, Grad Norm: 0.065747
9409.7s 860 Epoch 3, Step 7550, Loss: 0.1875, LR: 0.000008, Grad Norm: 0.049406
9409.9s 861 Epoch 3, Step 7550, Loss: 0.1877, LR: 0.000008, Grad Norm: 0.049406
9410.2s 862 Epoch 3, Step 7550, Loss: 0.1876, LR: 0.000008, Grad Norm: 0.049406
9410.5s 863 Epoch 3, Step 7550, Loss: 0.1876, LR: 0.000008, Grad Norm: 0.049406
9468.4s 864 Epoch 3, Step 7600, Loss: 0.1867, LR: 0.000008, Grad Norm: 0.118235
9468.6s 865 Epoch 3, Step 7600, Loss: 0.1866, LR: 0.000008, Grad Norm: 0.118235
9468.9s 866 Epoch 3, Step 7600, Loss: 0.1866, LR: 0.000008, Grad Norm: 0.118235
9469.2s 867 Epoch 3, Step 7600, Loss: 0.1866, LR: 0.000008, Grad Norm: 0.118235
9526.6s 868 Epoch 3, Step 7650, Loss: 0.1867, LR: 0.000007, Grad Norm: 0.055366
9526.8s 869 Epoch 3, Step 7650, Loss: 0.1867, LR: 0.000007, Grad Norm: 0.055366
9527.1s 870 Epoch 3, Step 7650, Loss: 0.1866, LR: 0.000007, Grad Norm: 0.055366
9527.4s 871 Epoch 3, Step 7650, Loss: 0.1866, LR: 0.000007, Grad Norm: 0.055366
9585.4s 872 Epoch 3, Step 7700, Loss: 0.1871, LR: 0.000007, Grad Norm: 0.086937
9585.6s 873 Epoch 3, Step 7700, Loss: 0.1871, LR: 0.000007, Grad Norm: 0.086937
9585.9s 874 Epoch 3, Step 7700, Loss: 0.1871, LR: 0.000007, Grad Norm: 0.086937
9586.1s 875 Epoch 3, Step 7700, Loss: 0.1871, LR: 0.000007, Grad Norm: 0.086937
9643.5s 876 Epoch 3, Step 7750, Loss: 0.1857, LR: 0.000007, Grad Norm: 0.053216
9643.7s 877 Epoch 3, Step 7750, Loss: 0.1857, LR: 0.000007, Grad Norm: 0.053216
9644.0s 878 Epoch 3, Step 7750, Loss: 0.1857, LR: 0.000007, Grad Norm: 0.053216
9644.3s 879 Epoch 3, Step 7750, Loss: 0.1858, LR: 0.000007, Grad Norm: 0.053216
9702.4s 880 Epoch 3, Step 7800, Loss: 0.1858, LR: 0.000007, Grad Norm: 0.110642
9702.6s 881 Epoch 3, Step 7800, Loss: 0.1859, LR: 0.000007, Grad Norm: 0.110642
9702.9s 882 Epoch 3, Step 7800, Loss: 0.1859, LR: 0.000007, Grad Norm: 0.110642
9703.2s 883 Epoch 3, Step 7800, Loss: 0.1858, LR: 0.000007, Grad Norm: 0.110642
9760.8s 884 Epoch 3, Step 7850, Loss: 0.1858, LR: 0.000007, Grad Norm: 0.120929
9761.0s 885 Epoch 3, Step 7850, Loss: 0.1859, LR: 0.000007, Grad Norm: 0.120929
9761.3s 886 Epoch 3, Step 7850, Loss: 0.1859, LR: 0.000007, Grad Norm: 0.120929
9761.6s 887 Epoch 3, Step 7850, Loss: 0.1859, LR: 0.000007, Grad Norm: 0.120929
9819.5s 888 Epoch 3, Step 7900, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.068539
9819.8s 889 Epoch 3, Step 7900, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.068539
9820.0s 890 Epoch 3, Step 7900, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.068539
9820.3s 891 Epoch 3, Step 7900, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.068539
9877.6s 892 Epoch 3, Step 7950, Loss: 0.1869, LR: 0.000007, Grad Norm: 0.071192
9877.8s 893 Epoch 3, Step 7950, Loss: 0.1869, LR: 0.000007, Grad Norm: 0.071192
9878.1s 894 Epoch 3, Step 7950, Loss: 0.1870, LR: 0.000007, Grad Norm: 0.071192
9878.4s 895 Epoch 3, Step 7950, Loss: 0.1870, LR: 0.000007, Grad Norm: 0.071192
9936.7s 896 Epoch 3, Step 8000, Loss: 0.1867, LR: 0.000007, Grad Norm: 0.078390
9936.9s 897 Epoch 3, Step 8000, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.078390
9937.2s 898 Epoch 3, Step 8000, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.078390
9937.5s 899 Epoch 3, Step 8000, Loss: 0.1869, LR: 0.000007, Grad Norm: 0.078390
9995.7s 900 Epoch 3, Step 8050, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.105354
9995.9s 901 Epoch 3, Step 8050, Loss: 0.1867, LR: 0.000007, Grad Norm: 0.105354
9996.2s 902 Epoch 3, Step 8050, Loss: 0.1869, LR: 0.000007, Grad Norm: 0.105354
9996.5s 903 Epoch 3, Step 8050, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.105354
10055.1s 904 Epoch 3, Step 8100, Loss: 0.1862, LR: 0.000007, Grad Norm: 0.052601
10055.3s 905 Epoch 3, Step 8100, Loss: 0.1862, LR: 0.000007, Grad Norm: 0.052601
10055.6s 906 Epoch 3, Step 8100, Loss: 0.1864, LR: 0.000007, Grad Norm: 0.052601
10055.8s 907 Epoch 3, Step 8100, Loss: 0.1863, LR: 0.000007, Grad Norm: 0.052601
10113.7s 908 Epoch 3, Step 8150, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.090955
10113.9s 909 Epoch 3, Step 8150, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.090955
10114.2s 910 Epoch 3, Step 8150, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.090955
10114.5s 911 Epoch 3, Step 8150, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.090955
10173.0s 912 Epoch 3, Step 8200, Loss: 0.1876, LR: 0.000007, Grad Norm: 0.069553
10173.3s 913 Epoch 3, Step 8200, Loss: 0.1876, LR: 0.000007, Grad Norm: 0.069553
10173.6s 914 Epoch 3, Step 8200, Loss: 0.1876, LR: 0.000007, Grad Norm: 0.069553
10173.9s 915 Epoch 3, Step 8200, Loss: 0.1876, LR: 0.000007, Grad Norm: 0.069553
10232.3s 916 Epoch 3, Step 8250, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.097837
10232.5s 917 Epoch 3, Step 8250, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.097837
10232.8s 918 Epoch 3, Step 8250, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.097837
10233.1s 919 Epoch 3, Step 8250, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.097837
10291.1s 920 Epoch 3, Step 8300, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.065583
10291.4s 921 Epoch 3, Step 8300, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.065583
10291.6s 922 Epoch 3, Step 8300, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.065583
10291.9s 923 Epoch 3, Step 8300, Loss: 0.1876, LR: 0.000007, Grad Norm: 0.065583
10350.1s 924 Epoch 3, Step 8350, Loss: 0.1878, LR: 0.000007, Grad Norm: 0.053681
10350.3s 925 Epoch 3, Step 8350, Loss: 0.1878, LR: 0.000007, Grad Norm: 0.053681
10350.6s 926 Epoch 3, Step 8350, Loss: 0.1878, LR: 0.000007, Grad Norm: 0.053681
10350.9s 927 Epoch 3, Step 8350, Loss: 0.1878, LR: 0.000007, Grad Norm: 0.053681
10409.4s 928 Epoch 3, Step 8400, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.140967
10409.6s 929 Epoch 3, Step 8400, Loss: 0.1878, LR: 0.000007, Grad Norm: 0.140967
10409.9s 930 Epoch 3, Step 8400, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.140967
10410.2s 931 Epoch 3, Step 8400, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.140967
10467.9s 932 Epoch 3, Step 8450, Loss: 0.1876, LR: 0.000007, Grad Norm: 0.104132
10468.1s 933 Epoch 3, Step 8450, Loss: 0.1876, LR: 0.000007, Grad Norm: 0.104132
10468.4s 934 Epoch 3, Step 8450, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.104132
10468.7s 935 Epoch 3, Step 8450, Loss: 0.1878, LR: 0.000007, Grad Norm: 0.104132
10527.3s 936 Epoch 3, Step 8500, Loss: 0.1881, LR: 0.000007, Grad Norm: 0.115150
10527.5s 937 Epoch 3, Step 8500, Loss: 0.1882, LR: 0.000007, Grad Norm: 0.115150
10527.8s 938 Epoch 3, Step 8500, Loss: 0.1881, LR: 0.000007, Grad Norm: 0.115150
10528.1s 939 Epoch 3, Step 8500, Loss: 0.1882, LR: 0.000007, Grad Norm: 0.115150
10585.8s 940 Epoch 3, Step 8550, Loss: 0.1880, LR: 0.000007, Grad Norm: 0.070721
10586.1s 941 Epoch 3, Step 8550, Loss: 0.1880, LR: 0.000007, Grad Norm: 0.070721
10586.3s 942 Epoch 3, Step 8550, Loss: 0.1880, LR: 0.000007, Grad Norm: 0.070721
10586.6s 943 Epoch 3, Step 8550, Loss: 0.1880, LR: 0.000007, Grad Norm: 0.070721
10645.2s 944 Epoch 3, Step 8600, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.141411
10645.4s 945 Epoch 3, Step 8600, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.141411
10645.7s 946 Epoch 3, Step 8600, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.141411
10646.0s 947 Epoch 3, Step 8600, Loss: 0.1877, LR: 0.000007, Grad Norm: 0.141411
10704.0s 948 Epoch 3, Step 8650, Loss: 0.1876, LR: 0.000007, Grad Norm: 0.116085
10704.2s 949 Epoch 3, Step 8650, Loss: 0.1876, LR: 0.000007, Grad Norm: 0.116085
10704.5s 950 Epoch 3, Step 8650, Loss: 0.1876, LR: 0.000007, Grad Norm: 0.116085
10704.8s 951 Epoch 3, Step 8650, Loss: 0.1876, LR: 0.000007, Grad Norm: 0.116085
10762.8s 952 Epoch 3, Step 8700, Loss: 0.1874, LR: 0.000007, Grad Norm: 0.132920
10763.1s 953 Epoch 3, Step 8700, Loss: 0.1874, LR: 0.000007, Grad Norm: 0.132920
10763.3s 954 Epoch 3, Step 8700, Loss: 0.1874, LR: 0.000007, Grad Norm: 0.132920
10763.6s 955 Epoch 3, Step 8700, Loss: 0.1874, LR: 0.000007, Grad Norm: 0.132920
10821.3s 956 Epoch 3, Step 8750, Loss: 0.1870, LR: 0.000007, Grad Norm: 0.104200
10821.5s 957 Epoch 3, Step 8750, Loss: 0.1870, LR: 0.000007, Grad Norm: 0.104200
10821.8s 958 Epoch 3, Step 8750, Loss: 0.1870, LR: 0.000007, Grad Norm: 0.104200
10822.1s 959 Epoch 3, Step 8750, Loss: 0.1870, LR: 0.000007, Grad Norm: 0.104200
10880.0s 960 Epoch 3, Step 8800, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.054613
10880.2s 961 Epoch 3, Step 8800, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.054613
10880.5s 962 Epoch 3, Step 8800, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.054613
10880.8s 963 Epoch 3, Step 8800, Loss: 0.1868, LR: 0.000007, Grad Norm: 0.054613
10938.4s 964 Epoch 3, Step 8850, Loss: 0.1864, LR: 0.000007, Grad Norm: 0.057848
10938.6s 965 Epoch 3, Step 8850, Loss: 0.1864, LR: 0.000007, Grad Norm: 0.057848
10938.9s 966 Epoch 3, Step 8850, Loss: 0.1863, LR: 0.000007, Grad Norm: 0.057848
10939.2s 967 Epoch 3, Step 8850, Loss: 0.1863, LR: 0.000007, Grad Norm: 0.057848
10997.3s 968 Epoch 3, Step 8900, Loss: 0.1864, LR: 0.000007, Grad Norm: 0.118494
10997.5s 969 Epoch 3, Step 8900, Loss: 0.1864, LR: 0.000007, Grad Norm: 0.118494
10997.8s 970 Epoch 3, Step 8900, Loss: 0.1864, LR: 0.000007, Grad Norm: 0.118494
10998.0s 971 Epoch 3, Step 8900, Loss: 0.1864, LR: 0.000007, Grad Norm: 0.118494
11055.6s 972 Epoch 3, Step 8950, Loss: 0.1862, LR: 0.000007, Grad Norm: 0.092471
11055.8s 973 Epoch 3, Step 8950, Loss: 0.1862, LR: 0.000007, Grad Norm: 0.092471
11056.1s 974 Epoch 3, Step 8950, Loss: 0.1862, LR: 0.000007, Grad Norm: 0.092471
11056.4s 975 Epoch 3, Step 8950, Loss: 0.1862, LR: 0.000007, Grad Norm: 0.092471
11114.4s 976 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
11114.4s 977 To disable this warning, you can either:
11114.4s 978 - Avoid using `tokenizers` before the fork if possible
11114.4s 979 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11114.4s 980 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
11114.4s 981 To disable this warning, you can either:
11114.4s 982 - Avoid using `tokenizers` before the fork if possible
11114.4s 983 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11114.5s 984 Epoch 3, Step 9000, Loss: 0.1864, LR: 0.000007, Grad Norm: 0.099148
11114.5s 985 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
11114.5s 986 To disable this warning, you can either:
11114.5s 987 - Avoid using `tokenizers` before the fork if possible
11114.5s 988 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11114.6s 989 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
11114.6s 990 To disable this warning, you can either:
11114.6s 991 - Avoid using `tokenizers` before the fork if possible
11114.6s 992 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11220.4s 993 Epoch 3 completed. Train Loss: 0.1864, Validation Loss: 0.1897
11220.4s 994 Selected animals for validation CLIP score: ['fox', 'antelope', 'eagle', 'flamingo', 'caterpillar']
11220.4s 995 Expected types for unet: (<class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionModel'>,), got <class 'peft.peft_model.PeftModel'>.
11220.4s 996 You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
11224.5s 997 Animal: fox, CLIP Score: 31.1159
11227.1s 998 Animal: antelope, CLIP Score: 30.6798
11229.6s 999 Animal: eagle, CLIP Score: 30.2027
11232.2s 1000 Animal: flamingo, CLIP Score: 30.4008
11234.8s 1001 Animal: caterpillar, CLIP Score: 27.5673
11234.8s 1002 Average Validation CLIP Score: 29.9933
11234.8s 1003 EarlyStopping counter: 2 out of 5
11234.8s 1004 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
11234.8s 1005 To disable this warning, you can either:
11234.8s 1006 - Avoid using `tokenizers` before the fork if possible
11234.8s 1007 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11234.9s 1008 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
11234.9s 1009 To disable this warning, you can either:
11234.9s 1010 - Avoid using `tokenizers` before the fork if possible
11234.9s 1011 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11234.9s 1012 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
11234.9s 1013 To disable this warning, you can either:
11234.9s 1014 - Avoid using `tokenizers` before the fork if possible
11234.9s 1015 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11235.0s 1016 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
11235.0s 1017 To disable this warning, you can either:
11235.0s 1018 - Avoid using `tokenizers` before the fork if possible
11235.0s 1019 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
11235.5s 1020 Epoch 4, Step 9000, Loss: 0.0170, LR: 0.000007, Grad Norm: 0.000000
11235.7s 1021 Epoch 4, Step 9000, Loss: 0.1113, LR: 0.000007, Grad Norm: 0.000000
11236.0s 1022 Epoch 4, Step 9000, Loss: 0.2210, LR: 0.000007, Grad Norm: 0.000000
11294.3s 1023 Epoch 4, Step 9050, Loss: 0.1999, LR: 0.000007, Grad Norm: 0.073677
11294.5s 1024 Epoch 4, Step 9050, Loss: 0.1992, LR: 0.000007, Grad Norm: 0.073677
11294.8s 1025 Epoch 4, Step 9050, Loss: 0.2002, LR: 0.000007, Grad Norm: 0.073677
11295.1s 1026 Epoch 4, Step 9050, Loss: 0.1993, LR: 0.000007, Grad Norm: 0.073677
11353.4s 1027 Epoch 4, Step 9100, Loss: 0.1891, LR: 0.000007, Grad Norm: 0.107968
11353.6s 1028 Epoch 4, Step 9100, Loss: 0.1901, LR: 0.000007, Grad Norm: 0.107968
11353.9s 1029 Epoch 4, Step 9100, Loss: 0.1896, LR: 0.000007, Grad Norm: 0.107968
11354.2s 1030 Epoch 4, Step 9100, Loss: 0.1893, LR: 0.000007, Grad Norm: 0.107968
11411.9s 1031 Epoch 4, Step 9150, Loss: 0.1849, LR: 0.000007, Grad Norm: 0.098898
11412.2s 1032 Epoch 4, Step 9150, Loss: 0.1847, LR: 0.000007, Grad Norm: 0.098898
11412.4s 1033 Epoch 4, Step 9150, Loss: 0.1845, LR: 0.000007, Grad Norm: 0.098898
11413.0s 1034 Epoch 4, Step 9150, Loss: 0.1843, LR: 0.000007, Grad Norm: 0.098898
11471.0s 1035 Epoch 4, Step 9200, Loss: 0.1847, LR: 0.000006, Grad Norm: 0.077673
11471.2s 1036 Epoch 4, Step 9200, Loss: 0.1845, LR: 0.000006, Grad Norm: 0.077673
11471.5s 1037 Epoch 4, Step 9200, Loss: 0.1842, LR: 0.000006, Grad Norm: 0.077673
11471.8s 1038 Epoch 4, Step 9200, Loss: 0.1841, LR: 0.000006, Grad Norm: 0.077673
11529.6s 1039 Epoch 4, Step 9250, Loss: 0.1856, LR: 0.000006, Grad Norm: 0.086869
11530.0s 1040 Epoch 4, Step 9250, Loss: 0.1855, LR: 0.000006, Grad Norm: 0.086869
11530.5s 1041 Epoch 4, Step 9250, Loss: 0.1855, LR: 0.000006, Grad Norm: 0.086869
11530.7s 1042 Epoch 4, Step 9250, Loss: 0.1856, LR: 0.000006, Grad Norm: 0.086869
11588.6s 1043 Epoch 4, Step 9300, Loss: 0.1886, LR: 0.000006, Grad Norm: 0.136099
11588.8s 1044 Epoch 4, Step 9300, Loss: 0.1889, LR: 0.000006, Grad Norm: 0.136099
11589.1s 1045 Epoch 4, Step 9300, Loss: 0.1889, LR: 0.000006, Grad Norm: 0.136099
11589.4s 1046 Epoch 4, Step 9300, Loss: 0.1888, LR: 0.000006, Grad Norm: 0.136099
11647.2s 1047 Epoch 4, Step 9350, Loss: 0.1895, LR: 0.000006, Grad Norm: 0.093534
11647.7s 1048 Epoch 4, Step 9350, Loss: 0.1896, LR: 0.000006, Grad Norm: 0.093534
11648.1s 1049 Epoch 4, Step 9350, Loss: 0.1895, LR: 0.000006, Grad Norm: 0.093534
11648.4s 1050 Epoch 4, Step 9350, Loss: 0.1894, LR: 0.000006, Grad Norm: 0.093534
11706.2s 1051 Epoch 4, Step 9400, Loss: 0.1882, LR: 0.000006, Grad Norm: 0.113164
11706.4s 1052 Epoch 4, Step 9400, Loss: 0.1881, LR: 0.000006, Grad Norm: 0.113164
11706.7s 1053 Epoch 4, Step 9400, Loss: 0.1880, LR: 0.000006, Grad Norm: 0.113164
11707.0s 1054 Epoch 4, Step 9400, Loss: 0.1880, LR: 0.000006, Grad Norm: 0.113164
11765.0s 1055 Epoch 4, Step 9450, Loss: 0.1906, LR: 0.000006, Grad Norm: 0.090890
11765.3s 1056 Epoch 4, Step 9450, Loss: 0.1907, LR: 0.000006, Grad Norm: 0.090890
11765.6s 1057 Epoch 4, Step 9450, Loss: 0.1907, LR: 0.000006, Grad Norm: 0.090890
11765.9s 1058 Epoch 4, Step 9450, Loss: 0.1909, LR: 0.000006, Grad Norm: 0.090890
11823.4s 1059 Epoch 4, Step 9500, Loss: 0.1901, LR: 0.000006, Grad Norm: 0.152622
11823.7s 1060 Epoch 4, Step 9500, Loss: 0.1902, LR: 0.000006, Grad Norm: 0.152622
11823.9s 1061 Epoch 4, Step 9500, Loss: 0.1901, LR: 0.000006, Grad Norm: 0.152622
11824.2s 1062 Epoch 4, Step 9500, Loss: 0.1902, LR: 0.000006, Grad Norm: 0.152622
11881.9s 1063 Epoch 4, Step 9550, Loss: 0.1921, LR: 0.000006, Grad Norm: 0.070797
11882.1s 1064 Epoch 4, Step 9550, Loss: 0.1923, LR: 0.000006, Grad Norm: 0.070797
11882.4s 1065 Epoch 4, Step 9550, Loss: 0.1922, LR: 0.000006, Grad Norm: 0.070797
11882.7s 1066 Epoch 4, Step 9550, Loss: 0.1922, LR: 0.000006, Grad Norm: 0.070797
11940.1s 1067 Epoch 4, Step 9600, Loss: 0.1936, LR: 0.000006, Grad Norm: 0.095661
11940.3s 1068 Epoch 4, Step 9600, Loss: 0.1936, LR: 0.000006, Grad Norm: 0.095661
11940.6s 1069 Epoch 4, Step 9600, Loss: 0.1936, LR: 0.000006, Grad Norm: 0.095661
11940.9s 1070 Epoch 4, Step 9600, Loss: 0.1935, LR: 0.000006, Grad Norm: 0.095661
11998.6s 1071 Epoch 4, Step 9650, Loss: 0.1944, LR: 0.000006, Grad Norm: 0.039337
11998.8s 1072 Epoch 4, Step 9650, Loss: 0.1944, LR: 0.000006, Grad Norm: 0.039337
11999.1s 1073 Epoch 4, Step 9650, Loss: 0.1943, LR: 0.000006, Grad Norm: 0.039337
11999.4s 1074 Epoch 4, Step 9650, Loss: 0.1942, LR: 0.000006, Grad Norm: 0.039337
12056.9s 1075 Epoch 4, Step 9700, Loss: 0.1930, LR: 0.000006, Grad Norm: 0.146204
12057.1s 1076 Epoch 4, Step 9700, Loss: 0.1931, LR: 0.000006, Grad Norm: 0.146204
12057.4s 1077 Epoch 4, Step 9700, Loss: 0.1932, LR: 0.000006, Grad Norm: 0.146204
12057.6s 1078 Epoch 4, Step 9700, Loss: 0.1932, LR: 0.000006, Grad Norm: 0.146204
12115.5s 1079 Epoch 4, Step 9750, Loss: 0.1929, LR: 0.000006, Grad Norm: 0.136314
12115.6s 1080 Epoch 4, Step 9750, Loss: 0.1929, LR: 0.000006, Grad Norm: 0.136314
12115.9s 1081 Epoch 4, Step 9750, Loss: 0.1928, LR: 0.000006, Grad Norm: 0.136314
12116.2s 1082 Epoch 4, Step 9750, Loss: 0.1929, LR: 0.000006, Grad Norm: 0.136314
12173.7s 1083 Epoch 4, Step 9800, Loss: 0.1924, LR: 0.000006, Grad Norm: 0.097770
12173.9s 1084 Epoch 4, Step 9800, Loss: 0.1924, LR: 0.000006, Grad Norm: 0.097770
12174.2s 1085 Epoch 4, Step 9800, Loss: 0.1924, LR: 0.000006, Grad Norm: 0.097770
12174.4s 1086 Epoch 4, Step 9800, Loss: 0.1923, LR: 0.000006, Grad Norm: 0.097770
12232.1s 1087 Epoch 4, Step 9850, Loss: 0.1913, LR: 0.000006, Grad Norm: 0.106049
12232.3s 1088 Epoch 4, Step 9850, Loss: 0.1912, LR: 0.000006, Grad Norm: 0.106049
12232.6s 1089 Epoch 4, Step 9850, Loss: 0.1912, LR: 0.000006, Grad Norm: 0.106049
12232.8s 1090 Epoch 4, Step 9850, Loss: 0.1912, LR: 0.000006, Grad Norm: 0.106049
12290.2s 1091 Epoch 4, Step 9900, Loss: 0.1913, LR: 0.000006, Grad Norm: 0.106292
12290.4s 1092 Epoch 4, Step 9900, Loss: 0.1913, LR: 0.000006, Grad Norm: 0.106292
12290.7s 1093 Epoch 4, Step 9900, Loss: 0.1912, LR: 0.000006, Grad Norm: 0.106292
12291.0s 1094 Epoch 4, Step 9900, Loss: 0.1912, LR: 0.000006, Grad Norm: 0.106292
12348.9s 1095 Epoch 4, Step 9950, Loss: 0.1919, LR: 0.000006, Grad Norm: 0.089886
12349.1s 1096 Epoch 4, Step 9950, Loss: 0.1918, LR: 0.000006, Grad Norm: 0.089886
12349.4s 1097 Epoch 4, Step 9950, Loss: 0.1918, LR: 0.000006, Grad Norm: 0.089886
12349.7s 1098 Epoch 4, Step 9950, Loss: 0.1917, LR: 0.000006, Grad Norm: 0.089886
12407.3s 1099 Epoch 4, Step 10000, Loss: 0.1917, LR: 0.000006, Grad Norm: 0.033673
12407.5s 1100 Epoch 4, Step 10000, Loss: 0.1917, LR: 0.000006, Grad Norm: 0.033673
12407.8s 1101 Epoch 4, Step 10000, Loss: 0.1918, LR: 0.000006, Grad Norm: 0.033673
12408.0s 1102 Epoch 4, Step 10000, Loss: 0.1918, LR: 0.000006, Grad Norm: 0.033673
12466.0s 1103 Epoch 4, Step 10050, Loss: 0.1911, LR: 0.000006, Grad Norm: 0.071472
12466.2s 1104 Epoch 4, Step 10050, Loss: 0.1910, LR: 0.000006, Grad Norm: 0.071472
12466.5s 1105 Epoch 4, Step 10050, Loss: 0.1910, LR: 0.000006, Grad Norm: 0.071472
12466.7s 1106 Epoch 4, Step 10050, Loss: 0.1910, LR: 0.000006, Grad Norm: 0.071472
12524.4s 1107 Epoch 4, Step 10100, Loss: 0.1904, LR: 0.000006, Grad Norm: 0.089690
12524.6s 1108 Epoch 4, Step 10100, Loss: 0.1904, LR: 0.000006, Grad Norm: 0.089690
12524.9s 1109 Epoch 4, Step 10100, Loss: 0.1903, LR: 0.000006, Grad Norm: 0.089690
12525.2s 1110 Epoch 4, Step 10100, Loss: 0.1904, LR: 0.000006, Grad Norm: 0.089690
12583.1s 1111 Epoch 4, Step 10150, Loss: 0.1900, LR: 0.000006, Grad Norm: 0.044764
12583.3s 1112 Epoch 4, Step 10150, Loss: 0.1901, LR: 0.000006, Grad Norm: 0.044764
12583.6s 1113 Epoch 4, Step 10150, Loss: 0.1901, LR: 0.000006, Grad Norm: 0.044764
12583.9s 1114 Epoch 4, Step 10150, Loss: 0.1902, LR: 0.000006, Grad Norm: 0.044764
12641.6s 1115 Epoch 4, Step 10200, Loss: 0.1894, LR: 0.000006, Grad Norm: 0.117242
12641.8s 1116 Epoch 4, Step 10200, Loss: 0.1894, LR: 0.000006, Grad Norm: 0.117242
12642.1s 1117 Epoch 4, Step 10200, Loss: 0.1894, LR: 0.000006, Grad Norm: 0.117242
12642.4s 1118 Epoch 4, Step 10200, Loss: 0.1893, LR: 0.000006, Grad Norm: 0.117242
12700.3s 1119 Epoch 4, Step 10250, Loss: 0.1891, LR: 0.000006, Grad Norm: 0.112512
12700.5s 1120 Epoch 4, Step 10250, Loss: 0.1890, LR: 0.000006, Grad Norm: 0.112512
12700.8s 1121 Epoch 4, Step 10250, Loss: 0.1890, LR: 0.000006, Grad Norm: 0.112512
12701.1s 1122 Epoch 4, Step 10250, Loss: 0.1890, LR: 0.000006, Grad Norm: 0.112512
12758.4s 1123 Epoch 4, Step 10300, Loss: 0.1887, LR: 0.000006, Grad Norm: 0.107125
12758.6s 1124 Epoch 4, Step 10300, Loss: 0.1887, LR: 0.000006, Grad Norm: 0.107125
12758.9s 1125 Epoch 4, Step 10300, Loss: 0.1886, LR: 0.000006, Grad Norm: 0.107125
12759.2s 1126 Epoch 4, Step 10300, Loss: 0.1887, LR: 0.000006, Grad Norm: 0.107125
12817.0s 1127 Epoch 4, Step 10350, Loss: 0.1887, LR: 0.000006, Grad Norm: 0.150350
12817.2s 1128 Epoch 4, Step 10350, Loss: 0.1887, LR: 0.000006, Grad Norm: 0.150350
12817.5s 1129 Epoch 4, Step 10350, Loss: 0.1888, LR: 0.000006, Grad Norm: 0.150350
12817.8s 1130 Epoch 4, Step 10350, Loss: 0.1888, LR: 0.000006, Grad Norm: 0.150350
12875.1s 1131 Epoch 4, Step 10400, Loss: 0.1880, LR: 0.000006, Grad Norm: 0.078511
12875.3s 1132 Epoch 4, Step 10400, Loss: 0.1880, LR: 0.000006, Grad Norm: 0.078511
12875.6s 1133 Epoch 4, Step 10400, Loss: 0.1880, LR: 0.000006, Grad Norm: 0.078511
12875.9s 1134 Epoch 4, Step 10400, Loss: 0.1881, LR: 0.000006, Grad Norm: 0.078511
12933.9s 1135 Epoch 4, Step 10450, Loss: 0.1885, LR: 0.000006, Grad Norm: 0.059848
12934.1s 1136 Epoch 4, Step 10450, Loss: 0.1885, LR: 0.000006, Grad Norm: 0.059848
12934.4s 1137 Epoch 4, Step 10450, Loss: 0.1885, LR: 0.000006, Grad Norm: 0.059848
12934.6s 1138 Epoch 4, Step 10450, Loss: 0.1884, LR: 0.000006, Grad Norm: 0.059848
12992.5s 1139 Epoch 4, Step 10500, Loss: 0.1878, LR: 0.000006, Grad Norm: 0.031414
12992.7s 1140 Epoch 4, Step 10500, Loss: 0.1877, LR: 0.000006, Grad Norm: 0.031414
12992.9s 1141 Epoch 4, Step 10500, Loss: 0.1878, LR: 0.000006, Grad Norm: 0.031414
12993.2s 1142 Epoch 4, Step 10500, Loss: 0.1878, LR: 0.000006, Grad Norm: 0.031414
13050.8s 1143 Epoch 4, Step 10550, Loss: 0.1880, LR: 0.000006, Grad Norm: 0.077363
13051.1s 1144 Epoch 4, Step 10550, Loss: 0.1880, LR: 0.000006, Grad Norm: 0.077363
13051.3s 1145 Epoch 4, Step 10550, Loss: 0.1880, LR: 0.000006, Grad Norm: 0.077363
13051.6s 1146 Epoch 4, Step 10550, Loss: 0.1880, LR: 0.000006, Grad Norm: 0.077363
13109.4s 1147 Epoch 4, Step 10600, Loss: 0.1869, LR: 0.000006, Grad Norm: 0.064947
13109.7s 1148 Epoch 4, Step 10600, Loss: 0.1869, LR: 0.000006, Grad Norm: 0.064947
13109.9s 1149 Epoch 4, Step 10600, Loss: 0.1869, LR: 0.000006, Grad Norm: 0.064947
13110.2s 1150 Epoch 4, Step 10600, Loss: 0.1869, LR: 0.000006, Grad Norm: 0.064947
13168.5s 1151 Epoch 4, Step 10650, Loss: 0.1866, LR: 0.000005, Grad Norm: 0.034967
13168.7s 1152 Epoch 4, Step 10650, Loss: 0.1866, LR: 0.000005, Grad Norm: 0.034967
13169.0s 1153 Epoch 4, Step 10650, Loss: 0.1866, LR: 0.000005, Grad Norm: 0.034967
13169.2s 1154 Epoch 4, Step 10650, Loss: 0.1865, LR: 0.000005, Grad Norm: 0.034967
13227.1s 1155 Epoch 4, Step 10700, Loss: 0.1869, LR: 0.000005, Grad Norm: 0.095675
13227.3s 1156 Epoch 4, Step 10700, Loss: 0.1869, LR: 0.000005, Grad Norm: 0.095675
13227.6s 1157 Epoch 4, Step 10700, Loss: 0.1868, LR: 0.000005, Grad Norm: 0.095675
13227.9s 1158 Epoch 4, Step 10700, Loss: 0.1868, LR: 0.000005, Grad Norm: 0.095675
13285.3s 1159 Epoch 4, Step 10750, Loss: 0.1867, LR: 0.000005, Grad Norm: 0.134312
13285.5s 1160 Epoch 4, Step 10750, Loss: 0.1867, LR: 0.000005, Grad Norm: 0.134312
13285.8s 1161 Epoch 4, Step 10750, Loss: 0.1867, LR: 0.000005, Grad Norm: 0.134312
13286.1s 1162 Epoch 4, Step 10750, Loss: 0.1868, LR: 0.000005, Grad Norm: 0.134312
13343.3s 1163 Epoch 4, Step 10800, Loss: 0.1862, LR: 0.000005, Grad Norm: 0.061577
13343.5s 1164 Epoch 4, Step 10800, Loss: 0.1862, LR: 0.000005, Grad Norm: 0.061577
13343.8s 1165 Epoch 4, Step 10800, Loss: 0.1862, LR: 0.000005, Grad Norm: 0.061577
13344.0s 1166 Epoch 4, Step 10800, Loss: 0.1862, LR: 0.000005, Grad Norm: 0.061577
13401.6s 1167 Epoch 4, Step 10850, Loss: 0.1864, LR: 0.000005, Grad Norm: 0.126272
13401.8s 1168 Epoch 4, Step 10850, Loss: 0.1864, LR: 0.000005, Grad Norm: 0.126272
13402.1s 1169 Epoch 4, Step 10850, Loss: 0.1865, LR: 0.000005, Grad Norm: 0.126272
13402.4s 1170 Epoch 4, Step 10850, Loss: 0.1864, LR: 0.000005, Grad Norm: 0.126272
13459.5s 1171 Epoch 4, Step 10900, Loss: 0.1872, LR: 0.000005, Grad Norm: 0.096421
13459.7s 1172 Epoch 4, Step 10900, Loss: 0.1872, LR: 0.000005, Grad Norm: 0.096421
13460.0s 1173 Epoch 4, Step 10900, Loss: 0.1872, LR: 0.000005, Grad Norm: 0.096421
13460.2s 1174 Epoch 4, Step 10900, Loss: 0.1872, LR: 0.000005, Grad Norm: 0.096421
13518.0s 1175 Epoch 4, Step 10950, Loss: 0.1870, LR: 0.000005, Grad Norm: 0.064789
13518.2s 1176 Epoch 4, Step 10950, Loss: 0.1870, LR: 0.000005, Grad Norm: 0.064789
13518.5s 1177 Epoch 4, Step 10950, Loss: 0.1870, LR: 0.000005, Grad Norm: 0.064789
13518.8s 1178 Epoch 4, Step 10950, Loss: 0.1870, LR: 0.000005, Grad Norm: 0.064789
13576.1s 1179 Epoch 4, Step 11000, Loss: 0.1869, LR: 0.000005, Grad Norm: 0.118174
13576.3s 1180 Epoch 4, Step 11000, Loss: 0.1869, LR: 0.000005, Grad Norm: 0.118174
13576.6s 1181 Epoch 4, Step 11000, Loss: 0.1869, LR: 0.000005, Grad Norm: 0.118174
13576.9s 1182 Epoch 4, Step 11000, Loss: 0.1869, LR: 0.000005, Grad Norm: 0.118174
13634.8s 1183 Epoch 4, Step 11050, Loss: 0.1862, LR: 0.000005, Grad Norm: 0.085413
13635.0s 1184 Epoch 4, Step 11050, Loss: 0.1862, LR: 0.000005, Grad Norm: 0.085413
13635.3s 1185 Epoch 4, Step 11050, Loss: 0.1862, LR: 0.000005, Grad Norm: 0.085413
13635.6s 1186 Epoch 4, Step 11050, Loss: 0.1862, LR: 0.000005, Grad Norm: 0.085413
13693.2s 1187 Epoch 4, Step 11100, Loss: 0.1856, LR: 0.000005, Grad Norm: 0.090044
13693.4s 1188 Epoch 4, Step 11100, Loss: 0.1856, LR: 0.000005, Grad Norm: 0.090044
13693.7s 1189 Epoch 4, Step 11100, Loss: 0.1856, LR: 0.000005, Grad Norm: 0.090044
13694.0s 1190 Epoch 4, Step 11100, Loss: 0.1856, LR: 0.000005, Grad Norm: 0.090044
13752.0s 1191 Epoch 4, Step 11150, Loss: 0.1858, LR: 0.000005, Grad Norm: 0.083425
13752.2s 1192 Epoch 4, Step 11150, Loss: 0.1858, LR: 0.000005, Grad Norm: 0.083425
13752.5s 1193 Epoch 4, Step 11150, Loss: 0.1858, LR: 0.000005, Grad Norm: 0.083425
13752.8s 1194 Epoch 4, Step 11150, Loss: 0.1858, LR: 0.000005, Grad Norm: 0.083425
13810.6s 1195 Epoch 4, Step 11200, Loss: 0.1855, LR: 0.000005, Grad Norm: 0.177089
13810.8s 1196 Epoch 4, Step 11200, Loss: 0.1855, LR: 0.000005, Grad Norm: 0.177089
13811.1s 1197 Epoch 4, Step 11200, Loss: 0.1855, LR: 0.000005, Grad Norm: 0.177089
13811.4s 1198 Epoch 4, Step 11200, Loss: 0.1855, LR: 0.000005, Grad Norm: 0.177089
13869.8s 1199 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
13869.8s 1200 To disable this warning, you can either:
13869.8s 1201 - Avoid using `tokenizers` before the fork if possible
13869.8s 1202 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
13869.8s 1203 Epoch 4, Step 11250, Loss: 0.1852, LR: 0.000005, Grad Norm: 0.080427
13869.8s 1204 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
13869.8s 1205 To disable this warning, you can either:
13869.8s 1206 - Avoid using `tokenizers` before the fork if possible
13869.8s 1207 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
13869.8s 1208 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
13869.8s 1209 To disable this warning, you can either:
13869.8s 1210 - Avoid using `tokenizers` before the fork if possible
13869.8s 1211 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
13870.0s 1212 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
13870.0s 1213 To disable this warning, you can either:
13870.0s 1214 - Avoid using `tokenizers` before the fork if possible
13870.0s 1215 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
13975.9s 1216 Epoch 4 completed. Train Loss: 0.1852, Validation Loss: 0.1846
13975.9s 1217 Selected animals for validation CLIP score: ['chimpanzee', 'horse', 'moth', 'spider', 'cat']
13975.9s 1218 Expected types for unet: (<class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionModel'>,), got <class 'peft.peft_model.PeftModel'>.
13975.9s 1219 You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
13980.0s 1220 Animal: chimpanzee, CLIP Score: 30.0180
13982.6s 1221 Animal: horse, CLIP Score: 27.7810
13985.1s 1222 Animal: moth, CLIP Score: 30.0374
13987.7s 1223 Animal: spider, CLIP Score: 27.0373
13990.3s 1224 Animal: cat, CLIP Score: 26.7871
13990.3s 1225 Average Validation CLIP Score: 28.3322
13990.3s 1226 EarlyStopping counter: 3 out of 5
13990.3s 1227 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
13990.3s 1228 To disable this warning, you can either:
13990.3s 1229 - Avoid using `tokenizers` before the fork if possible
13990.3s 1230 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
13990.4s 1231 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
13990.4s 1232 To disable this warning, you can either:
13990.4s 1233 - Avoid using `tokenizers` before the fork if possible
13990.4s 1234 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
13990.4s 1235 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
13990.4s 1236 To disable this warning, you can either:
13990.4s 1237 - Avoid using `tokenizers` before the fork if possible
13990.4s 1238 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
13990.5s 1239 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
13990.5s 1240 To disable this warning, you can either:
13990.5s 1241 - Avoid using `tokenizers` before the fork if possible
13990.5s 1242 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
13991.0s 1243 Epoch 5, Step 11250, Loss: 0.4797, LR: 0.000005, Grad Norm: 0.000000
13991.3s 1244 Epoch 5, Step 11250, Loss: 0.5445, LR: 0.000005, Grad Norm: 0.000000
13991.6s 1245 Epoch 5, Step 11250, Loss: 0.5331, LR: 0.000005, Grad Norm: 0.000000
14050.5s 1246 Epoch 5, Step 11300, Loss: 0.1837, LR: 0.000005, Grad Norm: 0.071586
14050.7s 1247 Epoch 5, Step 11300, Loss: 0.1863, LR: 0.000005, Grad Norm: 0.071586
14051.0s 1248 Epoch 5, Step 11300, Loss: 0.1854, LR: 0.000005, Grad Norm: 0.071586
14051.2s 1249 Epoch 5, Step 11300, Loss: 0.1847, LR: 0.000005, Grad Norm: 0.071586
14110.2s 1250 Epoch 5, Step 11350, Loss: 0.1790, LR: 0.000005, Grad Norm: 0.129836
14110.4s 1251 Epoch 5, Step 11350, Loss: 0.1786, LR: 0.000005, Grad Norm: 0.129836
14110.7s 1252 Epoch 5, Step 11350, Loss: 0.1783, LR: 0.000005, Grad Norm: 0.129836
14110.9s 1253 Epoch 5, Step 11350, Loss: 0.1780, LR: 0.000005, Grad Norm: 0.129836
14169.3s 1254 Epoch 5, Step 11400, Loss: 0.1772, LR: 0.000005, Grad Norm: 0.073623
14169.5s 1255 Epoch 5, Step 11400, Loss: 0.1769, LR: 0.000005, Grad Norm: 0.073623
14169.8s 1256 Epoch 5, Step 11400, Loss: 0.1766, LR: 0.000005, Grad Norm: 0.073623
14170.1s 1257 Epoch 5, Step 11400, Loss: 0.1764, LR: 0.000005, Grad Norm: 0.073623
14228.6s 1258 Epoch 5, Step 11450, Loss: 0.1795, LR: 0.000005, Grad Norm: 0.136954
14228.8s 1259 Epoch 5, Step 11450, Loss: 0.1793, LR: 0.000005, Grad Norm: 0.136954
14229.1s 1260 Epoch 5, Step 11450, Loss: 0.1792, LR: 0.000005, Grad Norm: 0.136954
14229.4s 1261 Epoch 5, Step 11450, Loss: 0.1790, LR: 0.000005, Grad Norm: 0.136954
14288.2s 1262 Epoch 5, Step 11500, Loss: 0.1771, LR: 0.000005, Grad Norm: 0.071520
14288.4s 1263 Epoch 5, Step 11500, Loss: 0.1774, LR: 0.000005, Grad Norm: 0.071520
14288.7s 1264 Epoch 5, Step 11500, Loss: 0.1778, LR: 0.000005, Grad Norm: 0.071520
14289.0s 1265 Epoch 5, Step 11500, Loss: 0.1777, LR: 0.000005, Grad Norm: 0.071520
14347.7s 1266 Epoch 5, Step 11550, Loss: 0.1754, LR: 0.000005, Grad Norm: 0.093871
14348.0s 1267 Epoch 5, Step 11550, Loss: 0.1758, LR: 0.000005, Grad Norm: 0.093871
14348.2s 1268 Epoch 5, Step 11550, Loss: 0.1757, LR: 0.000005, Grad Norm: 0.093871
14348.5s 1269 Epoch 5, Step 11550, Loss: 0.1757, LR: 0.000005, Grad Norm: 0.093871
14406.9s 1270 Epoch 5, Step 11600, Loss: 0.1756, LR: 0.000005, Grad Norm: 0.052446
14407.2s 1271 Epoch 5, Step 11600, Loss: 0.1755, LR: 0.000005, Grad Norm: 0.052446
14407.4s 1272 Epoch 5, Step 11600, Loss: 0.1760, LR: 0.000005, Grad Norm: 0.052446
14407.7s 1273 Epoch 5, Step 11600, Loss: 0.1759, LR: 0.000005, Grad Norm: 0.052446
14466.3s 1274 Epoch 5, Step 11650, Loss: 0.1763, LR: 0.000005, Grad Norm: 0.072234
14466.5s 1275 Epoch 5, Step 11650, Loss: 0.1762, LR: 0.000005, Grad Norm: 0.072234
14466.8s 1276 Epoch 5, Step 11650, Loss: 0.1762, LR: 0.000005, Grad Norm: 0.072234
14467.1s 1277 Epoch 5, Step 11650, Loss: 0.1762, LR: 0.000005, Grad Norm: 0.072234
14525.0s 1278 Epoch 5, Step 11700, Loss: 0.1770, LR: 0.000005, Grad Norm: 0.017814
14525.2s 1279 Epoch 5, Step 11700, Loss: 0.1770, LR: 0.000005, Grad Norm: 0.017814
14525.5s 1280 Epoch 5, Step 11700, Loss: 0.1769, LR: 0.000005, Grad Norm: 0.017814
14525.8s 1281 Epoch 5, Step 11700, Loss: 0.1771, LR: 0.000005, Grad Norm: 0.017814
14584.2s 1282 Epoch 5, Step 11750, Loss: 0.1817, LR: 0.000005, Grad Norm: 0.080627
14584.4s 1283 Epoch 5, Step 11750, Loss: 0.1817, LR: 0.000005, Grad Norm: 0.080627
14584.7s 1284 Epoch 5, Step 11750, Loss: 0.1816, LR: 0.000005, Grad Norm: 0.080627
14585.0s 1285 Epoch 5, Step 11750, Loss: 0.1816, LR: 0.000005, Grad Norm: 0.080627
14643.1s 1286 Epoch 5, Step 11800, Loss: 0.1790, LR: 0.000005, Grad Norm: 0.069404
14643.4s 1287 Epoch 5, Step 11800, Loss: 0.1791, LR: 0.000005, Grad Norm: 0.069404
14643.6s 1288 Epoch 5, Step 11800, Loss: 0.1794, LR: 0.000005, Grad Norm: 0.069404
14643.9s 1289 Epoch 5, Step 11800, Loss: 0.1793, LR: 0.000005, Grad Norm: 0.069404
14702.3s 1290 Epoch 5, Step 11850, Loss: 0.1766, LR: 0.000005, Grad Norm: 0.064846
14702.5s 1291 Epoch 5, Step 11850, Loss: 0.1766, LR: 0.000005, Grad Norm: 0.064846
14702.8s 1292 Epoch 5, Step 11850, Loss: 0.1765, LR: 0.000005, Grad Norm: 0.064846
14703.1s 1293 Epoch 5, Step 11850, Loss: 0.1764, LR: 0.000005, Grad Norm: 0.064846
14761.3s 1294 Epoch 5, Step 11900, Loss: 0.1769, LR: 0.000005, Grad Norm: 0.091986
14761.5s 1295 Epoch 5, Step 11900, Loss: 0.1768, LR: 0.000005, Grad Norm: 0.091986
14762.0s 1296 Epoch 5, Step 11900, Loss: 0.1768, LR: 0.000005, Grad Norm: 0.091986
14762.5s 1297 Epoch 5, Step 11900, Loss: 0.1768, LR: 0.000005, Grad Norm: 0.091986
14820.5s 1298 Epoch 5, Step 11950, Loss: 0.1783, LR: 0.000005, Grad Norm: 0.060050
14820.7s 1299 Epoch 5, Step 11950, Loss: 0.1784, LR: 0.000005, Grad Norm: 0.060050
14821.0s 1300 Epoch 5, Step 11950, Loss: 0.1784, LR: 0.000005, Grad Norm: 0.060050
14821.2s 1301 Epoch 5, Step 11950, Loss: 0.1783, LR: 0.000005, Grad Norm: 0.060050
14879.4s 1302 Epoch 5, Step 12000, Loss: 0.1782, LR: 0.000005, Grad Norm: 0.065881
14879.9s 1303 Epoch 5, Step 12000, Loss: 0.1782, LR: 0.000005, Grad Norm: 0.065881
14880.3s 1304 Epoch 5, Step 12000, Loss: 0.1781, LR: 0.000005, Grad Norm: 0.065881
14880.6s 1305 Epoch 5, Step 12000, Loss: 0.1781, LR: 0.000005, Grad Norm: 0.065881
14938.6s 1306 Epoch 5, Step 12050, Loss: 0.1771, LR: 0.000005, Grad Norm: 0.108887
14938.8s 1307 Epoch 5, Step 12050, Loss: 0.1771, LR: 0.000005, Grad Norm: 0.108887
14939.1s 1308 Epoch 5, Step 12050, Loss: 0.1770, LR: 0.000005, Grad Norm: 0.108887
14939.4s 1309 Epoch 5, Step 12050, Loss: 0.1771, LR: 0.000005, Grad Norm: 0.108887
14997.4s 1310 Epoch 5, Step 12100, Loss: 0.1782, LR: 0.000004, Grad Norm: 0.099201
14997.9s 1311 Epoch 5, Step 12100, Loss: 0.1783, LR: 0.000004, Grad Norm: 0.099201
14998.3s 1312 Epoch 5, Step 12100, Loss: 0.1784, LR: 0.000004, Grad Norm: 0.099201
14998.6s 1313 Epoch 5, Step 12100, Loss: 0.1785, LR: 0.000004, Grad Norm: 0.099201
15056.7s 1314 Epoch 5, Step 12150, Loss: 0.1779, LR: 0.000004, Grad Norm: 0.100146
15056.9s 1315 Epoch 5, Step 12150, Loss: 0.1781, LR: 0.000004, Grad Norm: 0.100146
15057.2s 1316 Epoch 5, Step 12150, Loss: 0.1780, LR: 0.000004, Grad Norm: 0.100146
15057.5s 1317 Epoch 5, Step 12150, Loss: 0.1780, LR: 0.000004, Grad Norm: 0.100146
15115.9s 1318 Epoch 5, Step 12200, Loss: 0.1774, LR: 0.000004, Grad Norm: 0.148973
15116.1s 1319 Epoch 5, Step 12200, Loss: 0.1774, LR: 0.000004, Grad Norm: 0.148973
15116.4s 1320 Epoch 5, Step 12200, Loss: 0.1773, LR: 0.000004, Grad Norm: 0.148973
15116.7s 1321 Epoch 5, Step 12200, Loss: 0.1774, LR: 0.000004, Grad Norm: 0.148973
15174.6s 1322 Epoch 5, Step 12250, Loss: 0.1764, LR: 0.000004, Grad Norm: 0.092507
15174.9s 1323 Epoch 5, Step 12250, Loss: 0.1764, LR: 0.000004, Grad Norm: 0.092507
15175.1s 1324 Epoch 5, Step 12250, Loss: 0.1766, LR: 0.000004, Grad Norm: 0.092507
15175.4s 1325 Epoch 5, Step 12250, Loss: 0.1766, LR: 0.000004, Grad Norm: 0.092507
15233.5s 1326 Epoch 5, Step 12300, Loss: 0.1761, LR: 0.000004, Grad Norm: 0.057316
15233.7s 1327 Epoch 5, Step 12300, Loss: 0.1761, LR: 0.000004, Grad Norm: 0.057316
15234.0s 1328 Epoch 5, Step 12300, Loss: 0.1760, LR: 0.000004, Grad Norm: 0.057316
15234.3s 1329 Epoch 5, Step 12300, Loss: 0.1760, LR: 0.000004, Grad Norm: 0.057316
15292.3s 1330 Epoch 5, Step 12350, Loss: 0.1770, LR: 0.000004, Grad Norm: 0.049260
15292.5s 1331 Epoch 5, Step 12350, Loss: 0.1770, LR: 0.000004, Grad Norm: 0.049260
15292.8s 1332 Epoch 5, Step 12350, Loss: 0.1770, LR: 0.000004, Grad Norm: 0.049260
15293.0s 1333 Epoch 5, Step 12350, Loss: 0.1769, LR: 0.000004, Grad Norm: 0.049260
15351.3s 1334 Epoch 5, Step 12400, Loss: 0.1760, LR: 0.000004, Grad Norm: 0.085139
15351.5s 1335 Epoch 5, Step 12400, Loss: 0.1761, LR: 0.000004, Grad Norm: 0.085139
15351.8s 1336 Epoch 5, Step 12400, Loss: 0.1761, LR: 0.000004, Grad Norm: 0.085139
15352.1s 1337 Epoch 5, Step 12400, Loss: 0.1760, LR: 0.000004, Grad Norm: 0.085139
15410.2s 1338 Epoch 5, Step 12450, Loss: 0.1757, LR: 0.000004, Grad Norm: 0.084462
15410.5s 1339 Epoch 5, Step 12450, Loss: 0.1757, LR: 0.000004, Grad Norm: 0.084462
15410.7s 1340 Epoch 5, Step 12450, Loss: 0.1757, LR: 0.000004, Grad Norm: 0.084462
15411.0s 1341 Epoch 5, Step 12450, Loss: 0.1756, LR: 0.000004, Grad Norm: 0.084462
15469.4s 1342 Epoch 5, Step 12500, Loss: 0.1758, LR: 0.000004, Grad Norm: 0.098681
15469.6s 1343 Epoch 5, Step 12500, Loss: 0.1758, LR: 0.000004, Grad Norm: 0.098681
15469.9s 1344 Epoch 5, Step 12500, Loss: 0.1758, LR: 0.000004, Grad Norm: 0.098681
15470.2s 1345 Epoch 5, Step 12500, Loss: 0.1758, LR: 0.000004, Grad Norm: 0.098681
15528.2s 1346 Epoch 5, Step 12550, Loss: 0.1761, LR: 0.000004, Grad Norm: 0.081874
15528.4s 1347 Epoch 5, Step 12550, Loss: 0.1760, LR: 0.000004, Grad Norm: 0.081874
15528.7s 1348 Epoch 5, Step 12550, Loss: 0.1760, LR: 0.000004, Grad Norm: 0.081874
15529.0s 1349 Epoch 5, Step 12550, Loss: 0.1760, LR: 0.000004, Grad Norm: 0.081874
15587.3s 1350 Epoch 5, Step 12600, Loss: 0.1769, LR: 0.000004, Grad Norm: 0.101956
15587.5s 1351 Epoch 5, Step 12600, Loss: 0.1769, LR: 0.000004, Grad Norm: 0.101956
15587.8s 1352 Epoch 5, Step 12600, Loss: 0.1769, LR: 0.000004, Grad Norm: 0.101956
15588.1s 1353 Epoch 5, Step 12600, Loss: 0.1769, LR: 0.000004, Grad Norm: 0.101956
15646.0s 1354 Epoch 5, Step 12650, Loss: 0.1766, LR: 0.000004, Grad Norm: 0.088965
15646.2s 1355 Epoch 5, Step 12650, Loss: 0.1765, LR: 0.000004, Grad Norm: 0.088965
15646.5s 1356 Epoch 5, Step 12650, Loss: 0.1765, LR: 0.000004, Grad Norm: 0.088965
15646.8s 1357 Epoch 5, Step 12650, Loss: 0.1765, LR: 0.000004, Grad Norm: 0.088965
15705.2s 1358 Epoch 5, Step 12700, Loss: 0.1762, LR: 0.000004, Grad Norm: 0.101574
15705.4s 1359 Epoch 5, Step 12700, Loss: 0.1762, LR: 0.000004, Grad Norm: 0.101574
15705.6s 1360 Epoch 5, Step 12700, Loss: 0.1762, LR: 0.000004, Grad Norm: 0.101574
15705.9s 1361 Epoch 5, Step 12700, Loss: 0.1762, LR: 0.000004, Grad Norm: 0.101574
15763.9s 1362 Epoch 5, Step 12750, Loss: 0.1762, LR: 0.000004, Grad Norm: 0.102178
15764.1s 1363 Epoch 5, Step 12750, Loss: 0.1762, LR: 0.000004, Grad Norm: 0.102178
15764.4s 1364 Epoch 5, Step 12750, Loss: 0.1762, LR: 0.000004, Grad Norm: 0.102178
15764.7s 1365 Epoch 5, Step 12750, Loss: 0.1762, LR: 0.000004, Grad Norm: 0.102178
15823.1s 1366 Epoch 5, Step 12800, Loss: 0.1765, LR: 0.000004, Grad Norm: 0.116960
15823.3s 1367 Epoch 5, Step 12800, Loss: 0.1765, LR: 0.000004, Grad Norm: 0.116960
15823.6s 1368 Epoch 5, Step 12800, Loss: 0.1766, LR: 0.000004, Grad Norm: 0.116960
15823.9s 1369 Epoch 5, Step 12800, Loss: 0.1765, LR: 0.000004, Grad Norm: 0.116960
15882.2s 1370 Epoch 5, Step 12850, Loss: 0.1768, LR: 0.000004, Grad Norm: 0.075028
15882.4s 1371 Epoch 5, Step 12850, Loss: 0.1768, LR: 0.000004, Grad Norm: 0.075028
15882.7s 1372 Epoch 5, Step 12850, Loss: 0.1767, LR: 0.000004, Grad Norm: 0.075028
15883.0s 1373 Epoch 5, Step 12850, Loss: 0.1767, LR: 0.000004, Grad Norm: 0.075028
15941.3s 1374 Epoch 5, Step 12900, Loss: 0.1775, LR: 0.000004, Grad Norm: 0.063071
15941.5s 1375 Epoch 5, Step 12900, Loss: 0.1775, LR: 0.000004, Grad Norm: 0.063071
15941.8s 1376 Epoch 5, Step 12900, Loss: 0.1775, LR: 0.000004, Grad Norm: 0.063071
15942.1s 1377 Epoch 5, Step 12900, Loss: 0.1776, LR: 0.000004, Grad Norm: 0.063071
16000.5s 1378 Epoch 5, Step 12950, Loss: 0.1777, LR: 0.000004, Grad Norm: 0.080178
16000.7s 1379 Epoch 5, Step 12950, Loss: 0.1777, LR: 0.000004, Grad Norm: 0.080178
16001.0s 1380 Epoch 5, Step 12950, Loss: 0.1777, LR: 0.000004, Grad Norm: 0.080178
16001.3s 1381 Epoch 5, Step 12950, Loss: 0.1777, LR: 0.000004, Grad Norm: 0.080178
16059.8s 1382 Epoch 5, Step 13000, Loss: 0.1780, LR: 0.000004, Grad Norm: 0.039880
16060.0s 1383 Epoch 5, Step 13000, Loss: 0.1781, LR: 0.000004, Grad Norm: 0.039880
16060.3s 1384 Epoch 5, Step 13000, Loss: 0.1781, LR: 0.000004, Grad Norm: 0.039880
16060.6s 1385 Epoch 5, Step 13000, Loss: 0.1781, LR: 0.000004, Grad Norm: 0.039880
16118.8s 1386 Epoch 5, Step 13050, Loss: 0.1776, LR: 0.000004, Grad Norm: 0.102674
16119.0s 1387 Epoch 5, Step 13050, Loss: 0.1776, LR: 0.000004, Grad Norm: 0.102674
16119.3s 1388 Epoch 5, Step 13050, Loss: 0.1776, LR: 0.000004, Grad Norm: 0.102674
16119.5s 1389 Epoch 5, Step 13050, Loss: 0.1776, LR: 0.000004, Grad Norm: 0.102674
16177.9s 1390 Epoch 5, Step 13100, Loss: 0.1775, LR: 0.000004, Grad Norm: 0.084397
16178.1s 1391 Epoch 5, Step 13100, Loss: 0.1775, LR: 0.000004, Grad Norm: 0.084397
16178.4s 1392 Epoch 5, Step 13100, Loss: 0.1776, LR: 0.000004, Grad Norm: 0.084397
16178.7s 1393 Epoch 5, Step 13100, Loss: 0.1776, LR: 0.000004, Grad Norm: 0.084397
16236.7s 1394 Epoch 5, Step 13150, Loss: 0.1776, LR: 0.000004, Grad Norm: 0.083228
16236.9s 1395 Epoch 5, Step 13150, Loss: 0.1776, LR: 0.000004, Grad Norm: 0.083228
16237.2s 1396 Epoch 5, Step 13150, Loss: 0.1776, LR: 0.000004, Grad Norm: 0.083228
16237.5s 1397 Epoch 5, Step 13150, Loss: 0.1776, LR: 0.000004, Grad Norm: 0.083228
16295.9s 1398 Epoch 5, Step 13200, Loss: 0.1781, LR: 0.000004, Grad Norm: 0.086981
16296.2s 1399 Epoch 5, Step 13200, Loss: 0.1781, LR: 0.000004, Grad Norm: 0.086981
16296.4s 1400 Epoch 5, Step 13200, Loss: 0.1781, LR: 0.000004, Grad Norm: 0.086981
16296.7s 1401 Epoch 5, Step 13200, Loss: 0.1781, LR: 0.000004, Grad Norm: 0.086981
16354.9s 1402 Epoch 5, Step 13250, Loss: 0.1787, LR: 0.000004, Grad Norm: 0.128404
16355.1s 1403 Epoch 5, Step 13250, Loss: 0.1787, LR: 0.000004, Grad Norm: 0.128404
16355.4s 1404 Epoch 5, Step 13250, Loss: 0.1787, LR: 0.000004, Grad Norm: 0.128404
16355.6s 1405 Epoch 5, Step 13250, Loss: 0.1787, LR: 0.000004, Grad Norm: 0.128404
16414.2s 1406 Epoch 5, Step 13300, Loss: 0.1792, LR: 0.000004, Grad Norm: 0.100960
16414.4s 1407 Epoch 5, Step 13300, Loss: 0.1792, LR: 0.000004, Grad Norm: 0.100960
16414.7s 1408 Epoch 5, Step 13300, Loss: 0.1792, LR: 0.000004, Grad Norm: 0.100960
16415.0s 1409 Epoch 5, Step 13300, Loss: 0.1792, LR: 0.000004, Grad Norm: 0.100960
16473.4s 1410 Epoch 5, Step 13350, Loss: 0.1793, LR: 0.000004, Grad Norm: 0.055075
16473.6s 1411 Epoch 5, Step 13350, Loss: 0.1793, LR: 0.000004, Grad Norm: 0.055075
16473.9s 1412 Epoch 5, Step 13350, Loss: 0.1793, LR: 0.000004, Grad Norm: 0.055075
16474.1s 1413 Epoch 5, Step 13350, Loss: 0.1794, LR: 0.000004, Grad Norm: 0.055075
16532.5s 1414 Epoch 5, Step 13400, Loss: 0.1798, LR: 0.000004, Grad Norm: 0.079434
16532.7s 1415 Epoch 5, Step 13400, Loss: 0.1798, LR: 0.000004, Grad Norm: 0.079434
16533.0s 1416 Epoch 5, Step 13400, Loss: 0.1799, LR: 0.000004, Grad Norm: 0.079434
16533.2s 1417 Epoch 5, Step 13400, Loss: 0.1799, LR: 0.000004, Grad Norm: 0.079434
16591.2s 1418 Epoch 5, Step 13450, Loss: 0.1801, LR: 0.000004, Grad Norm: 0.132112
16591.4s 1419 Epoch 5, Step 13450, Loss: 0.1801, LR: 0.000004, Grad Norm: 0.132112
16591.7s 1420 Epoch 5, Step 13450, Loss: 0.1801, LR: 0.000004, Grad Norm: 0.132112
16591.9s 1421 Epoch 5, Step 13450, Loss: 0.1801, LR: 0.000004, Grad Norm: 0.132112
16650.2s 1422 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
16650.2s 1423 To disable this warning, you can either:
16650.2s 1424 - Avoid using `tokenizers` before the fork if possible
16650.2s 1425 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
16650.2s 1426 Epoch 5, Step 13500, Loss: 0.1801, LR: 0.000004, Grad Norm: 0.024714
16650.2s 1427 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
16650.2s 1428 To disable this warning, you can either:
16650.2s 1429 - Avoid using `tokenizers` before the fork if possible
16650.2s 1430 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
16650.2s 1431 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
16650.2s 1432 To disable this warning, you can either:
16650.2s 1433 - Avoid using `tokenizers` before the fork if possible
16650.2s 1434 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
16650.4s 1435 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
16650.4s 1436 To disable this warning, you can either:
16650.4s 1437 - Avoid using `tokenizers` before the fork if possible
16650.4s 1438 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
16756.1s 1439 Epoch 5 completed. Train Loss: 0.1801, Validation Loss: 0.1844
16756.1s 1440 Selected animals for validation CLIP score: ['pangolin', 'platypus', 'capybara', 'parrot', 'turtle']
16756.2s 1441 Expected types for unet: (<class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionModel'>,), got <class 'peft.peft_model.PeftModel'>.
16756.2s 1442 You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
16760.3s 1443 Animal: pangolin, CLIP Score: 34.3486
16762.9s 1444 Animal: platypus, CLIP Score: 30.6531
16765.4s 1445 Animal: capybara, CLIP Score: 32.6390
16768.0s 1446 Animal: parrot, CLIP Score: 30.2719
16770.6s 1447 Animal: turtle, CLIP Score: 28.8620
16770.6s 1448 Average Validation CLIP Score: 31.3549
16770.6s 1449 EarlyStopping counter: 4 out of 5
16770.6s 1450 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
16770.6s 1451 To disable this warning, you can either:
16770.6s 1452 - Avoid using `tokenizers` before the fork if possible
16770.6s 1453 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
16770.7s 1454 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
16770.7s 1455 To disable this warning, you can either:
16770.7s 1456 - Avoid using `tokenizers` before the fork if possible
16770.7s 1457 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
16770.7s 1458 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
16770.7s 1459 To disable this warning, you can either:
16770.7s 1460 - Avoid using `tokenizers` before the fork if possible
16770.7s 1461 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
16770.8s 1462 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
16770.8s 1463 To disable this warning, you can either:
16770.8s 1464 - Avoid using `tokenizers` before the fork if possible
16770.8s 1465 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
16771.3s 1466 Epoch 6, Step 13500, Loss: 0.0315, LR: 0.000004, Grad Norm: 0.000000
16771.6s 1467 Epoch 6, Step 13500, Loss: 0.3102, LR: 0.000004, Grad Norm: 0.000000
16771.9s 1468 Epoch 6, Step 13500, Loss: 0.2088, LR: 0.000004, Grad Norm: 0.000000
16830.2s 1469 Epoch 6, Step 13550, Loss: 0.1883, LR: 0.000003, Grad Norm: 0.130044
16830.4s 1470 Epoch 6, Step 13550, Loss: 0.1900, LR: 0.000003, Grad Norm: 0.130044
16830.7s 1471 Epoch 6, Step 13550, Loss: 0.1892, LR: 0.000003, Grad Norm: 0.130044
16831.0s 1472 Epoch 6, Step 13550, Loss: 0.1885, LR: 0.000003, Grad Norm: 0.130044
16889.7s 1473 Epoch 6, Step 13600, Loss: 0.1824, LR: 0.000003, Grad Norm: 0.162076
16889.9s 1474 Epoch 6, Step 13600, Loss: 0.1820, LR: 0.000003, Grad Norm: 0.162076
16890.2s 1475 Epoch 6, Step 13600, Loss: 0.1829, LR: 0.000003, Grad Norm: 0.162076
16890.5s 1476 Epoch 6, Step 13600, Loss: 0.1825, LR: 0.000003, Grad Norm: 0.162076
16948.6s 1477 Epoch 6, Step 13650, Loss: 0.1848, LR: 0.000003, Grad Norm: 0.133923
16948.8s 1478 Epoch 6, Step 13650, Loss: 0.1849, LR: 0.000003, Grad Norm: 0.133923
16949.1s 1479 Epoch 6, Step 13650, Loss: 0.1851, LR: 0.000003, Grad Norm: 0.133923
16949.4s 1480 Epoch 6, Step 13650, Loss: 0.1848, LR: 0.000003, Grad Norm: 0.133923
17008.0s 1481 Epoch 6, Step 13700, Loss: 0.1840, LR: 0.000003, Grad Norm: 0.092867
17008.2s 1482 Epoch 6, Step 13700, Loss: 0.1845, LR: 0.000003, Grad Norm: 0.092867
17008.5s 1483 Epoch 6, Step 13700, Loss: 0.1844, LR: 0.000003, Grad Norm: 0.092867
17008.8s 1484 Epoch 6, Step 13700, Loss: 0.1846, LR: 0.000003, Grad Norm: 0.092867
17066.8s 1485 Epoch 6, Step 13750, Loss: 0.1848, LR: 0.000003, Grad Norm: 0.087253
17067.0s 1486 Epoch 6, Step 13750, Loss: 0.1846, LR: 0.000003, Grad Norm: 0.087253
17067.2s 1487 Epoch 6, Step 13750, Loss: 0.1844, LR: 0.000003, Grad Norm: 0.087253
17067.5s 1488 Epoch 6, Step 13750, Loss: 0.1843, LR: 0.000003, Grad Norm: 0.087253
17125.7s 1489 Epoch 6, Step 13800, Loss: 0.1826, LR: 0.000003, Grad Norm: 0.079095
17125.9s 1490 Epoch 6, Step 13800, Loss: 0.1827, LR: 0.000003, Grad Norm: 0.079095
17126.2s 1491 Epoch 6, Step 13800, Loss: 0.1826, LR: 0.000003, Grad Norm: 0.079095
17126.5s 1492 Epoch 6, Step 13800, Loss: 0.1830, LR: 0.000003, Grad Norm: 0.079095
17184.4s 1493 Epoch 6, Step 13850, Loss: 0.1881, LR: 0.000003, Grad Norm: 0.097256
17184.5s 1494 Epoch 6, Step 13850, Loss: 0.1879, LR: 0.000003, Grad Norm: 0.097256
17184.8s 1495 Epoch 6, Step 13850, Loss: 0.1881, LR: 0.000003, Grad Norm: 0.097256
17185.1s 1496 Epoch 6, Step 13850, Loss: 0.1882, LR: 0.000003, Grad Norm: 0.097256
17243.4s 1497 Epoch 6, Step 13900, Loss: 0.1842, LR: 0.000003, Grad Norm: 0.062160
17243.6s 1498 Epoch 6, Step 13900, Loss: 0.1842, LR: 0.000003, Grad Norm: 0.062160
17243.9s 1499 Epoch 6, Step 13900, Loss: 0.1841, LR: 0.000003, Grad Norm: 0.062160
17244.2s 1500 Epoch 6, Step 13900, Loss: 0.1840, LR: 0.000003, Grad Norm: 0.062160
17302.0s 1501 Epoch 6, Step 13950, Loss: 0.1853, LR: 0.000003, Grad Norm: 0.041932
17302.2s 1502 Epoch 6, Step 13950, Loss: 0.1852, LR: 0.000003, Grad Norm: 0.041932
17302.5s 1503 Epoch 6, Step 13950, Loss: 0.1851, LR: 0.000003, Grad Norm: 0.041932
17302.8s 1504 Epoch 6, Step 13950, Loss: 0.1850, LR: 0.000003, Grad Norm: 0.041932
17360.9s 1505 Epoch 6, Step 14000, Loss: 0.1857, LR: 0.000003, Grad Norm: 0.130828
17361.1s 1506 Epoch 6, Step 14000, Loss: 0.1856, LR: 0.000003, Grad Norm: 0.130828
17361.4s 1507 Epoch 6, Step 14000, Loss: 0.1856, LR: 0.000003, Grad Norm: 0.130828
17361.7s 1508 Epoch 6, Step 14000, Loss: 0.1855, LR: 0.000003, Grad Norm: 0.130828
17419.5s 1509 Epoch 6, Step 14050, Loss: 0.1851, LR: 0.000003, Grad Norm: 0.088612
17419.7s 1510 Epoch 6, Step 14050, Loss: 0.1853, LR: 0.000003, Grad Norm: 0.088612
17420.0s 1511 Epoch 6, Step 14050, Loss: 0.1852, LR: 0.000003, Grad Norm: 0.088612
17420.2s 1512 Epoch 6, Step 14050, Loss: 0.1852, LR: 0.000003, Grad Norm: 0.088612
17478.6s 1513 Epoch 6, Step 14100, Loss: 0.1836, LR: 0.000003, Grad Norm: 0.100962
17478.8s 1514 Epoch 6, Step 14100, Loss: 0.1838, LR: 0.000003, Grad Norm: 0.100962
17479.1s 1515 Epoch 6, Step 14100, Loss: 0.1839, LR: 0.000003, Grad Norm: 0.100962
17479.4s 1516 Epoch 6, Step 14100, Loss: 0.1839, LR: 0.000003, Grad Norm: 0.100962
17537.1s 1517 Epoch 6, Step 14150, Loss: 0.1830, LR: 0.000003, Grad Norm: 0.131185
17537.3s 1518 Epoch 6, Step 14150, Loss: 0.1830, LR: 0.000003, Grad Norm: 0.131185
17537.6s 1519 Epoch 6, Step 14150, Loss: 0.1830, LR: 0.000003, Grad Norm: 0.131185
17537.9s 1520 Epoch 6, Step 14150, Loss: 0.1831, LR: 0.000003, Grad Norm: 0.131185
17596.3s 1521 Epoch 6, Step 14200, Loss: 0.1822, LR: 0.000003, Grad Norm: 0.095068
17596.5s 1522 Epoch 6, Step 14200, Loss: 0.1821, LR: 0.000003, Grad Norm: 0.095068
17596.8s 1523 Epoch 6, Step 14200, Loss: 0.1821, LR: 0.000003, Grad Norm: 0.095068
17597.0s 1524 Epoch 6, Step 14200, Loss: 0.1822, LR: 0.000003, Grad Norm: 0.095068
17654.9s 1525 Epoch 6, Step 14250, Loss: 0.1824, LR: 0.000003, Grad Norm: 0.103802
17655.1s 1526 Epoch 6, Step 14250, Loss: 0.1823, LR: 0.000003, Grad Norm: 0.103802
17655.4s 1527 Epoch 6, Step 14250, Loss: 0.1824, LR: 0.000003, Grad Norm: 0.103802
17655.6s 1528 Epoch 6, Step 14250, Loss: 0.1824, LR: 0.000003, Grad Norm: 0.103802
17714.0s 1529 Epoch 6, Step 14300, Loss: 0.1825, LR: 0.000003, Grad Norm: 0.051135
17714.2s 1530 Epoch 6, Step 14300, Loss: 0.1824, LR: 0.000003, Grad Norm: 0.051135
17714.5s 1531 Epoch 6, Step 14300, Loss: 0.1824, LR: 0.000003, Grad Norm: 0.051135
17714.8s 1532 Epoch 6, Step 14300, Loss: 0.1824, LR: 0.000003, Grad Norm: 0.051135
17772.7s 1533 Epoch 6, Step 14350, Loss: 0.1822, LR: 0.000003, Grad Norm: 0.081830
17772.9s 1534 Epoch 6, Step 14350, Loss: 0.1822, LR: 0.000003, Grad Norm: 0.081830
17773.4s 1535 Epoch 6, Step 14350, Loss: 0.1823, LR: 0.000003, Grad Norm: 0.081830
17773.8s 1536 Epoch 6, Step 14350, Loss: 0.1823, LR: 0.000003, Grad Norm: 0.081830
17831.6s 1537 Epoch 6, Step 14400, Loss: 0.1828, LR: 0.000003, Grad Norm: 0.082741
17831.8s 1538 Epoch 6, Step 14400, Loss: 0.1828, LR: 0.000003, Grad Norm: 0.082741
17832.1s 1539 Epoch 6, Step 14400, Loss: 0.1828, LR: 0.000003, Grad Norm: 0.082741
17832.4s 1540 Epoch 6, Step 14400, Loss: 0.1827, LR: 0.000003, Grad Norm: 0.082741
17890.2s 1541 Epoch 6, Step 14450, Loss: 0.1828, LR: 0.000003, Grad Norm: 0.143856
17890.4s 1542 Epoch 6, Step 14450, Loss: 0.1827, LR: 0.000003, Grad Norm: 0.143856
17891.0s 1543 Epoch 6, Step 14450, Loss: 0.1828, LR: 0.000003, Grad Norm: 0.143856
17891.4s 1544 Epoch 6, Step 14450, Loss: 0.1827, LR: 0.000003, Grad Norm: 0.143856
17949.4s 1545 Epoch 6, Step 14500, Loss: 0.1835, LR: 0.000003, Grad Norm: 0.082539
17949.6s 1546 Epoch 6, Step 14500, Loss: 0.1836, LR: 0.000003, Grad Norm: 0.082539
17949.9s 1547 Epoch 6, Step 14500, Loss: 0.1836, LR: 0.000003, Grad Norm: 0.082539
17950.1s 1548 Epoch 6, Step 14500, Loss: 0.1835, LR: 0.000003, Grad Norm: 0.082539
18007.8s 1549 Epoch 6, Step 14550, Loss: 0.1832, LR: 0.000003, Grad Norm: 0.120378
18008.1s 1550 Epoch 6, Step 14550, Loss: 0.1832, LR: 0.000003, Grad Norm: 0.120378
18008.6s 1551 Epoch 6, Step 14550, Loss: 0.1832, LR: 0.000003, Grad Norm: 0.120378
18009.0s 1552 Epoch 6, Step 14550, Loss: 0.1831, LR: 0.000003, Grad Norm: 0.120378
18067.3s 1553 Epoch 6, Step 14600, Loss: 0.1830, LR: 0.000003, Grad Norm: 0.091034
18067.5s 1554 Epoch 6, Step 14600, Loss: 0.1830, LR: 0.000003, Grad Norm: 0.091034
18067.8s 1555 Epoch 6, Step 14600, Loss: 0.1830, LR: 0.000003, Grad Norm: 0.091034
18068.0s 1556 Epoch 6, Step 14600, Loss: 0.1830, LR: 0.000003, Grad Norm: 0.091034
18126.8s 1557 Epoch 6, Step 14650, Loss: 0.1826, LR: 0.000003, Grad Norm: 0.049885
18127.0s 1558 Epoch 6, Step 14650, Loss: 0.1826, LR: 0.000003, Grad Norm: 0.049885
18127.3s 1559 Epoch 6, Step 14650, Loss: 0.1826, LR: 0.000003, Grad Norm: 0.049885
18127.6s 1560 Epoch 6, Step 14650, Loss: 0.1827, LR: 0.000003, Grad Norm: 0.049885
18185.9s 1561 Epoch 6, Step 14700, Loss: 0.1834, LR: 0.000003, Grad Norm: 0.076768
18186.1s 1562 Epoch 6, Step 14700, Loss: 0.1834, LR: 0.000003, Grad Norm: 0.076768
18186.4s 1563 Epoch 6, Step 14700, Loss: 0.1833, LR: 0.000003, Grad Norm: 0.076768
18186.7s 1564 Epoch 6, Step 14700, Loss: 0.1833, LR: 0.000003, Grad Norm: 0.076768
18245.1s 1565 Epoch 6, Step 14750, Loss: 0.1833, LR: 0.000003, Grad Norm: 0.096232
18245.3s 1566 Epoch 6, Step 14750, Loss: 0.1833, LR: 0.000003, Grad Norm: 0.096232
18245.6s 1567 Epoch 6, Step 14750, Loss: 0.1832, LR: 0.000003, Grad Norm: 0.096232
18245.9s 1568 Epoch 6, Step 14750, Loss: 0.1832, LR: 0.000003, Grad Norm: 0.096232
18304.8s 1569 Epoch 6, Step 14800, Loss: 0.1835, LR: 0.000003, Grad Norm: 0.014286
18305.0s 1570 Epoch 6, Step 14800, Loss: 0.1835, LR: 0.000003, Grad Norm: 0.014286
18305.3s 1571 Epoch 6, Step 14800, Loss: 0.1835, LR: 0.000003, Grad Norm: 0.014286
18305.6s 1572 Epoch 6, Step 14800, Loss: 0.1835, LR: 0.000003, Grad Norm: 0.014286
18364.8s 1573 Epoch 6, Step 14850, Loss: 0.1835, LR: 0.000003, Grad Norm: 0.049321
18365.0s 1574 Epoch 6, Step 14850, Loss: 0.1834, LR: 0.000003, Grad Norm: 0.049321
18365.2s 1575 Epoch 6, Step 14850, Loss: 0.1834, LR: 0.000003, Grad Norm: 0.049321
18365.5s 1576 Epoch 6, Step 14850, Loss: 0.1834, LR: 0.000003, Grad Norm: 0.049321
18424.2s 1577 Epoch 6, Step 14900, Loss: 0.1846, LR: 0.000003, Grad Norm: 0.173198
18424.4s 1578 Epoch 6, Step 14900, Loss: 0.1845, LR: 0.000003, Grad Norm: 0.173198
18424.7s 1579 Epoch 6, Step 14900, Loss: 0.1846, LR: 0.000003, Grad Norm: 0.173198
18425.0s 1580 Epoch 6, Step 14900, Loss: 0.1846, LR: 0.000003, Grad Norm: 0.173198
18483.8s 1581 Epoch 6, Step 14950, Loss: 0.1849, LR: 0.000003, Grad Norm: 0.119885
18484.0s 1582 Epoch 6, Step 14950, Loss: 0.1850, LR: 0.000003, Grad Norm: 0.119885
18484.2s 1583 Epoch 6, Step 14950, Loss: 0.1850, LR: 0.000003, Grad Norm: 0.119885
18484.5s 1584 Epoch 6, Step 14950, Loss: 0.1850, LR: 0.000003, Grad Norm: 0.119885
18542.0s 1585 Epoch 6, Step 15000, Loss: 0.1845, LR: 0.000003, Grad Norm: 0.049905
18542.2s 1586 Epoch 6, Step 15000, Loss: 0.1845, LR: 0.000003, Grad Norm: 0.049905
18542.5s 1587 Epoch 6, Step 15000, Loss: 0.1845, LR: 0.000003, Grad Norm: 0.049905
18542.8s 1588 Epoch 6, Step 15000, Loss: 0.1844, LR: 0.000003, Grad Norm: 0.049905
18601.9s 1589 Epoch 6, Step 15050, Loss: 0.1843, LR: 0.000003, Grad Norm: 0.108923
18602.1s 1590 Epoch 6, Step 15050, Loss: 0.1843, LR: 0.000003, Grad Norm: 0.108923
18602.4s 1591 Epoch 6, Step 15050, Loss: 0.1843, LR: 0.000003, Grad Norm: 0.108923
18602.6s 1592 Epoch 6, Step 15050, Loss: 0.1844, LR: 0.000003, Grad Norm: 0.108923
18661.3s 1593 Epoch 6, Step 15100, Loss: 0.1850, LR: 0.000002, Grad Norm: 0.136029
18661.5s 1594 Epoch 6, Step 15100, Loss: 0.1850, LR: 0.000002, Grad Norm: 0.136029
18661.8s 1595 Epoch 6, Step 15100, Loss: 0.1850, LR: 0.000002, Grad Norm: 0.136029
18662.1s 1596 Epoch 6, Step 15100, Loss: 0.1850, LR: 0.000002, Grad Norm: 0.136029
18720.7s 1597 Epoch 6, Step 15150, Loss: 0.1853, LR: 0.000002, Grad Norm: 0.049035
18720.8s 1598 Epoch 6, Step 15150, Loss: 0.1852, LR: 0.000002, Grad Norm: 0.049035
18721.2s 1599 Epoch 6, Step 15150, Loss: 0.1853, LR: 0.000002, Grad Norm: 0.049035
18721.4s 1600 Epoch 6, Step 15150, Loss: 0.1854, LR: 0.000002, Grad Norm: 0.049035
18778.8s 1601 Epoch 6, Step 15200, Loss: 0.1860, LR: 0.000002, Grad Norm: 0.069640
18779.0s 1602 Epoch 6, Step 15200, Loss: 0.1859, LR: 0.000002, Grad Norm: 0.069640
18779.3s 1603 Epoch 6, Step 15200, Loss: 0.1860, LR: 0.000002, Grad Norm: 0.069640
18779.6s 1604 Epoch 6, Step 15200, Loss: 0.1860, LR: 0.000002, Grad Norm: 0.069640
18838.2s 1605 Epoch 6, Step 15250, Loss: 0.1857, LR: 0.000002, Grad Norm: 0.065402
18838.4s 1606 Epoch 6, Step 15250, Loss: 0.1857, LR: 0.000002, Grad Norm: 0.065402
18838.7s 1607 Epoch 6, Step 15250, Loss: 0.1857, LR: 0.000002, Grad Norm: 0.065402
18839.0s 1608 Epoch 6, Step 15250, Loss: 0.1857, LR: 0.000002, Grad Norm: 0.065402
18897.7s 1609 Epoch 6, Step 15300, Loss: 0.1858, LR: 0.000002, Grad Norm: 0.076959
18897.9s 1610 Epoch 6, Step 15300, Loss: 0.1859, LR: 0.000002, Grad Norm: 0.076959
18898.1s 1611 Epoch 6, Step 15300, Loss: 0.1859, LR: 0.000002, Grad Norm: 0.076959
18898.4s 1612 Epoch 6, Step 15300, Loss: 0.1859, LR: 0.000002, Grad Norm: 0.076959
18957.8s 1613 Epoch 6, Step 15350, Loss: 0.1859, LR: 0.000002, Grad Norm: 0.054207
18958.0s 1614 Epoch 6, Step 15350, Loss: 0.1858, LR: 0.000002, Grad Norm: 0.054207
18958.2s 1615 Epoch 6, Step 15350, Loss: 0.1859, LR: 0.000002, Grad Norm: 0.054207
18958.5s 1616 Epoch 6, Step 15350, Loss: 0.1858, LR: 0.000002, Grad Norm: 0.054207
19017.0s 1617 Epoch 6, Step 15400, Loss: 0.1867, LR: 0.000002, Grad Norm: 0.121739
19017.2s 1618 Epoch 6, Step 15400, Loss: 0.1867, LR: 0.000002, Grad Norm: 0.121739
19017.5s 1619 Epoch 6, Step 15400, Loss: 0.1867, LR: 0.000002, Grad Norm: 0.121739
19017.8s 1620 Epoch 6, Step 15400, Loss: 0.1867, LR: 0.000002, Grad Norm: 0.121739
19076.2s 1621 Epoch 6, Step 15450, Loss: 0.1866, LR: 0.000002, Grad Norm: 0.113424
19076.4s 1622 Epoch 6, Step 15450, Loss: 0.1866, LR: 0.000002, Grad Norm: 0.113424
19076.6s 1623 Epoch 6, Step 15450, Loss: 0.1866, LR: 0.000002, Grad Norm: 0.113424
19076.9s 1624 Epoch 6, Step 15450, Loss: 0.1866, LR: 0.000002, Grad Norm: 0.113424
19135.5s 1625 Epoch 6, Step 15500, Loss: 0.1864, LR: 0.000002, Grad Norm: 0.009763
19135.7s 1626 Epoch 6, Step 15500, Loss: 0.1864, LR: 0.000002, Grad Norm: 0.009763
19136.0s 1627 Epoch 6, Step 15500, Loss: 0.1863, LR: 0.000002, Grad Norm: 0.009763
19136.3s 1628 Epoch 6, Step 15500, Loss: 0.1863, LR: 0.000002, Grad Norm: 0.009763
19195.4s 1629 Epoch 6, Step 15550, Loss: 0.1857, LR: 0.000002, Grad Norm: 0.029109
19195.6s 1630 Epoch 6, Step 15550, Loss: 0.1857, LR: 0.000002, Grad Norm: 0.029109
19195.9s 1631 Epoch 6, Step 15550, Loss: 0.1856, LR: 0.000002, Grad Norm: 0.029109
19196.2s 1632 Epoch 6, Step 15550, Loss: 0.1856, LR: 0.000002, Grad Norm: 0.029109
19254.8s 1633 Epoch 6, Step 15600, Loss: 0.1860, LR: 0.000002, Grad Norm: 0.102572
19255.0s 1634 Epoch 6, Step 15600, Loss: 0.1860, LR: 0.000002, Grad Norm: 0.102572
19255.3s 1635 Epoch 6, Step 15600, Loss: 0.1860, LR: 0.000002, Grad Norm: 0.102572
19255.6s 1636 Epoch 6, Step 15600, Loss: 0.1860, LR: 0.000002, Grad Norm: 0.102572
19314.3s 1637 Epoch 6, Step 15650, Loss: 0.1862, LR: 0.000002, Grad Norm: 0.067672
19314.5s 1638 Epoch 6, Step 15650, Loss: 0.1862, LR: 0.000002, Grad Norm: 0.067672
19314.8s 1639 Epoch 6, Step 15650, Loss: 0.1862, LR: 0.000002, Grad Norm: 0.067672
19315.0s 1640 Epoch 6, Step 15650, Loss: 0.1862, LR: 0.000002, Grad Norm: 0.067672
19373.4s 1641 Epoch 6, Step 15700, Loss: 0.1864, LR: 0.000002, Grad Norm: 0.078547
19373.5s 1642 Epoch 6, Step 15700, Loss: 0.1864, LR: 0.000002, Grad Norm: 0.078547
19373.8s 1643 Epoch 6, Step 15700, Loss: 0.1864, LR: 0.000002, Grad Norm: 0.078547
19374.1s 1644 Epoch 6, Step 15700, Loss: 0.1864, LR: 0.000002, Grad Norm: 0.078547
19433.2s 1645 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
19433.2s 1646 To disable this warning, you can either:
19433.2s 1647 - Avoid using `tokenizers` before the fork if possible
19433.2s 1648 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
19433.3s 1649 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
19433.3s 1650 To disable this warning, you can either:
19433.3s 1651 - Avoid using `tokenizers` before the fork if possible
19433.3s 1652 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
19433.4s 1653 Epoch 6, Step 15750, Loss: 0.1860, LR: 0.000002, Grad Norm: 0.085262
19433.4s 1654 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
19433.4s 1655 To disable this warning, you can either:
19433.4s 1656 - Avoid using `tokenizers` before the fork if possible
19433.4s 1657 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
19433.4s 1658 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
19433.4s 1659 To disable this warning, you can either:
19433.4s 1660 - Avoid using `tokenizers` before the fork if possible
19433.4s 1661 - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
19539.6s 1662 Epoch 6 completed. Train Loss: 0.1860, Validation Loss: 0.1821
19539.6s 1663 Selected animals for validation CLIP score: ['antelope', 'deer', 'scorpion', 'seahorse', 'frog']
19539.6s 1664 Expected types for unet: (<class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionModel'>,), got <class 'peft.peft_model.PeftModel'>.
19539.6s 1665 You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
19543.7s 1666 Animal: antelope, CLIP Score: 31.8624
19546.2s 1667 Animal: deer, CLIP Score: 30.1202
19548.8s 1668 Animal: scorpion, CLIP Score: 30.2577
19551.4s 1669 Animal: seahorse, CLIP Score: 24.5078
19554.1s 1670 Animal: frog, CLIP Score: 32.1001
19554.1s 1671 Average Validation CLIP Score: 29.7696
19554.1s 1672 EarlyStopping counter: 5 out of 5
19554.1s 1673 Early stopping triggered
19554.1s 1674 Training completed!
19554.1s 1675 Generating validation samples...
19554.1s 1676 Selected animals for validation: ['dog', 'bat', 'pig', 'cat', 'rhinoceros']
19554.2s 1677 Expected types for unet: (<class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionModel'>,), got <class 'peft.peft_model.PeftModel'>.
19554.2s 1678 You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
19566.2s 1679 Generated image for dog saved at /kaggle/working/output/validation_samples/dog.png
19578.2s 1680 Generated image for bat saved at /kaggle/working/output/validation_samples/bat.png
19590.1s 1681 Generated image for pig saved at /kaggle/working/output/validation_samples/pig.png
19602.0s 1682 Generated image for cat saved at /kaggle/working/output/validation_samples/cat.png
19614.2s 1683 Generated image for rhinoceros saved at /kaggle/working/output/validation_samples/rhinoceros.png
19614.2s 1684 Comparison image saved at: /kaggle/working/output/comparison_samples/animal_comparison.png
19614.2s 1685 Evaluating model with CLIP Score...
19615.7s 1686 Selected animals for evaluation: ['cockroach', 'possum', 'scallop', 'seahorse', 'swan', 'jellyfish', 'duck', 'caterpillar', 'bee', 'crocodile']
19615.7s 1687 Expected types for unet: (<class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionModel'>,), got <class 'peft.peft_model.PeftModel'>.
19615.7s 1688 You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
19627.8s 1689 Animal: cockroach, CLIP Score: 32.1370
19639.8s 1690 Animal: possum, CLIP Score: 31.8816
19651.8s 1691 Animal: scallop, CLIP Score: 28.1066
19663.8s 1692 Animal: seahorse, CLIP Score: 31.6945
19675.8s 1693 Animal: swan, CLIP Score: 30.9865
19687.8s 1694 Animal: jellyfish, CLIP Score: 30.3364
19699.8s 1695 Animal: duck, CLIP Score: 32.2639
19711.7s 1696 Animal: caterpillar, CLIP Score: 30.2457
19723.7s 1697 Animal: bee, CLIP Score: 32.6509
19735.7s 1698 Animal: crocodile, CLIP Score: 31.5736
19735.7s 1699 Average CLIP Score: 31.1877
19735.7s 1700 Evaluation results saved to /kaggle/working/output/evaluation_results.xlsx
19735.7s 1701 All done! Average CLIP Score: 31.1877
19735.7s 1702 Check the output directory for results.
19741.5s 1703 /usr/local/lib/python3.11/dist-packages/traitlets/traitlets.py:2915: FutureWarning: --Exporter.preprocessors=["remove_papermill_header.RemovePapermillHeader"] for containers is deprecated in traitlets 5.0. You can pass `--Exporter.preprocessors item` ... multiple times to add items to a list.
19741.5s 1704 warn(
19741.5s 1705 [NbConvertApp] Converting notebook __notebook__.ipynb to notebook
19742.3s 1706 [NbConvertApp] Writing 1130046 bytes to __notebook__.ipynb
19743.5s 1707 /usr/local/lib/python3.11/dist-packages/traitlets/traitlets.py:2915: FutureWarning: --Exporter.preprocessors=["nbconvert.preprocessors.ExtractOutputPreprocessor"] for containers is deprecated in traitlets 5.0. You can pass `--Exporter.preprocessors item` ... multiple times to add items to a list.
19743.5s 1708 warn(
19743.5s 1709 [NbConvertApp] Converting notebook __notebook__.ipynb to html
19744.5s 1710 [NbConvertApp] Writing 1277126 bytes to __results__.html